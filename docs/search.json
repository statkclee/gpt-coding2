[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "챗GPT 코딩2",
    "section": "",
    "text": "서문\n지난 몇 년간, 인공지능(AI)의 발전은 우리가 코드를 이해하고 데이터를 바라보는 방식을 근본적으로 바꾸어 놓았습니다. AI가 코드 초안을 작성해주고, 막혔던 질문에 길을 터주는 시대에, 어떤 역량을 갖춰야 할까요? 넘쳐나는 정보와 도구 속에서 길을 잃지 않고, 데이터로부터 진정한 가치를 만들어내는 전문가로 성장하려면 무엇을 배워야 할까요?\n많은 이들이 데이터 과학 프로젝트를 ’일회성 실험’으로 생각합니다. 데이터를 가져와 모델을 만들고, 결과를 보고하는 작업은 분명 중요합니다. 하지만 거기서 멈춘다면, 그 노력은 파편적인 코드와 지식으로만 남게 됩니다. AI에게 복잡한 작업을 맡기더라도, 그 결과가 얼마나 믿을 수 있는지, 내일 또 다른 데이터가 들어왔을 때 전체 과정을 손쉽게 반복할 수 있는지 확신하기 어렵습니다.\n이 책은 그 한계를 넘어서기 위한 여정으로의 초대입니다.\n단순히 코딩 문법이나 AI 도구 사용법을 나열하는 데 그치지 않고, 이 책은 ‘스스로 살아 움직이며 가치를 창출하는 지능형 데이터 시스템’ 을 구축하는 전체 과정을 안내합니다.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#감사의-글",
    "href": "index.html#감사의-글",
    "title": "챗GPT 코딩2",
    "section": "감사의 글",
    "text": "감사의 글\n\n이 책이 탄생할 수 있도록 도움을 주신 여러분께 깊은 감사의 마음을 표합니다.\n공익법인 한국 R 사용자회가 없었다면 데이터 과학분야 챗GPT 시리즈가 세상에 나오지 못했을 것입니다. 한국 R 사용자회의 유충현 회장님, 신종화 사무처장님, 홍성학 감사님, 올해부터 새롭게 공익법인 한국 R 사용자를 이끌어주실 형환희 회장님께 감사드립니다.\n또한 이 책은 2014년 처음 몸담게 된 소프트웨어 카펜트리 그렉 윌슨 박사님과 Python for Informatics 저자인 미시건 대학 찰스 세브란스 교수님을 비롯한 전세계 수많은 익명의 기여자들의 노력과 지원이 있었고, 서울 R 미트업에서 발표해주시고 참여해주신 수많은 분들이 격려와 영감을 주셨기에 가능했습니다.\n이 책이 출간되는데 있어 이들 모든 분들의 도움 없이는 어려웠을 것입니다. 그동안의 관심과 지원에 깊은 감사를 드리며, 이 책이 데이터 과학의 발전과 독자들에게 도움이 될 수 있기를 바라는 마음으로 마무리하겠습니다.\n\n2024년 3월 속초 청초호\n이광춘",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  프로그래밍 시작하기",
    "section": "",
    "text": "1.1 왜 지금인가\n2022년 11월, OpenAI가 챗GPT(ChatGPT)를 공개하면서 세상이 바뀌었다. 코드 작성, 글쓰기, 이미지 생성까지 해내는 인공지능(AI)이 공상과학에서 현실로 넘어온 순간이다. AI가 자연어 질문에 막힘없이 답하고, 복잡한 코드 초안을 순식간에 만들어내는 시대가 열렸다.\n“왜 여전히 코딩을 배워야 하는가?”\n계산기가 발명되었을 때 “왜 산수를 배워야 하는가?”라고 물었고, 스프레드시트가 등장했을 때 “왜 회계를 배워야 하는가?”라고 물었다. 계산기는 수학자를 대체하지 않았다. 단순 반복 계산에서 해방시켜 고차원적 문제에 집중하게 했을 뿐이다. 스프레드시트도 회계사를 없애지 않았다. 재무 데이터를 분석하고 미래를 예측하는 ’금융 모델러’로 진화시켰다.\nAI도 마찬가지다. 개발자를 대체하는 것이 아니라, 한 줄 한 줄 코드를 타이핑하는 고된 노동에서 해방시킨다. 전체 시스템을 구상하고 더 큰 그림을 그리는 설계자(Architect)로 거듭나게 한다.\n진짜 문제는 ’AI가 코딩을 할 수 있는가’가 아니다. “AI를 포함한 복잡한 시스템을 어떻게 신뢰하고, 관리하며, 자동화할 것인가?”가 핵심이다. AI라는 강력한 엔진을 장착한 자동차의 운전석에 앉아 있다고 생각해보라. 엔진의 힘을 최대한 활용하되, 핸들을 굳게 잡고 브레이크를 밟을 준비를 해야 한다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 시작하기</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-hardware",
    "href": "01-intro.html#sec-intro-hardware",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.5 컴퓨터 하드웨어 아키텍처",
    "text": "1.5 컴퓨터 하드웨어 아키텍처\n컴퓨터 언어를 배우기 전에, 컴퓨터가 어떻게 구성되어 있는지 알아야 한다. 컴퓨터나 스마트폰을 분해하면 다음과 같은 주요 부품이 있다.\n\n\n\n\n\n그림 1.3: 컴퓨터 아키텍처\n\n\n\n\n중앙처리장치(CPU): “다음에 뭘 할까?”라는 명령을 처리하는 핵심 부품이다. 3.0 GHz CPU라면 초당 30억 개의 명령을 처리할 수 있다.\n\n주기억장치(Main Memory): CPU가 즉시 필요한 정보를 저장한다. CPU만큼 빠르지만, 전원이 꺼지면 내용이 사라진다.\n\n보조기억장치(Secondary Memory): 주기억장치보다 느리지만, 전원이 꺼져도 정보가 유지된다. USB 메모리, SSD, 하드디스크가 여기에 속한다.\n\n입출력장치(I/O Devices): 화면, 키보드, 마우스, 마이크, 스피커, 터치패드 등 컴퓨터와 사람이 상호작용하는 장치다.\n\n네트워크(Network): 요즘 거의 모든 컴퓨터에는 네트워크 연결 기능이 있다. 느리고 때로는 불안정하지만, 외부와 데이터를 주고받는 보조기억장치의 일종으로 볼 수 있다.\n\n부품의 세부 작동 원리는 제조사 영역이지만, 프로그래밍할 때 이런 용어가 등장하므로 기본 개념을 알아두면 도움이 된다.\n프로그래머로서 대체로 CPU와 “대화”해서 다음 무엇을 실행하라고 지시한다. 때때로 CPU에 주기억장치, 보조기억장치, 네트워크 혹은 입출력장치도 사용하라고 지시한다.\n\n\n\n\n\n그림 1.4: AI 시대의 컴퓨팅 스택\n\n\n프로그래머는 컴퓨터의 “다음 무엇을 수행할까요?”에 대한 답을 하는 사람이기도 하다. 하지만 컴퓨터에 답하기 위해서 5mm 크기로 프로그래머를 컴퓨터에 집어넣고 초당 30억 개 명령어로 답을 하게 만드는 것은 매우 불편하다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-programming",
    "href": "01-intro.html#sec-intro-programming",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.6 프로그래밍 이해하기",
    "text": "1.6 프로그래밍 이해하기\n이 책은 독자를 프로그래밍 장인으로 이끈다. 전문 프로그래머가 아니더라도, 자료와 정보 분석 문제를 보고 해결할 수 있는 기술을 갖추게 된다. \n프로그래머가 되려면 두 가지 기술이 필요하다.\n\n첫째, 파이썬 같은 프로그래밍 언어의 어휘와 문법을 알아야 한다. 새로운 언어로 단어를 쓰고, 올바른 “문장”을 구성하는 법을 익혀야 한다.\n둘째, 스토리를 말할 줄 알아야 한다. 글을 쓸 때 단어와 문장을 조합해 아이디어를 전달하듯이, 프로그래밍에서는 프로그램이 “스토리”가 되고 해결하려는 문제가 “아이디어”가 된다. 스토리 구성에는 기술적인 면과 예술적인 면이 있다. 연습과 피드백으로 향상된다.\n\n파이썬을 배우면, 자바스크립트나 C++, Go 같은 두 번째 언어를 배우기가 훨씬 쉬워진다. 어휘와 문법은 다르지만, 문제 해결 기술은 모든 언어에 공통으로 적용되기 때문이다.\n파이썬의 어휘와 문장은 빠르게 익힐 수 있다. 하지만 새로운 문제를 풀기 위해 논리적인 프로그램을 작성하는 것은 시간이 걸린다. 작문을 배우듯이 프로그래밍을 배운다. 프로그램을 읽고 설명하는 것으로 시작해서, 간단한 프로그램을 작성하고, 점차 복잡한 프로그램으로 나아간다. 어느 순간 패턴이 눈에 들어오기 시작한다. 문제를 보면 자연스럽게 해결 프로그램이 떠오른다. 그 순간부터 프로그래밍은 즐겁고 창의적인 과정이 된다.\n파이썬의 어휘와 구조부터 시작한다. 간단한 예제가 나오더라도 인내심을 갖자.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-words",
    "href": "01-intro.html#sec-intro-words",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.7 단어와 문장",
    "text": "1.7 단어와 문장\n \n사람 언어와 달리 파이썬 어휘는 매우 적다. 예약어(reserved words)라 부른다. 예약어를 만나면 파이썬은 딱 하나의 의미만 인식한다. 프로그램을 작성할 때 자신만의 단어도 만들 수 있다. 변수(Variable)다. 변수 이름은 자유롭게 지을 수 있지만, 예약어를 변수 이름으로 쓸 수는 없다.\n강아지 훈련과 비슷하다. “앉아”, “기다려”, “가져와” 같은 특별한 명령어가 있다. 예약어가 아닌 말을 하면 강아지는 물끄러미 쳐다볼 뿐이다. “건강을 위해 함께 걷기 운동에 동참하면 좋겠어”라고 말하면, 강아지는 “뭐라뭐라 걷기 뭐라뭐라”로 듣는다. “걷기”가 강아지 언어의 예약어이기 때문이다.1\n파이썬 예약어 목록은 다음과 같다. 파이썬 REPL에서 help(\"keywords\")를 입력하면 자세한 내용을 확인할 수 있다.\n\n\n파이썬 예약어\n설명\n\n\n\nif, elif, else\n조건문에 사용\n\n\nfor, while, break, continue\n반복문에 사용\n\n\ndef, return, lambda\n함수 정의에 사용\n\n\nclass\n클래스 정의에 사용\n\n\nimport, from, as\n모듈 가져오기에 사용\n\n\ntry, except, finally, raise\n예외 처리에 사용\n\n\nTrue, False, None\n논리 상수 및 빈 값\n\n\nand, or, not, in, is\n논리 연산자\n\n\nwith, pass, yield, assert\n기타 제어문\n\n\n\n강아지와 달리 파이썬은 이미 완벽하게 훈련되어 있다. “try”라고 말하면 매번 정확히 시도한다.\n예약어는 차차 배울 것이다. 지금은 파이썬에 말을 거는 방법에 집중하자. 괄호 안에 따옴표로 감싼 메시지를 넣으면 된다.\nprint(\"헬로 월드!\")\n파이썬 구문(Syntax)에 맞는 문장이다. 내장 함수 print에 출력할 문자열을 괄호 안에 따옴표로 감싸 전달했다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-conversation",
    "href": "01-intro.html#sec-intro-conversation",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.8 파이썬과 대화하기",
    "text": "1.8 파이썬과 대화하기\n\n간단한 문장을 만들었으니, 이제 파이썬과 대화를 시작하는 방법을 알아보자.\n먼저 파이썬을 컴퓨터에 설치해야 한다. 설치 방법은 파이썬 공식 웹사이트에 윈도우, 리눅스, 맥 각각에 대해 상세히 안내되어 있다. 설치 후 터미널이나 명령 프롬프트에서 python을 입력하면 인터프리터가 인터랙티브 모드로 시작된다.\nPython 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n&gt;&gt;&gt; 프롬프트가 “다음에 뭘 할까요?”라고 묻고 있다. 파이썬이 대화할 준비가 되었다.\n파이썬 언어를 전혀 모른다고 가정하자. 외계 행성에 착륙한 우주비행사처럼 말을 걸어보자.\n&gt;&gt;&gt; I come in peace, please take me to your leader\n  File \"&lt;stdin&gt;\", line 1\n    I come in peace, please take me to your leader\n      ^^^^^\nSyntaxError: invalid syntax\n통하지 않는다. 다행히 이 책을 가져왔다. 빠르게 타이핑한다.\n&gt;&gt;&gt; print(\"헬로 파이썬!\")\n헬로 파이썬!\n훨씬 낫다. 대화가 이어진다.\n&gt;&gt;&gt; print('당신은 하늘에서 온 전설적인 신이 분명합니다')\n당신은 하늘에서 온 전설적인 신이 분명합니다\n&gt;&gt;&gt; print('오랫동안 기다려왔습니다')\n오랫동안 기다려왔습니다\n&gt;&gt;&gt; print('전설에 따르면 겨자와 함께 먹으면 아주 맛있다고 합니다')\n전설에 따르면 겨자와 함께 먹으면 아주 맛있다고 합니다\n대화가 잘 진행되다가, 사소한 실수를 저지르면 파이썬이 다시 창을 겨눈다.\n파이썬은 복잡하고 강력하지만, 구문(Syntax)에 엄격하다. 정해진 규칙대로 말해야 알아듣는다.\n다른 사람이 작성한 프로그램을 사용할 때, 사실 그 프로그래머와 파이썬을 매개로 대화하고 있다. 앞으로 몇 장에 걸쳐 여러분도 프로그램을 작성하고, 사용자와 대화하게 된다.\n대화를 끝내는 방법도 알아야 한다.\n&gt;&gt;&gt; good-bye\nNameError: name 'good' is not defined\n&gt;&gt;&gt; if you don not mind, I need to leave\nSyntaxError: invalid syntax\n&gt;&gt;&gt; exit()\n처음 두 시도는 오류가 난다. 두 번째 오류가 다른 이유는 if가 예약어이기 때문이다. 파이썬은 if를 보고 조건문이 시작된다고 기대했는데, 구문이 맞지 않아 오류를 낸다.\n파이썬을 종료하려면 &gt;&gt;&gt; 프롬프트에서 exit() 또는 quit()를 입력한다.\n\n\n\n\n\n\n힌트전문용어: 인터프리터와 컴파일러\n\n\n\n파이썬은 사람이 읽고 쓸 수 있으면서 컴퓨터도 처리할 수 있는 고수준(High-level) 언어다. 자바, C++, PHP, 루비, 자바스크립트 등도 고수준 언어다. 하지만 CPU는 고수준 언어를 직접 이해하지 못한다.\nCPU가 이해하는 것은 기계어(machine language)뿐이다. 기계어는 0과 1로만 이루어져 있다.\n01010001110100100101010000001111\n11100110000011101010010101101101\n...\n0과 1로만 되어 있어 단순해 보이지만, 실제로는 파이썬보다 훨씬 복잡하다. 기계어로 직접 코드를 작성하는 프로그래머는 극소수다. 대신 고수준 언어를 기계어로 변환하는 번역기(translator)가 있다.\n기계어는 특정 하드웨어에 종속되어 다른 하드웨어로 이식(portable)되지 않는다. 고수준 언어로 작성된 프로그램은 두 가지 방식으로 이식할 수 있다. 새 하드웨어에 맞게 재컴파일하거나, 새 하드웨어용 인터프리터를 사용하면 된다.\n번역기는 두 종류로 나뉜다.\n\n\n인터프리터(Interpreter): 소스 코드를 한 줄씩 읽고 즉석에서 실행한다. 파이썬이 여기에 속한다.\n\n컴파일러(Compiler): 전체 소스 코드를 먼저 기계어로 번역한 뒤 실행 파일을 만든다.\n\n파이썬을 인터랙티브 모드로 실행하면, 명령문 하나를 입력할 때마다 즉시 처리하고 다음 입력을 기다린다.\n값을 나중에 쓰려면 파이썬에게 기억시킨다. 이름을 붙여 저장하고, 나중에 같은 이름으로 불러온다. 이름표가 바로 변수(variable)다.\n&gt;&gt;&gt; x = 6\n&gt;&gt;&gt; print(x)\n6\n&gt;&gt;&gt; y = x * 7\n&gt;&gt;&gt; print(y)\n42\n파이썬이 값 6을 x라는 이름으로 기억한다. print 함수로 확인할 수 있다. x에 7을 곱해 y에 저장하고, y를 출력한다.\n한 줄씩 입력하지만, 앞 명령에서 만든 데이터를 뒤 명령에서 사용할 수 있다. 파이썬이 순차적으로 처리하기 때문이다.\n인터프리터는 이렇게 대화형으로 동작한다. 컴파일러는 다르다. 전체 프로그램을 파일에 담고, 기계어로 번역한 뒤, 실행 파일로 저장한다.\n윈도우에서 실행 파일은 .exe나 .dll 확장자를 갖는다. 리눅스나 맥에서는 특정 확장자가 없다.\n실행 파일을 텍스트 편집기로 열면 이런 괴상한 문자가 보인다.\n^?ELF^A^A^A^@^@^@^@^@^@^@^@^@^B^@^C^@^A^@^@^@\\xa0\\x82\n^D^H4^@^@^@\\x90^]^@^@^@^@^@^@4^@ ^@^G^@(^@$^@!^@^F^@\n....\n기계어를 직접 읽고 쓰기는 어렵다. 그래서 인터프리터와 컴파일러가 있다.\n파이썬 인터프리터 자체는 무엇으로 만들어졌을까? C 언어로 작성되었다(CPython이라 부른다). 소스 코드는 https://github.com/python/cpython에서 볼 수 있다. 파이썬을 설치한다는 것은 컴파일된 파이썬 기계어 코드를 컴퓨터에 복사하는 것이다. 윈도우에서는 이런 경로에 있다.\n\nC:\\Users\\사용자명\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n\n프로그래밍을 시작하는 데 이 정도까지 알 필요는 없다. 하지만 이런 궁금증을 초반에 해결해두면 나중에 도움이 된다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-write-program",
    "href": "01-intro.html#sec-intro-write-program",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.9 프로그램 작성하기",
    "text": "1.9 프로그램 작성하기\n\n인터프리터에 직접 명령어를 입력하는 것은 기능을 익히기에 좋지만, 복잡한 문제를 풀기에는 적합하지 않다.\n프로그램을 작성할 때는 텍스트 편집기로 스크립트(script)라는 파일에 명령어를 저장한다. 파이썬 스크립트는 .py 확장자를 사용한다.\n스크립트를 실행하려면 파이썬 인터프리터에 파일 이름을 전달한다.\n$ cat hello.py\nprint(\"헬로 월드!\")\n$ python hello.py\n헬로 월드!\n$\n$는 운영체제 프롬프트고, python hello.py는 hello.py 파일을 실행하라는 명령이다.\n인터랙티브 모드 대신 파일에서 코드를 읽어 실행한다. 파일 끝에 도달하면 자동으로 종료되므로 exit()를 입력할 필요가 없다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-what-is-program",
    "href": "01-intro.html#sec-intro-what-is-program",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.10 프로그램이란 무엇인가?",
    "text": "1.10 프로그램이란 무엇인가?\n프로그램(Program)은 특정 작업을 수행하는 파이썬 문장의 집합이다. hello.py처럼 한 줄짜리도 프로그램이다.\n프로그램을 이해하려면, 먼저 해결하려는 문제를 생각하고 문제를 푸는 프로그램을 살펴보는 편이 좋다.\n예를 들어, 소셜 미디어 게시물에서 가장 자주 사용된 단어를 찾는 연구를 한다고 하자. 글을 출력해서 눈으로 세면 시간도 오래 걸리고 실수하기 쉽다. 파이썬 프로그램으로 빠르고 정확하게 처리하는 것이 낫다.\n다음 텍스트에서 가장 많이 나오는 단어와 빈도를 세어보라.\nthe clown ran after the car and the car ran into the tent\nand the tent fell down on the clown and the car\n이걸 수백만 줄에 대해 한다고 상상해보라. 파이썬을 배워 프로그램을 작성하는 게 훨씬 빠르다.\n다음은 가장 빈도가 높은 단어를 찾는 프로그램이다.\ntext = 'the clown ran after the car and the car ran into the tent and the tent fell down on the clown and the car'\n\nwords = text.split()\ncounts = dict()\n\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nbigcount = None\nbigword = None\n\nfor word, count in counts.items():\n    if bigcount is None or count &gt; bigcount:\n        bigword = word\n        bigcount = count\n\nprint(bigword, bigcount)\n# 출력: the 7\n위 프로그램을 쓰는 데 파이썬을 완벽히 알 필요는 없다. 파이썬 프로그램 만드는 법은 앞으로 여러 장에 걸쳐 배운다. 지금은 코드를 words.py로 저장하고 실행하면 된다.\n파이썬은 사용자와 프로그래머 사이의 중개자다. 파이썬이 설치된 컴퓨터라면 누구나 같은 프로그램을 실행할 수 있다. 파이썬을 통해 서로 소통한다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-components",
    "href": "01-intro.html#sec-intro-components",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.11 프로그램 구성요소",
    "text": "1.11 프로그램 구성요소\n앞으로 파이썬의 어휘, 문장, 문단, 스토리 구조를 배운다. 파이썬의 능력과 능력을 조합하는 방법을 익힌다.\n프로그램 작성에 사용되는 기본 패턴이 있다. 파이썬만이 아니라 기계어부터 고수준 언어까지 모든 언어에 공통된다.\n\n\n입력: 외부에서 데이터를 가져온다. 파일, 키보드, 센서 등에서 받는다.\n\n출력: 결과를 화면에 출력하거나 파일에 저장한다. 스피커로 소리를 내보낼 수도 있다.\n\n순차 실행: 작성된 순서대로 한 줄씩 실행한다.\n\n조건 실행: 조건을 확인하고 실행하거나 건너뛴다.\n\n반복 실행: 같은 명령을 반복한다. 보통 반복할 때마다 변화가 있다.\n\n재사용: 명령문에 이름을 붙여 저장하고, 필요할 때 불러 쓴다.\n\n간단해 보이지만 그렇지 않다. “걷기”를 “한 발을 다른 발 앞에 놓는 것”이라고 설명하는 것과 비슷하다. 프로그래밍의 예술은 이 기본 요소를 조합해 사용자에게 유용한 것을 만드는 데 있다.\n앞서 본 단어 세기 프로그램은 입력을 제외한 모든 기본 요소를 사용한다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-bug",
    "href": "01-intro.html#sec-intro-bug",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.12 프로그램이 잘못되면?",
    "text": "1.12 프로그램이 잘못되면?\n앞서 보았듯이 파이썬 코드는 명확하게 작성해야 한다. 작은 실수도 파이썬이 실행을 포기하게 만든다.\n초보자는 파이썬이 냉정하다고 느낀다. 하지만 파이썬은 도구일 뿐 감정이 없다.\n&gt;&gt;&gt; primt(\"안녕 세상!\")\nNameError: name 'primt' is not defined\n&gt;&gt;&gt; 나는 파이썬이 싫어!\nSyntaxError: invalid syntax\n파이썬과 싸워봐야 소용없다. 오류 메시지는 도움 요청이다. “입력한 것을 이해하지 못하겠다”고 말하는 것뿐이다.\n파이썬은 강아지와 비슷하다. 정해진 단어만 이해하고, &gt;&gt;&gt; 프롬프트로 다음 명령을 기다린다. “NameError”는 “뭔가 말씀하시는 것 같은데 이해 못 하겠어요, 다시 말씀해주세요”와 같다.\n프로그램이 복잡해지면 세 종류의 오류를 만나게 된다.\n\n\n구문 오류(Syntax Error): 가장 흔하고 고치기 쉽다. 파이썬 문법에 맞지 않는다는 뜻이다. 파이썬이 오류 위치를 알려주지만, 실제 원인은 그 앞에 있을 수도 있다.\n\n논리 오류(Logic Error): 구문은 맞지만 실행 순서가 잘못됐다. 예: “물병에서 한 모금 마시고, 가방에 넣고, 도서관으로 걸어가서, 물병을 닫는다.”\n\n의미론적 오류(Semantic Error): 구문도 맞고 순서도 맞지만, 의도한 대로 동작하지 않는다. 예: 식당 가는 길을 알려줬는데 “왼쪽”이라고 해야 할 곳에서 “오른쪽”이라고 말한 경우. 친구는 지시대로 완벽히 따랐지만 농장에 도착한다.\n\n어떤 오류든 파이썬은 요청받은 대로 충실히 실행할 뿐이다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-journey",
    "href": "01-intro.html#sec-intro-journey",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.13 학습으로의 여정",
    "text": "1.13 학습으로의 여정\n처음에 개념이 잘 와닿지 않아도 기죽지 말자. 말하기를 배울 때도 처음 몇 년은 옹알이를 한다. 어휘에서 문장으로, 문장에서 문단으로 넘어가는 데 몇 년이 걸린다. 스토리를 쓰는 데는 더 오래 걸린다.\n이 책은 파이썬을 빠르게 배울 수 있도록 정보를 제공한다. 하지만 새 언어를 익히는 것처럼, 자연스럽게 느껴지기까지 시간이 필요하다. 큰 그림을 이루는 조각들을 정의하면서 여러 주제를 오가다 보면 혼란스러울 수 있다. 꼭 순서대로 읽을 필요는 없다. 앞뒤를 넘나들며 읽어도 좋다. 세부 사항을 완벽히 이해하지 않고 고급 내용을 먼저 훑어보며 “왜”를 이해할 수도 있다. 나중에 다시 돌아와 연습문제를 풀면, 어려웠던 주제가 더 쉽게 느껴진다.\n첫 프로그래밍 언어를 배울 때는 “유레카!” 순간이 몇 번 찾아온다. 망치와 끌로 돌을 깎아 조각품을 만드는 것과 같다.\n뭔가 특별히 어렵다면 밤새 붙잡고 있지 말자. 쉬고, 낮잠 자고, 간식 먹고, 다른 사람(또는 강아지)에게 문제를 설명해보라. 머리를 식힌 뒤 다시 시도하면 된다. 이 책의 개념을 익히고 나면, 프로그래밍이 쉽고 멋진 것이었다는 걸 알게 된다. 배울 가치가 있다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "01-intro.html#생각해볼-점",
    "href": "01-intro.html#생각해볼-점",
    "title": "1  프로그래밍 시작하기",
    "section": "💡 생각해볼 점",
    "text": "💡 생각해볼 점\n컴퓨터는 “다음에 무엇을 할까?”라는 질문을 끊임없이 던지는 기계이고, 프로그래머는 질문에 대한 답을 미리 작성해두는 사람이다. CPU가 연산을 수행하고, 메모리가 데이터를 보관하며, 저장장치가 전원이 꺼져도 정보를 유지한다는 사실은 변하지 않는다. 달라진 것은 답을 작성하는 방식이다.\n전통적인 프로그래밍 학습에서는 구문을 암기하고 직접 코드를 타이핑하는 것이 핵심이었다. 하지만 AI 시대에 접어들면서 프로그래머 역할이 “코드 작성자”에서 “의도 설계자”이자 “코드 검토자”로 이동하고 있다. 키보드로 코드를 얼마나 빨리 칠 수 있느냐보다, 원하는 동작을 얼마나 명확하게 정의할 수 있느냐가 중요해졌다.\n다음 장에서는 의도를 명확하게 정의하는 방법을 다룬다. 테스트 코드를 통해 “이렇게 동작해야 한다”는 명세를 작성하고, AI에게 전달하여 실제 코드를 생성받는 워크플로우를 배운다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 시작하기</span>"
    ]
  },
  {
    "objectID": "01-intro.html#footnotes",
    "href": "01-intro.html#footnotes",
    "title": "1  프로그래밍 시작하기",
    "section": "",
    "text": "고양이 친밀감(Cat Proximity)↩︎",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 시작하기</span>"
    ]
  },
  {
    "objectID": "02-spec.html",
    "href": "02-spec.html",
    "title": "2  의도 명세하기",
    "section": "",
    "text": "2.1 AI 시대 프로그래밍 패러다임\n프로그래밍의 본질은 “컴퓨터에게 원하는 일을 시키는 것”이다. 전통적으로 이 과정은 프로그래밍 언어의 구문(syntax)을 배우고, 그 구문으로 코드를 직접 작성하는 방식으로 진행되었다. 그러나 AI 시대에는 근본적인 전환이 일어났다. 이제 코드 작성은 AI가 더 빠르고 정확하게 수행한다. 그렇다면 인간의 역할은 무엇인가?\n의도를 명확하게 표현하는 것이다.\n기존 프로그래밍 교육은 구문 암기 → 예제 따라하기 → 연습 문제 풀기 → 디버깅의 순서로 진행되었다. 이 방식의 문제점은 “왜 이렇게 짜는가”보다 “어떻게 쓰는가”에 집중한다는 것이다.\nAI 시대 프로그래밍은 순서가 바뀐다:\n이 패러다임에서 인간의 핵심 역량은 의도를 명확하게 표현하는 능력이다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>의도 명세하기</span>"
    ]
  },
  {
    "objectID": "02-spec.html#sec-spec-paradigm",
    "href": "02-spec.html#sec-spec-paradigm",
    "title": "2  의도 명세하기",
    "section": "",
    "text": "그림 2.1: 전통적 학습과 AI 시대 학습의 비교\n\n\n\n\n\n의도 명세 - “무엇을 원하는가”를 명확히 정의\nAI 코드 생성 - AI에게 구현을 위임\n코드 이해 - 생성된 코드의 동작 원리 파악\n검증과 개선 - 의도대로 동작하는지 확인",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>의도 명세하기</span>"
    ]
  },
  {
    "objectID": "02-spec.html#sec-spec-test-is-spec",
    "href": "02-spec.html#sec-spec-test-is-spec",
    "title": "2  의도 명세하기",
    "section": "2.2 테스트는 명세서다",
    "text": "2.2 테스트는 명세서다\n의도를 표현하는 가장 효과적인 방법은 테스트 코드를 작성하는 것이다. 테스트는 단순히 코드의 정확성을 검증하는 도구가 아니다. AI 시대에 테스트는 “내가 원하는 것”을 AI가 이해할 수 있는 형식으로 표현한 명세서(Specification)가 된다.\n\n\n\n\n\n\n그림 2.2: 테스트 = AI에게 의도를 전달하는 명세서\n\n\n\n\n2.2.1 명세서가 담아야 할 정보\n좋은 테스트(명세서)는 다음 정보를 담아야 한다:\n\n함수/기능의 이름: 무엇을 하는 기능인가\n입력(Input): 어떤 데이터를 받는가\n출력(Output): 어떤 결과를 반환하는가\n경계 조건: 특수한 상황에서 어떻게 동작해야 하는가",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>의도 명세하기</span>"
    ]
  },
  {
    "objectID": "02-spec.html#sec-spec-calculator",
    "href": "02-spec.html#sec-spec-calculator",
    "title": "2  의도 명세하기",
    "section": "2.3 관통 예제: 간단한 계산기",
    "text": "2.3 관통 예제: 간단한 계산기\n이 책 전체를 관통하는 예제로 간단한 계산기를 만들어 보자. 계산기는 단순해 보이지만, 프로그래밍의 핵심 개념을 모두 담고 있다.\n\n2.3.1 덧셈 기능의 의도 명세\n“두 숫자를 더하는 기능이 필요해.”\n이 의도를 테스트 코드로 표현하면:\n#| eval: false\ndef test_add():\n    \"\"\"덧셈 기능에 대한 명세서\"\"\"\n\n    # 기본 덧셈\n    assert add(2, 3) == 5\n    assert add(0, 0) == 0\n    assert add(100, 200) == 300\n\n    # 음수 처리\n    assert add(-1, 1) == 0\n    assert add(-2, -3) == -5\n\n    # 소수점\n    assert add(1.5, 2.5) == 4.0\n이 테스트 코드가 담고 있는 명세:\n\n\n\n\n\n\n항목\n내용\n\n\n\n\n함수 이름\nadd\n\n\n입력\n두 개의 숫자 (정수, 음수, 소수 모두 가능)\n\n\n출력\n두 수의 합\n\n\n특이사항\n음수와 소수점도 올바르게 처리해야 함\n\n\n\n\n\n표 2.1: 테스트 코드에서 추출한 명세서\n\n\n\n\n\n2.3.2 왜 테스트가 먼저인가?\n전통적 프로그래밍에서는 코드를 먼저 작성하고 테스트는 나중에(혹은 아예 안) 작성했다. AI 시대에는 순서가 바뀐다:\nRed:     실패하는 테스트 작성 (인간) ← 핵심 역량\nGreen:   테스트 통과하는 코드 생성 (AI)\nRefactor: 코드 리뷰 및 개선 (인간 + AI 협업)\n\n\n\n\n\n\n그림 2.3: AI 시대 TDD 워크플로우\n\n\n\n테스트를 먼저 작성하면:\n\n의도가 명확해진다 - “무엇을 원하는가”를 구체적으로 생각하게 됨\nAI 소통이 정확해진다 - 모호한 자연어 대신 정확한 코드로 의도 전달\n검증이 자동화된다 - AI가 생성한 코드가 맞는지 즉시 확인 가능",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>의도 명세하기</span>"
    ]
  },
  {
    "objectID": "02-spec.html#sec-spec-basics",
    "href": "02-spec.html#sec-spec-basics",
    "title": "2  의도 명세하기",
    "section": "2.4 테스트 작성의 기초",
    "text": "2.4 테스트 작성의 기초\n\n2.4.1 assert 문 이해하기\nassert는 “이것이 참이어야 한다”는 선언이다:\n#| eval: false\n# 기본 형태\nassert 1 + 1 == 2          # 통과: 참이므로\n# assert 1 + 1 == 3        # 실패: AssertionError 발생\n#| eval: false\n# 변수와 함께\nresult = 10 * 5\nassert result == 50        # result가 50인지 확인\n\n\n2.4.2 좋은 테스트의 특징\n1. 하나의 테스트는 하나의 동작을 검증한다\n#| eval: false\n# 좋은 예: 각각 분리\ndef test_add_positive_numbers():\n    assert add(2, 3) == 5\n\ndef test_add_negative_numbers():\n    assert add(-2, -3) == -5\n2. 테스트 이름이 의도를 설명한다\n#| eval: false\n# 나쁜 예\ndef test1():\n    assert add(2, 3) == 5\n\n# 좋은 예\ndef test_add_returns_sum_of_two_positive_integers():\n    assert add(2, 3) == 5\n3. 경계 조건을 포함한다\n#| eval: false\ndef test_add_edge_cases():\n    assert add(0, 0) == 0              # 영(零)\n    assert add(-1, 1) == 0             # 상쇄\n    assert add(999999, 1) == 1000000   # 큰 수",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>의도 명세하기</span>"
    ]
  },
  {
    "objectID": "02-spec.html#sec-spec-practice",
    "href": "02-spec.html#sec-spec-practice",
    "title": "2  의도 명세하기",
    "section": "2.5 실습: 뺄셈 기능 명세하기",
    "text": "2.5 실습: 뺄셈 기능 명세하기\n계산기에 뺄셈 기능을 추가해보자. 먼저 테스트(명세서)를 작성한다:\n#| eval: false\ndef test_subtract():\n    \"\"\"뺄셈 기능에 대한 명세서\"\"\"\n\n    # 기본 뺄셈\n    assert subtract(5, 3) == 2\n    assert subtract(10, 10) == 0\n\n    # 음수 결과\n    assert subtract(3, 5) == -2\n\n    # 음수 입력\n    assert subtract(-3, -5) == 2    # -3 - (-5) = 2\n    assert subtract(-3, 5) == -8    # -3 - 5 = -8\n이 테스트가 담고 있는 명세를 정리해보면:\n\n함수 이름: subtract\n입력: 두 개의 숫자 (첫 번째에서 두 번째를 뺌)\n출력: 차이 값\n특이사항: 음수 입력과 음수 결과를 올바르게 처리",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>의도 명세하기</span>"
    ]
  },
  {
    "objectID": "02-spec.html#sec-spec-summary",
    "href": "02-spec.html#sec-spec-summary",
    "title": "2  의도 명세하기",
    "section": "2.6 핵심 정리",
    "text": "2.6 핵심 정리\n\n\n\n\n\n\n힌트AI 시대 프로그래밍의 핵심\n\n\n\n\n테스트 = 명세서: 테스트는 “내가 원하는 것”을 정확히 표현한 문서다\n의도 먼저: 코드 작성 전에 의도를 명확히 정의한다\nAI는 실행자: 명세(테스트)를 주면 AI가 코드를 생성한다\n인간은 설계자: 무엇을 만들지 결정하고, 결과를 검증하는 것이 인간의 역할\n\n\n\n다음 장에서는 이렇게 작성한 테스트를 AI에게 전달하여 실제 코드를 생성받는 방법을 알아본다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>의도 명세하기</span>"
    ]
  },
  {
    "objectID": "03-ai-request.html",
    "href": "03-ai-request.html",
    "title": "3  AI에게 코드 요청하기",
    "section": "",
    "text": "3.1 효과적인 프롬프트 작성법\n앞 장에서 테스트를 통해 “원하는 것”을 명세했다. 이제 이 명세를 AI에게 전달하여 실제 코드를 생성받는 방법을 알아본다.\nAI에게 코드를 요청할 때 가장 중요한 것은 맥락(Context)을 제공하는 것이다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI에게 코드 요청하기</span>"
    ]
  },
  {
    "objectID": "03-ai-request.html#sec-ai-request-prompt",
    "href": "03-ai-request.html#sec-ai-request-prompt",
    "title": "3  AI에게 코드 요청하기",
    "section": "",
    "text": "3.1.1 맥락 전달의 3요소\n\n무엇을 만들려 하는가 (목적)\n어떤 제약이 있는가 (조건)\n어떻게 동작해야 하는가 (명세/테스트)",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI에게 코드 요청하기</span>"
    ]
  },
  {
    "objectID": "03-ai-request.html#sec-ai-request-add",
    "href": "03-ai-request.html#sec-ai-request-add",
    "title": "3  AI에게 코드 요청하기",
    "section": "3.2 계산기 예제: 덧셈 구현 요청",
    "text": "3.2 계산기 예제: 덧셈 구현 요청\n2장에서 작성한 테스트를 기반으로 AI에게 코드를 요청해보자.\n\n3.2.1 테스트 기반 프롬프트\n다음 테스트를 통과하는 add 함수를 작성해주세요:\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(0, 0) == 0\n    assert add(-1, 1) == 0\n    assert add(-2, -3) == -5\n    assert add(1.5, 2.5) == 4.0\n\n\n3.2.2 AI의 응답\n#| eval: false\ndef add(a, b):\n    \"\"\"두 수를 더한 값을 반환한다.\"\"\"\n    return a + b\n테스트 실행으로 검증:\n#| eval: false\n# 테스트 실행\nassert add(2, 3) == 5\nassert add(0, 0) == 0\nassert add(-1, 1) == 0\nassert add(-2, -3) == -5\nassert add(1.5, 2.5) == 4.0\n\nprint(\"모든 테스트 통과!\")",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI에게 코드 요청하기</span>"
    ]
  },
  {
    "objectID": "03-ai-request.html#sec-ai-request-rich-context",
    "href": "03-ai-request.html#sec-ai-request-rich-context",
    "title": "3  AI에게 코드 요청하기",
    "section": "3.3 맥락을 풍부하게 제공하기",
    "text": "3.3 맥락을 풍부하게 제공하기\n단순한 테스트만 전달하는 것보다 더 풍부한 맥락을 제공하면 더 좋은 코드를 받을 수 있다:\n# 프로젝트 맥락\nPython으로 간단한 계산기를 만들고 있습니다.\n초보자도 이해할 수 있는 깔끔한 코드가 필요합니다.\n\n# 요구사항\n- 덧셈 함수 구현\n- 정수, 실수, 음수 모두 처리\n- 타입 힌트 포함\n- docstring 포함\n\n# 테스트 (명세)\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(1.5, 2.5) == 4.0",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI에게 코드 요청하기</span>"
    ]
  },
  {
    "objectID": "03-ai-request.html#sec-ai-request-summary",
    "href": "03-ai-request.html#sec-ai-request-summary",
    "title": "3  AI에게 코드 요청하기",
    "section": "3.4 핵심 정리",
    "text": "3.4 핵심 정리\n\n\n\n\n\n\n힌트AI 코드 요청의 핵심\n\n\n\n\n테스트를 먼저 보여준다 - 가장 정확한 명세\n맥락을 제공한다 - 프로젝트 목적, 제약조건\n구체적으로 요청한다 - 모호함을 피한다\n\n\n\n다음 장에서는 AI가 생성한 코드를 읽고 이해하는 방법을 알아본다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI에게 코드 요청하기</span>"
    ]
  },
  {
    "objectID": "04-verify.html",
    "href": "04-verify.html",
    "title": "4  코드 읽기와 검증",
    "section": "",
    "text": "4.1 왜 코드를 읽어야 하는가\nAI가 생성한 코드를 무조건 신뢰해서는 안 된다. 코드가 의도대로 동작하는지 검증하고, 어떻게 동작하는지 이해하는 것이 AI 시대 프로그래머의 핵심 역량이다.\nAI가 코드를 생성해주지만, 그 코드에는 여전히 문제가 있을 수 있다:",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>코드 읽기와 검증</span>"
    ]
  },
  {
    "objectID": "04-verify.html#sec-verify-why-read",
    "href": "04-verify.html#sec-verify-why-read",
    "title": "4  코드 읽기와 검증",
    "section": "",
    "text": "잘못된 이해 - AI가 의도를 오해했을 수 있다\n엣지 케이스 누락 - 명세하지 않은 상황에서 오류 발생 가능\n비효율적 구현 - 동작하지만 성능이 나쁜 코드\n보안 취약점 - 의도치 않은 보안 문제",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>코드 읽기와 검증</span>"
    ]
  },
  {
    "objectID": "04-verify.html#sec-verify-calculator",
    "href": "04-verify.html#sec-verify-calculator",
    "title": "4  코드 읽기와 검증",
    "section": "4.2 계산기 예제: 코드 검증",
    "text": "4.2 계산기 예제: 코드 검증\nAI가 생성한 덧셈 함수를 검증해보자.\n\n4.2.1 기본 테스트 실행\n#| eval: false\ndef add(a, b):\n    return a + b\n\n# 기본 테스트\nassert add(2, 3) == 5\nassert add(-1, 1) == 0\nprint(\"기본 테스트 통과!\")\n\n\n4.2.2 엣지 케이스 추가\n명세에 없던 상황을 테스트해본다:\n#| eval: false\n# 큰 수\nassert add(10**10, 10**10) == 2 * 10**10\n\n# 아주 작은 소수\nresult = add(0.1, 0.2)\nprint(f\"0.1 + 0.2 = {result}\")  # 부동소수점 주의!\n부동소수점 연산의 특성상 0.1 + 0.2가 정확히 0.3이 아닐 수 있다. 이런 발견이 코드 읽기와 검증의 가치다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>코드 읽기와 검증</span>"
    ]
  },
  {
    "objectID": "04-verify.html#sec-verify-understand",
    "href": "04-verify.html#sec-verify-understand",
    "title": "4  코드 읽기와 검증",
    "section": "4.3 코드 이해하기",
    "text": "4.3 코드 이해하기\n\n4.3.1 단계별 실행 추적\n코드가 어떻게 동작하는지 단계별로 추적해본다:\n#| eval: false\ndef add_with_trace(a, b):\n    print(f\"입력: a={a}, b={b}\")\n    result = a + b\n    print(f\"계산: {a} + {b} = {result}\")\n    return result\n\nadd_with_trace(3, 5)\n\n\n4.3.2 타입 확인\n입력과 출력의 타입을 확인한다:\n#| eval: false\ndef check_types():\n    # 정수 + 정수\n    r1 = add(3, 5)\n    print(f\"3 + 5 = {r1}, type: {type(r1)}\")\n\n    # 실수 + 실수\n    r2 = add(3.0, 5.0)\n    print(f\"3.0 + 5.0 = {r2}, type: {type(r2)}\")\n\n    # 정수 + 실수\n    r3 = add(3, 5.0)\n    print(f\"3 + 5.0 = {r3}, type: {type(r3)}\")\n\ncheck_types()",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>코드 읽기와 검증</span>"
    ]
  },
  {
    "objectID": "04-verify.html#sec-verify-checklist",
    "href": "04-verify.html#sec-verify-checklist",
    "title": "4  코드 읽기와 검증",
    "section": "4.4 검증 체크리스트",
    "text": "4.4 검증 체크리스트\nAI 생성 코드를 받았을 때 확인할 사항:\n\n\n\n\n\n\n노트코드 검증 체크리스트\n\n\n\n\n원래 테스트가 모두 통과하는가?\n엣지 케이스에서도 동작하는가?\n예상치 못한 입력에 어떻게 반응하는가?\n코드가 읽기 쉽고 이해 가능한가?\n불필요하게 복잡하지 않은가?",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>코드 읽기와 검증</span>"
    ]
  },
  {
    "objectID": "04-verify.html#sec-verify-summary",
    "href": "04-verify.html#sec-verify-summary",
    "title": "4  코드 읽기와 검증",
    "section": "4.5 핵심 정리",
    "text": "4.5 핵심 정리\n\n\n\n\n\n\n힌트코드 검증의 핵심\n\n\n\n\n테스트 실행 - 명세한 테스트를 먼저 실행\n엣지 케이스 - 명세에 없던 상황도 테스트\n코드 이해 - 동작 원리를 파악\n개선 요청 - 문제 발견 시 AI에게 수정 요청\n\n\n\n다음 장부터는 AI가 생성한 코드를 뜯어보며 프로그래밍의 핵심 개념들을 학습한다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>코드 읽기와 검증</span>"
    ]
  },
  {
    "objectID": "05-data.html",
    "href": "05-data.html",
    "title": "5  데이터와 타입",
    "section": "",
    "text": "5.1 데이터란 무엇인가\nAI가 생성한 계산기 코드를 분석하면서 프로그래밍의 기본 요소인 데이터와 타입을 이해해보자.\n프로그램은 데이터를 처리하는 도구다. 계산기 예제에서:",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터와 타입</span>"
    ]
  },
  {
    "objectID": "05-data.html#sec-data-what",
    "href": "05-data.html#sec-data-what",
    "title": "5  데이터와 타입",
    "section": "",
    "text": "입력 데이터: 사용자가 입력한 숫자 (2, 3)\n출력 데이터: 계산 결과 (5)\n\n#| eval: false\ndef add(a, b):\n    return a + b\n\n# a와 b는 입력 데이터, 결과는 출력 데이터\nresult = add(2, 3)\nprint(f\"결과: {result}\")",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터와 타입</span>"
    ]
  },
  {
    "objectID": "05-data.html#sec-data-variable",
    "href": "05-data.html#sec-data-variable",
    "title": "5  데이터와 타입",
    "section": "5.2 변수: 데이터에 이름 붙이기",
    "text": "5.2 변수: 데이터에 이름 붙이기\na, b, result는 모두 변수(Variable)다. 변수는 데이터를 담는 이름표가 붙은 상자라고 생각하면 된다.\n#| eval: false\n# 변수에 값 할당\nfirst_number = 10\nsecond_number = 20\n\n# 변수 사용\ntotal = add(first_number, second_number)\nprint(f\"{first_number} + {second_number} = {total}\")\n\n5.2.1 변수 명명 규칙\n좋은 변수 이름은 코드의 의도를 드러낸다:\n#| eval: false\n# 나쁜 예\nx = 100\ny = 15\nz = x + y\n\n# 좋은 예\nprice = 100\ntax_rate = 15\ntotal_price = price + tax_rate",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터와 타입</span>"
    ]
  },
  {
    "objectID": "05-data.html#sec-data-type",
    "href": "05-data.html#sec-data-type",
    "title": "5  데이터와 타입",
    "section": "5.3 타입: 데이터의 종류",
    "text": "5.3 타입: 데이터의 종류\nPython의 기본 데이터 타입:\n\n5.3.1 숫자 타입\n#| eval: false\n# 정수 (int)\ncount = 42\nprint(f\"count의 타입: {type(count)}\")\n\n# 실수 (float)\nprice = 19.99\nprint(f\"price의 타입: {type(price)}\")\n\n\n5.3.2 문자열 타입\n\n#| eval: false\n# 문자열 (str)\nname = \"계산기\"\nmessage = '두 수를 더합니다'\nprint(f\"name의 타입: {type(name)}\")\n\n\n5.3.3 리스트 타입\n여러 값을 담는 자료구조:\n#| eval: false\n# 리스트 (list)\nnumbers = [1, 2, 3, 4, 5]\nprint(f\"numbers의 타입: {type(numbers)}\")\nprint(f\"첫 번째 요소: {numbers[0]}\")",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터와 타입</span>"
    ]
  },
  {
    "objectID": "05-data.html#sec-data-extend",
    "href": "05-data.html#sec-data-extend",
    "title": "5  데이터와 타입",
    "section": "5.4 계산기 확장: 여러 숫자 더하기",
    "text": "5.4 계산기 확장: 여러 숫자 더하기\n리스트를 활용해 계산기를 확장해보자.\n\n5.4.1 테스트 먼저 작성\n#| eval: false\ndef test_add_many():\n    \"\"\"여러 숫자를 더하는 기능 명세\"\"\"\n    assert add_many([1, 2, 3]) == 6\n    assert add_many([10]) == 10\n    assert add_many([]) == 0\n    assert add_many([-1, 1, -1, 1]) == 0\n\n\n5.4.2 AI 생성 코드\n#| eval: false\ndef add_many(numbers):\n    \"\"\"리스트에 있는 모든 숫자를 더한다.\"\"\"\n    total = 0\n    for num in numbers:\n        total = total + num\n    return total\n\n# 테스트\nassert add_many([1, 2, 3]) == 6\nassert add_many([10]) == 10\nassert add_many([]) == 0\nprint(\"모든 테스트 통과!\")",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터와 타입</span>"
    ]
  },
  {
    "objectID": "05-data.html#sec-data-summary",
    "href": "05-data.html#sec-data-summary",
    "title": "5  데이터와 타입",
    "section": "5.5 핵심 정리",
    "text": "5.5 핵심 정리\n\n\n\n\n\n\n힌트데이터와 타입의 핵심\n\n\n\n\n변수 - 데이터에 의미 있는 이름을 붙인다\n타입 - 데이터의 종류를 구분한다 (정수, 실수, 문자열, 리스트)\n의미 있는 이름 - 변수명으로 코드의 의도를 전달한다",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>데이터와 타입</span>"
    ]
  },
  {
    "objectID": "06-logic.html",
    "href": "06-logic.html",
    "title": "6  논리와 흐름",
    "section": "",
    "text": "6.1 조건문: 분기 만들기\n프로그램은 순차적으로만 실행되지 않는다. 조건에 따라 다른 경로를 선택하고, 같은 작업을 반복해야 할 때가 있다. 계산기 코드를 확장하면서 조건문과 반복문을 이해해보자.\n계산기에 나눗셈 기능을 추가할 때, 0으로 나누는 경우를 처리해야 한다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>논리와 흐름</span>"
    ]
  },
  {
    "objectID": "06-logic.html#sec-logic-condition",
    "href": "06-logic.html#sec-logic-condition",
    "title": "6  논리와 흐름",
    "section": "",
    "text": "6.1.1 테스트 먼저\n#| eval: false\ndef test_divide():\n    \"\"\"나눗셈 기능 명세\"\"\"\n    assert divide(10, 2) == 5\n    assert divide(7, 2) == 3.5\n    assert divide(0, 5) == 0\n\n    # 0으로 나누기\n    assert divide(5, 0) is None  # 또는 에러 처리\n\n\n6.1.2 조건문 구현\n#| eval: false\ndef divide(a, b):\n    \"\"\"a를 b로 나눈다. b가 0이면 None 반환.\"\"\"\n    if b == 0:\n        return None\n    else:\n        return a / b\n\n# 테스트\nassert divide(10, 2) == 5\nassert divide(7, 2) == 3.5\nassert divide(5, 0) is None\nprint(\"나눗셈 테스트 통과!\")\n\n\n6.1.3 if-elif-else 구조\n여러 조건을 검사할 때:\n#| eval: false\ndef calculate(a, b, operation):\n    \"\"\"두 수와 연산자를 받아 계산한다.\"\"\"\n    if operation == \"+\":\n        return a + b\n    elif operation == \"-\":\n        return a - b\n    elif operation == \"*\":\n        return a * b\n    elif operation == \"/\":\n        if b == 0:\n            return None\n        return a / b\n    else:\n        return None  # 알 수 없는 연산자\n\n# 테스트\nassert calculate(10, 3, \"+\") == 13\nassert calculate(10, 3, \"-\") == 7\nassert calculate(10, 3, \"*\") == 30\nassert calculate(10, 2, \"/\") == 5\nprint(\"계산기 테스트 통과!\")",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>논리와 흐름</span>"
    ]
  },
  {
    "objectID": "06-logic.html#sec-logic-loop",
    "href": "06-logic.html#sec-logic-loop",
    "title": "6  논리와 흐름",
    "section": "6.2 반복문: 작업 되풀이하기",
    "text": "6.2 반복문: 작업 되풀이하기\n같은 작업을 여러 번 해야 할 때 반복문을 사용한다.\n\n6.2.1 for 반복문\n#| eval: false\ndef add_many(numbers):\n    \"\"\"리스트의 모든 숫자를 더한다.\"\"\"\n    total = 0\n    for num in numbers:\n        total = total + num\n    return total\n\nresult = add_many([1, 2, 3, 4, 5])\nprint(f\"1+2+3+4+5 = {result}\")\n\n\n6.2.2 while 반복문\n조건이 참인 동안 반복:\n#| eval: false\ndef countdown(n):\n    \"\"\"n부터 1까지 카운트다운한다.\"\"\"\n    while n &gt; 0:\n        print(n)\n        n = n - 1\n    print(\"발사!\")\n\ncountdown(3)",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>논리와 흐름</span>"
    ]
  },
  {
    "objectID": "06-logic.html#sec-logic-expression",
    "href": "06-logic.html#sec-logic-expression",
    "title": "6  논리와 흐름",
    "section": "6.3 계산기 확장: 수식 계산",
    "text": "6.3 계산기 확장: 수식 계산\n여러 연산을 연속으로 처리해보자.\n\n6.3.1 테스트 먼저\n#| eval: false\ndef test_calculate_expression():\n    \"\"\"수식 계산 명세\"\"\"\n    # \"2 + 3 * 4\" 같은 수식은 복잡하므로\n    # 단순화: [(숫자, 연산자), ...] 형태\n    assert calculate_sequence([(2, None), (3, \"+\")]) == 5  # 2 + 3\n\n\n6.3.2 구현\n#| eval: false\ndef calculate_sequence(operations):\n    \"\"\"순차적 계산을 수행한다.\n    operations: [(값, 연산자), ...] 형태\n    첫 번째 연산자는 None (초기값)\n    \"\"\"\n    if not operations:\n        return 0\n\n    result, _ = operations[0]  # 첫 번째 값이 시작점\n\n    for i in range(1, len(operations)):\n        value, operator = operations[i]\n        if operator == \"+\":\n            result = result + value\n        elif operator == \"-\":\n            result = result - value\n        elif operator == \"*\":\n            result = result * value\n        elif operator == \"/\":\n            if value != 0:\n                result = result / value\n\n    return result\n\n# 테스트: 2 + 3 - 1 = 4\nops = [(2, None), (3, \"+\"), (1, \"-\")]\nprint(f\"2 + 3 - 1 = {calculate_sequence(ops)}\")",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>논리와 흐름</span>"
    ]
  },
  {
    "objectID": "06-logic.html#sec-logic-summary",
    "href": "06-logic.html#sec-logic-summary",
    "title": "6  논리와 흐름",
    "section": "6.4 핵심 정리",
    "text": "6.4 핵심 정리\n\n\n\n\n\n\n힌트논리와 흐름의 핵심\n\n\n\n\n조건문 (if) - 조건에 따라 다른 코드를 실행\n반복문 (for, while) - 같은 작업을 여러 번 반복\n흐름 제어 - 프로그램의 실행 순서를 조절",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>논리와 흐름</span>"
    ]
  },
  {
    "objectID": "07-abstraction.html",
    "href": "07-abstraction.html",
    "title": "7  추상화",
    "section": "",
    "text": "7.1 함수: 코드에 이름 붙이기\n프로그래밍의 핵심 기술 중 하나는 추상화(Abstraction)다. 복잡한 것을 단순하게, 반복되는 것을 재사용 가능하게 만드는 것이다. 계산기 예제를 통해 함수와 모듈화를 이해해보자.\n함수는 특정 작업을 수행하는 코드 블록에 이름을 붙인 것이다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>추상화</span>"
    ]
  },
  {
    "objectID": "07-abstraction.html#sec-abstraction-function",
    "href": "07-abstraction.html#sec-abstraction-function",
    "title": "7  추상화",
    "section": "",
    "text": "7.1.1 함수의 구조\n#| eval: false\ndef add(a, b):\n    \"\"\"두 수를 더한다.\n\n    Args:\n        a: 첫 번째 숫자\n        b: 두 번째 숫자\n\n    Returns:\n        두 수의 합\n    \"\"\"\n    return a + b\n\ndef: 함수 정의 시작\nadd: 함수 이름\n(a, b): 매개변수 (입력)\n\"\"\"...\"\"\": 문서화 문자열 (docstring)\nreturn: 반환값 (출력)\n\n\n\n7.1.2 함수를 만드는 이유\n\n재사용 - 같은 코드를 여러 번 쓰지 않아도 된다\n추상화 - 복잡한 내용을 숨기고 이름으로 의미 전달\n테스트 - 독립적으로 테스트 가능\n유지보수 - 한 곳만 수정하면 모든 곳에 적용",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>추상화</span>"
    ]
  },
  {
    "objectID": "07-abstraction.html#sec-abstraction-module",
    "href": "07-abstraction.html#sec-abstraction-module",
    "title": "7  추상화",
    "section": "7.2 계산기 모듈화",
    "text": "7.2 계산기 모듈화\n지금까지 만든 계산기 함수들을 정리해보자.\n\n7.2.1 calculator.py\n#| eval: false\n# calculator.py\n\ndef add(a, b):\n    \"\"\"두 수를 더한다.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"첫 번째 수에서 두 번째 수를 뺀다.\"\"\"\n    return a - b\n\ndef multiply(a, b):\n    \"\"\"두 수를 곱한다.\"\"\"\n    return a * b\n\ndef divide(a, b):\n    \"\"\"첫 번째 수를 두 번째 수로 나눈다.\n    0으로 나누면 None을 반환한다.\n    \"\"\"\n    if b == 0:\n        return None\n    return a / b\n\ndef calculate(a, b, operation):\n    \"\"\"두 수와 연산자를 받아 계산한다.\"\"\"\n    operations = {\n        \"+\": add,\n        \"-\": subtract,\n        \"*\": multiply,\n        \"/\": divide\n    }\n    if operation in operations:\n        return operations[operation](a, b)\n    return None\n\n\n7.2.2 모듈 사용하기\n#| eval: false\n# main.py\nfrom calculator import add, subtract, calculate\n\nresult = calculate(10, 3, \"+\")\nprint(f\"10 + 3 = {result}\")",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>추상화</span>"
    ]
  },
  {
    "objectID": "07-abstraction.html#sec-abstraction-principles",
    "href": "07-abstraction.html#sec-abstraction-principles",
    "title": "7  추상화",
    "section": "7.3 좋은 함수 설계 원칙",
    "text": "7.3 좋은 함수 설계 원칙\n\n7.3.1 1. 한 가지 일만 한다\n#| eval: false\n# 나쁜 예: 너무 많은 일을 함\ndef process_and_print_and_save(data):\n    processed = data * 2\n    print(processed)\n    # save_to_file(processed)\n    return processed\n\n# 좋은 예: 각각 분리\ndef process(data):\n    return data * 2\n\ndef display(data):\n    print(data)\n\n\n7.3.2 2. 이름이 동작을 설명한다\n#| eval: false\n# 나쁜 예\ndef do_it(x, y):\n    return x + y\n\n# 좋은 예\ndef add_numbers(first, second):\n    return first + second\n\n\n7.3.3 3. 입력과 출력이 명확하다\n#| eval: false\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"두 정수를 더한다.\n\n    Args:\n        a: 첫 번째 정수\n        b: 두 번째 정수\n\n    Returns:\n        두 수의 합\n    \"\"\"\n    return a + b",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>추상화</span>"
    ]
  },
  {
    "objectID": "07-abstraction.html#sec-abstraction-test",
    "href": "07-abstraction.html#sec-abstraction-test",
    "title": "7  추상화",
    "section": "7.4 테스트와 함수",
    "text": "7.4 테스트와 함수\n좋은 함수는 테스트하기 쉽다:\n#| eval: false\ndef test_calculator():\n    \"\"\"계산기 모듈 테스트\"\"\"\n    # add\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n\n    # subtract\n    def subtract(a, b): return a - b  # 임시 정의\n    assert subtract(5, 3) == 2\n\n    # divide (0으로 나누기)\n    def divide(a, b):\n        return None if b == 0 else a / b\n    assert divide(10, 2) == 5\n    assert divide(5, 0) is None\n\n    print(\"모든 테스트 통과!\")\n\ntest_calculator()",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>추상화</span>"
    ]
  },
  {
    "objectID": "07-abstraction.html#sec-abstraction-summary",
    "href": "07-abstraction.html#sec-abstraction-summary",
    "title": "7  추상화",
    "section": "7.5 핵심 정리",
    "text": "7.5 핵심 정리\n\n\n\n\n\n\n힌트추상화의 핵심\n\n\n\n\n함수 - 반복되는 코드에 이름을 붙여 재사용\n모듈 - 관련 함수들을 파일로 묶어 정리\n단일 책임 - 함수는 한 가지 일만 잘 해야 한다\n명확한 인터페이스 - 입력과 출력이 분명해야 한다\n\n\n\n이로써 AI 시대 프로그래밍의 기초를 마쳤다. 다음 파트에서는 버전 관리와 협업을 통해 AI와 인간이 함께 코드를 발전시키는 방법을 알아본다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>추상화</span>"
    ]
  },
  {
    "objectID": "git-basics.html",
    "href": "git-basics.html",
    "title": "8  자동화된 버전 관리",
    "section": "",
    "text": "누군가 무엇을 했는지, 언제 했는지를 추적하기 위해서 버전 관리를 어떻게 사용할 수 있는지 탐색해보자. 다른 사람과 협업을 하지 않더라도, 자동화된 버전 관리가 다음 상황보다 훨씬 더 낫다:\n\n\n\n\n\n\n\nPiled Higher and Deeper by Jorge Cham, http://www.phdcomics.com/comics/archive_print.php?comicid=1531\n\n\n이전에 위와 같은 상황에 처했었다. 같은 문서에 대해서 거의 동일한 다수 버전을 관리하는 것은 우스꽝스러워 보인다. 일부 워드프로세서에는 이런 상황을 좀 더 잘 처리하도록 하는 기능이 있다. 예를 들어, 마이크로소프트 워드 “변경 내용 추적(Track Changes)”이나 구글 문서(Google Docs)의 버전 기록 기능이 그것이다.\n버전 관리 시스템은 문서의 기본 버전으로 시작한 후, 각 단계마다 변경한 이력을 저장한다. 테이프로 생각하면 이해하기 쉽다. 테이프를 되감으면 문서를 시작한 지점으로 돌아가고, 각 변경 사항을 다시 적용하면 가장 최신 버전이 된다.\n\n\n\n\n\n\n그림 8.1: 변경 사항이 순차적으로 저장된다.\n\n\n\n변경 사항을 문서 그 자체와 별개로 생각하면, 동일한 기반 문서에 서로 다른 변경 사항을 적용해보는 식으로 “재생(playback)”할 수 있고, 이를 별도의 문서 버전을 관리하는 것으로 간주할 수 있다. 예를 들어, 두 사용자가 같은 문서에 독립적으로 변경 작업을 수행할 수 있다.\n\n\n\n\n\n\n그림 8.2: 다른 버전이 저장될 수도 있다.\n\n\n\n만약 충돌이 발생하지 않는다면, 심지어 동일한 문서에 두 가지 변경 사항을 모두 적용할 수도 있다.\n\n\n\n\n\n\n\n그림 8.3: 여러 버전이 병합될 수도 있다.\n\n\n\n버전 관리 시스템은 사용자를 대신해서 변경 사항을 기록하고, 파일 버전을 생성하며 파일을 병합하는 데 유용한 도구이다. 버전 관리 시스템을 사용하면 어떤 변경 사항을 다음 버전에 반영할지 결정할 수 있는데, 이를 커밋(commit)이라고 부른다. 또한 커밋에 관한 유용한 메타 정보도 보관한다. 특정 프로젝트와 프로젝트 메타 정보에 대한 완전한 커밋 이력은 저장소(repository)에 보관된다. 저장소는 협업하는 여러 동료의 컴퓨터 간에 동기화될 수 있다. \n\n\n\n\n\n\n힌트버전 관리 시스템의 오랜 역사\n\n\n\n\n자동화된 버전 관리 시스템은 전혀 새로운 것이 아니다. 1980년대부터 RCS, CVS, Subversion 같은 도구가 존재했고, 많은 대기업에서 사용되어 왔다. 하지만 다양한 기능의 한계로 인해 이들 중 다수는 이제 레거시 시스템(legacy system)으로 간주된다. 최근에 등장한 Git과 Mercurial 같은 도구는 분산(distributed) 기능을 제공한다. 이는 저장소를 반드시 중앙 서버에 둘 필요가 없다는 의미이다. 이러한 최신 시스템에는 동시에 여러 저자가 동일한 파일을 편집하는 것을 가능하게 하는 강력한 병합(merge) 도구도 내장되어 있다.\n\n\n\n\n\n\n\n\n힌트논문 작성 시 버전 관리\n\n\n\n\n\n논문을 작성하면서 정말 멋진 문단을 초안으로 작성했지만, 나중에 망쳐버렸다고 상상해 보자. 어떻게 해야 정말 멋진 결론 부분이 포함된 문서 버전을 되살릴 수 있을까? 과연 가능할까?\n공동 저자가 5명 있다고 가정해 보자. 이들이 논문에 반영한 변경 사항과 의견(comment)을 어떻게 관리할 수 있을까? 마이크로소프트 워드나 리브레오피스 Writer를 사용한다면 변경 내용 추적 기능으로 변경 사항을 반영하면 어떻게 될까? 이러한 변경 내역을 계속 보관하고 있을 수 있을까?",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>자동화된 버전 관리</span>"
    ]
  },
  {
    "objectID": "git-setup-create.html",
    "href": "git-setup-create.html",
    "title": "9  Git 설정과 저장소",
    "section": "",
    "text": "9.1 Git 추가 설정\n처음 Git을 새로운 컴퓨터에서 사용할 때는 몇 가지 설정이 필요하다. 다음은 Git을 시작할 때 필수적으로 설정해야 하는 몇 가지 사례다.\n명령 줄에서 Git 명령어는 다음과 같이 작성된다. git verb options. 여기서 verb는 실제로 수행하고자 하는 명령어이고, options는 verb에 필요할 수 있는 추가 선택 사항 정보가 된다. 다음은 드라큘라(Dracula)가 새로 구입한 노트북에서 환경을 설정하는 방법이다.\n드라큘라(Dracula) 대신에 본인의 이름과 이메일 주소를 사용한다. 사용자명과 이메일 주소는 후속 Git 활동과 연관된다. 이것이 의미하는 바는 GitHub, BitBucket, GitLab 등 Git 호스팅 서버에 푸시하는 모든 변경 사항에 사용자명과 이메일 주소가 포함됨을 의미한다.\n이번 학습에서는 GitHub을 사용하는데, 여기에 사용되는 이메일 주소는 GitHub 계정 설정 시 사용한 것과 동일해야 한다. 만약 개인정보가 걱정된다면 GitHub의 이메일 주소 비공개 지침을 참조하자. GitHub에서 비공개 이메일 주소를 선택했다면 user.email에도 동일한 주소를 사용해야 한다. 즉, GitHub 설정의 username을 username@users.noreply.github.com으로 바꾼다. 나중에 git config 명령어로 이메일 주소를 변경할 수 있다.\n드라큘라도 선호하는 텍스트 편집기를 설정해야 하는데, 다음 표를 참조하면 된다.\n필요할 때마다 Git에서 사용할 텍스트 편집기 설정을 변경할 수 있다.\n앞서 실행한 상기 명령어는 한번만 실행하면 된다. --global 플래그는 Git으로 하여금 해당 컴퓨터에 본인 계정의 모든 프로젝트에 환경설정한 것을 사용하도록 한다.\n본인이 설정한 환경설정 내용은 언제라도 다음 명령어를 입력하여 확인할 수 있다.\n원하는 대로 설정을 변경할 수 있다. 편집기를 바꾸거나 이메일 주소를 업데이트할 때도 동일한 명령어를 사용하면 된다.",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Git 설정과 저장소</span>"
    ]
  },
  {
    "objectID": "git-setup-create.html#git-korean",
    "href": "git-setup-create.html#git-korean",
    "title": "9  Git 설정과 저장소",
    "section": "",
    "text": "9.1.1 로컬 PC와 SSH 키 연결\n\nGitHub에 저장소(repository)를 만들고 여러 PC에서 작업할 경우, GitHub 인증 작업을 거치는 것이 여러모로 편리하다. 그 중 하나는 공개 키(public key)를 GitHub에 등록하여 작업하는 방식이다.\n\n윈도우 사용자는 먼저 Git for Windows를 다운로드하여 설치한다.\nssh-keygen 명령어로 공개 키와 비밀 키를 생성한다.\n생성된 공개 키를 GitHub 계정에 등록한다.\n\n\n\n9.1.2 SSH 공개 키/비밀 키 생성\nSSH 공개 키와 비밀 키를 생성하고 GitHub에 등록하는 과정은 다음과 같다. 먼저 ssh-keygen 명령어에 매개변수와 GitHub 이메일 주소를 지정한다. 1\n$ ssh-keygen -t rsa -C \"your_email@example.com\" \nssh-keygen으로 생성된 키를 GitHub에 등록한다.\n\n우측 상단 [Settings] → [SSH and GPG keys] → [New SSH key]\n\n[New SSH key]를 클릭하면 Title과 Key 입력란이 나타난다. Title에는 식별 가능한 이름을 지정하고, 앞서 생성한 id_rsa.pub 내용을 Key에 복사하여 붙여넣는다.\n$ cat ~/.ssh/id_rsa.pub\n\nssh-rsa AAAxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxYxY9 email_address@mail.com\n\n\n9.1.3 첫 커밋(commit)\n인증을 완료하고 저장소에서 처음 파일을 커밋할 때 git add, git commit -m 명령어를 연속해서 실행하면 커밋 작성자 정보를 등록하라는 메시지가 나타난다. git config로 이메일과 사용자명을 설정하면 정상적으로 커밋할 수 있다.\n$ git config --global user.email \"you@example.com\"\n$ git config --global user.name \"Your Name\"\n\n\n9.1.4 비밀번호 없이 푸시하기\n다음 단계로 비밀번호 없이 커밋된 내용을 GitHub에 전달하는 방법은 자격인증(credential) 캐싱을 통한 간단한 방법이 있다. 물론 처음에는 사용자명과 비번을 입력하는 과정을 필수적으로 거치게 된다.2\n$ git config credential.helper store\n$ git push https://github.com/repo.git\n\nUsername for 'https://github.com': &lt;USERNAME&gt;\nPassword for 'https://USERNAME@github.com': &lt;PASSWORD&gt; \n보안을 강화하기 위해 캐시 시간 제한을 7200초(2시간)로 설정할 수 있다.\n$ git config --global credential.helper 'cache --timeout 7200'",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Git 설정과 저장소</span>"
    ]
  },
  {
    "objectID": "git-setup-create.html#git-create",
    "href": "git-setup-create.html#git-create",
    "title": "9  Git 설정과 저장소",
    "section": "9.2 저장소 생성",
    "text": "9.2 저장소 생성\n\nGit 환경설정이 완료되면, Git를 사용할 수 있다. 행성 착륙선을 화성에 보낼 수 있는지 조사를 하고 있는 늑대인간과 드라큘라 이야기를 계속해서 진행해 보자.\n\n\n\n\n\n\n그림 9.1: Git 동기 부여 사례\n\n\n\n먼저 바탕화면(Desktop)에 작업할 디렉토리를 생성하고, 생성한 디렉토리로 이동하자:\n$ cd ~/Desktop\n$ mkdir planets\n$ cd planets\n그리고 나서, planets을 저장소(repository)로 만든다. 저장소는 Git이 파일에 대한 버전 정보를 저장하는 장소다.\n$ git init\ngit init 명령어가 서브디렉토리(subdirectory)와 파일을 담고 있는 저장소를 생성하는데 주목한다. planets 저장소 내부에 중첩된 별도 저장소를 생성할 필요는 없다. 또한, planets 디렉토리를 생성하고 저장소로 초기화하는 것은 완전히 서로 다른 과정이다.\nls를 사용해서 디렉토리 내용을 살펴보면, 변한 것이 아무것도 없는 것처럼 보인다:\n$ ls\n하지만, 모든 것을 보여주는 -a 플래그를 추가하면, Git은 planets 디렉토리 내부에 .git 로 불리는 숨겨진 디렉토리를 생성한 것을 볼 수 있다:\n$ ls -a\n\n.   ..  .git\nGit은 .git이라는 특별한 하위 디렉토리에 프로젝트에 대한 정보를 저장한다. 여기에는 프로젝트 디렉토리 내부에 위치한 모든 파일과 서브 디렉토리가 포함된다. 만약 .git를 삭제하면, 프로젝트 이력을 모두 잃어버리게 된다.\n모든 것이 제대로 설정되었는지를 확인하려면, Git에게 다음과 같이 프로젝트 상태를 확인 명령어를 던진다:\n$ git status\n\n# On branch master\n#\n# Initial commit\n#\nnothing to commit (create/copy files and use \"git add\" to track)\n다른 git 버전을 사용할 경우, 출력 결과물이 다소 다를 수 있다.\n\n\n\n\n\n\n경고Git 저장소를 생성할 장소\n\n\n\n(이미 생성한 프로젝트) 행성에 대한 정보를 추적하면서, 드라큘라는 달에 관한 정보도 추적하고자 한다. planets 프로젝트와 관련된 새로운 프로젝트 moons를 시작한다. 늑대인간의 걱정에도 불구하고, Git 저장소 내부에 또 다른 Git 저장소를 생성하려고 다음 순서로 명령어를 입력해 나간다.\n$ cd ~/Desktop   # 바탕화면 디렉토리로 되돌아 간다.\n$ cd planets     # planets 디렉토리로 들어간다.\n$ ls -a          # planets 디렉토리에 .git 서브 디렉토리가 있는지 확인한다.\n$ mkdir moons    # planets/moons 서브 디렉토릴르 생성한다.\n$ cd moons       # moons 서브 디렉토리로 이동한다.\n$ git init       # Git 저장소를 moons 하위디렉토리에 생성한다.\n$ ls -a          # 새로운 Git 저장소가 .git 하위 디렉토리에 있는지 확인한다.\nmoons 서브 디렉토리에 저장된 파일을 추적하기 위해 moons 디렉토리 안에서 git init 명령을 실행해야 할까?\n\n\n\n\n\n\n노트해답\n\n\n\n\n\n아니다. moons 서브 디렉토리에 Git 저장소를 만들 필요는 없다. 왜냐하면, planets 저장소가 이미 모든 파일, 서브 디렉토리, planets 디렉토리 아래 서브 디렉토리 파일 모두를 추적하기 때문이다. 따라서, 달에 관한 모든 정보를 추정하는데, 드랴큘라는 planets 디렉토리 아래 moons 서브 디렉토리를 추가하는 것으로 충분하다.\n추가적으로, 만약 Git 저장소가 중첩(nested)되면, Git 저장소는 서로 방해할 수 있다. 바깥 저장소가 내부 저장소 버전관리를 하게 된다. 따라서, 별도 디렉토리에 서로 다른 신규 Git 저장소를 생성하는게 최선이다.\n디렉토리에 저장소가 서로 충돌하지 않도록 하려면, git status 출력물을 점검하면 된다. 만약, 다음과 같은 출력물이 생성되게 되면 신규 저장소를 생성하는 것이 권장된다:\n$ git status\n\nfatal: Not a git repository (or any of the parent directories): .git\n\n\n\n\n\n\n\n\n\n\n\n주의git init 실수 올바르게 고치기\n\n\n\n늑대인간은 드라큘라에게 중첩된 저장소가 중복되어 불필요할 뿐만 아니라 향후 혼란을 야기할 수 있다고 설명했다. 드라큘라는 중첩된 저장소를 제거하고자 한다.\nmoons 서브 디렉터리에서 마지막으로 실행한 git init 명령어를 어떻게 취소할 수 있을까?\n\n\n\n\n\n\n노트해답 – 주의해서 사용할 것!\n\n\n\n\n\n이러한 사소한 실수를 되돌리려면, 드라큘라는 planets 디렉토리에서 다음 명령어를 실행하여 .git 디렉토리를 제거하기만 하면 된다:\n$ rm -rf moons/.git\n하지만, 주의할 것! 디렉토리를 잘못 입력하게 되면, 보관해야 하는 프로젝트 정보를 담고 있는 Git 이력 전체가 삭제될 수 있다. 따라서, pwd 명령어를 사용해서 현재 작업 디렉토리를 항상 확인한다.",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Git 설정과 저장소</span>"
    ]
  },
  {
    "objectID": "git-setup-create.html#footnotes",
    "href": "git-setup-create.html#footnotes",
    "title": "9  Git 설정과 저장소",
    "section": "",
    "text": "nickjoIT (2017), “GitHub SSH 키 생성 및 등록하여 사용하기”↩︎\nStackoverflow, “How do I avoid the specification of the username and password at every git push?”↩︎",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Git 설정과 저장소</span>"
    ]
  },
  {
    "objectID": "git-change.html",
    "href": "git-change.html",
    "title": "10  변경사항 추적",
    "section": "",
    "text": "먼저 디렉토리 위치가 맞는지 확인하자. planets 디렉토리에 위치해야 한다.\n$ pwd\n\n/home/vlad/Desktop/planets\nmoons 디렉토리에 여전히 있다면, planets 디렉토리로 되돌아간다.\n$ pwd\n\n/home/vlad/Desktop/planets/moons\n$ cd ..\n전진기지로서 화성의 적합성에 관한 기록을 담고 있는 mars.txt 파일을 생성한다. (파일 편집을 위해서 nano 편집기를 사용한다; 원하는 어떤 편집기를 사용해도 된다. 특히, 앞에서 전역으로 설정한 core.editor일 필요는 없다. 하지만, 파일을 새로 생성하거나 편집할 때 배쉬 명령어는 사용자가 선택한 편집기에 의존하게 된다. (nano일 필요는 없다.) 텍스트 편집기에 대한 환기로, 챗GPT 유닉스 쉘의 “어떤 편집기가 좋을까요?” 부분을 참고한다.\n$ nano mars.txt\nmars.txt 파일에 다음 텍스트를 타이핑한다:\nCold and dry, but everything is my favorite color\nmars.txt 파일은 이제 한 줄을 포함하게 되어서, 다음 명령어로 내용을 확인할 수 있다:\n$ ls\n\nmars.txt\n$ cat mars.txt\n\nCold and dry, but everything is my favorite color\n다시 한번 프로젝트의 상태를 확인하고자 하면, Git이 새로운 파일이 인지되었다고 알려준다: \n$ git status\n\nOn branch master\n\nInitial commit\n\nUntracked files:\n   (use \"git add &lt;file&gt;...\" to include in what will be committed)\n\n    mars.txt\nnothing added to commit but untracked files present (use \"git add\" to track)\n“untracked files” 메시지가 의미하는 것은 Git이 추적하고 있지 않는 파일 하나가 디렉토리에 있다는 것이다. git add를 사용해서 Git에게 추적관리하라고 알려준다: \n$ git add mars.txt\n그리고 나서, 올바르게 처리되었는지 확인한다:\n$ git status\n\nOn branch master\n\nInitial commit\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n\n    new file:   mars.txt\n이제 Git은 mars.txt 파일을 추적할 것이라는 것을 알고 있지만, 커밋으로 아직 저장소에는 어떤 변경사항도 기록되지 않았다. 이를 위해서 명령어 하나 더 실행할 필요가 있다: \n$ git commit -m \"Start notes on Mars as a base\"\n\n[master (root-commit) f22b25e] Start notes on Mars as a base\n 1 file changed, 1 insertion(+)\n create mode 100644 mars.txt\ngit commit을 실행할 때, Git은 git add를 사용해서 저장하려고 하는 모든 대상을 받아서 .git 디렉토리 내부에 영구적으로 사본을 저장한다. 이 영구 사본을 커밋(commit) (혹은 수정(revision))이라고 하고, 짧은 식별자는 f22b25e이다. (여러분의 커밋번호의 짧은 식별자는 다를 수 있다.)\n-m (“message”를 의미) 플래그를 사용해서 나중에 무엇을 왜 했는지 기억에 도움이 될 수 있는 주석을 기록한다. -m 옵션 없이 git commit을 실행하면, Git은 nano (혹은 처음에 core.editor에서 설정한 다른 편집기)를 실행해서 좀 더 긴 메시지를 작성할 수 있다.\n좋은 커밋 메시지(Good commit messages) 작성은 커밋으로 만들어진 간략한 (영문자 기준 50문자 이하) 변경사항 요약으로 시작된다. 일반적으로 메시지는 완전한 문장이 되어야 한다. 예를 들어, “If applied, this commit will” . 만약 좀 더 상세한 사항을 남기려면, 요약줄 사이에 빈줄을 추가하고 추가적인 내역을 적는다. 추가되는 공간에 왜 변경을 하는지 사유를 남기고, 어떤 영향을 미치는지도 기록한다.\n이제 git status를 실행하면:\n$ git status\n\nOn branch master\nnothing to commit, working directory clean\n모든 것이 최신 상태라고 보여준다. 최근에 작업한 것을 알고자 한다면, git log를 사용해서 프로젝트 이력을 보여주도록 Git에게 명령어를 보낸다:\n$ git log\n\ncommit f22b25e3233b4645dabd0d81e651fe074bd8e73b\nAuthor: Vlad Dracula &lt;vlad@tran.sylvan.ia&gt;\nDate:   Thu Aug 22 09:51:46 2013 -0400\n\n    Start notes on Mars as a base\ngit log는 시간 역순으로 저장소의 모든 변경사항을 나열한다. 각 수정사항 목록은 전체 커밋 식별자(앞서 git commit 명령어로 출력한 짧은 문자와 동일하게 시작), \n수정한 사람, 언제 생성되었는지, 커밋을 생성할 때 Git에 남긴 로그 메시지가 포함된다.\n\n\n\n\n\n\n주의내가 작성한 변경사항은 어디에 있나?\n\n\n\n이 시점에서 ls 명령어를 다시 실행하면, mars.txt 파일만 덩그러니 보게 된다. 왜냐하면, Git이 앞에서 언급한 .git 특수 디렉토리에 파일 변경 이력 정보를 저장했기 때문이다. 그래서 파일 시스템이 뒤죽박죽되지 않게 된다. (따라서, 옛 버전을 실수로 편집하거나 삭제할 수 없다.)\n\n\n이제 드라큘라가 이 파일에 정보를 더 추가했다고 가정하자. (다시 한번 nano 편집기로 편집하고 나서 cat으로 파일 내용을 살펴본다. 다른 편집기를 사용할 수도 있고, cat으로 파일 내용을 꼭 볼 필요도 없다.)\n$ nano mars.txt\n$ cat mars.txt\n\nCold and dry, but everything is my favorite color\nThe two moons may be a problem for Wolfman\ngit status를 실행하면, Git이 이미 알고 있는 파일이 변경되었다고 알려준다:\n$ git status\n\nOn branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n\n    modified:   mars.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n마지막 줄이 중요한 문구다: “no changes added to commit”. mars.txt 파일을 변경했지만, 아직 Git에게는 변경사항을 저장하려고 하거나 (git add로 수행), 저장소에 저장하라고 (git commit로 수행) 알려주지 않았다. 이제 행동에 나서보자. 저장하기 전에 변경사항을 항상 검토하는 것은 좋은 습관이다. git diff를 사용해서 작업 내용을 두 번 검증한다. git diff는 현재 파일의 상태와 가장 최근에 저장된 버전의 차이를 보여준다:\n$ git diff\n\ndiff --git a/mars.txt b/mars.txt\nindex df0654a..315bf3a 100644\n--- a/mars.txt\n+++ b/mars.txt\n@@ -1 +1,2 @@\n Cold and dry, but everything is my favorite color\n+The two moons may be a problem for Wolfman\n출력 결과가 암호 같은데 이유는 한 파일이 주어졌을 때 다른 파일 하나를 어떻게 재구성하는지를 알려주는 patch와 편집기 같은 도구를 위한 일련의 명령어라서 그렇다. 만약 해당 내역을 조각내서 쪼개다면:\n\n첫 번째 행은 Git이 신규 파일과 옛 버전 파일을 비교하는 유닉스 diff 명령어와 유사한 출력 결과를 생성하고 있다.\n두 번째 행은 정확하게 Git이 파일 어느 버전을 비교하는지 알려준다; df0654a와 315bf3a은 해당 버전에 대해서 중복되지 않게 컴퓨터가 생성한 표식이다.\n세 번째와 네 번째 행은 변경되는 파일 명칭을 다시 한번 보여주고 있다.\n\n나머지 행이 가장 흥미롭다. 실제 차이가 나는 것과 어느 행에서 발생했는지 보여준다. 특히 첫 번째 열의 + 기호는 어디서 행이 추가되었는지 보여준다.\n\n변경사항 검토 후에, 변경사항을 커밋(commit)하자.\n$ git commit -m \"Add concerns about effects of Mars' moons on Wolfman\"\n$ git status\n\nOn branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n\n    modified:   mars.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n이럴 수가, git add를 먼저 하지 않아서 Git이 커밋을 할 수 없다. 고쳐보자.\n$ git add mars.txt\n$ git commit -m \"Add concerns about effects of Mars' moons on Wolfman\"\n\n[master 34961b1] Add concerns about effects of Mars' moons on Wolfman\n 1 file changed, 1 insertion(+)\n실제로 무엇을 커밋하기 전에 커밋하고자 하는 파일을 먼저 추가하라고 Git이 주문하는데, 이유는 한번에 모든 것을 커밋하고 싶지 않을 수도 있기 때문이다. 예를 들어, 작성하고 있는 논문에 지도교수 논문을 일부 인용하여 추가한다고 가정하자. 논문 중간에 인용되는 추가부분과 상응되는 참고문헌을 커밋하고는 싶지만, 결론 부분을 커밋하고는 싶지 않다. (아직 결론이 완성되지 않았다.)\n이런 점을 고려해서, Git은 특별한 준비 영역(staging)이 있어서 현재 변경부분(change set)을 추가는 했으나 아직 커밋하지 않는 것을 준비 영역에서 추적하고 있다.\n\n\n\n\n\n\n주의준비 영역(Staging area)\n\n\n\n프로젝트 기간 동안에 걸쳐 발생된 변경사항에 대해 스냅사진을 찍는 것으로 Git을 바라보면, git add 명령어는 무엇이 스냅사진(준비영역에 놓는 것)에 들어갈지 명세하고, git commit 명령어는 실제로 스냅사진을 찍는 것이다. 만약 git commit을 타이핑할 때 준비된 어떤 것도 없다면, Git이 git commit -a 혹은 git commit --all 명령어 사용을 재촉한다. 사진을 찍으려고 모두 모이세요 하는 것과 같다. 하지만, 준비영역에 추가할 것을 명시적으로 하는 것이 항상 좋다. 왜냐하면 커밋을 했는데 잊은 것이 있을 수도 있기 때문이다. (스냅사진으로 돌아가서, -a 옵션을 사용했기 때문에 스냅사진에 들어갈 항목을 불완전하게 작성했을 수도 있다!) 수작업으로 준비영역에 올리거나, 원하는 것보다 많은 것을 올렸다면 “git undo commit”을 찾아보라.\n\n\n\n\n\n\n\n\n그림 10.1: Git 준비(Staging) 영역\n\n\n\n파일 변경사항을 편집기에서 준비 영역으로, 그리고 장기 저장소로 옮기는 것을 살펴보자. 먼저, 파일에 행 하나를 더 추가한다. \n$ nano mars.txt\n$ cat mars.txt\n\nCold and dry, but everything is my favorite color\nThe two moons may be a problem for Wolfman\nBut the Mummy will appreciate the lack of humidity\n$ git diff\n\ndiff --git a/mars.txt b/mars.txt\nindex 315bf3a..b36abfd 100644\n--- a/mars.txt\n+++ b/mars.txt\n@@ -1,2 +1,3 @@\n Cold and dry, but everything is my favorite color\n The two moons may be a problem for Wolfman\n+But the Mummy will appreciate the lack of humidity\n지금까지 좋다. 파일의 끝에 행을 하나 추가했다(첫 열에 +이 보인다). 이제, 준비영역에 변경사항을 놓고, git diff 명령어가 보고하는 것을 살펴보자.\n$ git add mars.txt\n$ git diff\n출력결과가 없다. Git이 알려줄 수 있는 것은 영구히 저장되는 것과 현재 디렉토리에 작업하고 있는 것에 차이가 없다는 것이다. 하지만, 다음과 같이 명령어를 친다면:\n$ git diff --staged\n\ndiff --git a/mars.txt b/mars.txt\nindex 315bf3a..b36abfd 100644\n--- a/mars.txt\n+++ b/mars.txt\n@@ -1,2 +1,3 @@\n Cold and dry, but everything is my favorite color\n The two moons may be a problem for Wolfman\n+But the Mummy will appreciate the lack of humidity\n마지막으로 커밋된 변경사항과 준비 영역(Staging)에 있는 것과 차이를 보여준다. 변경사항을 저장하자:\n$ git commit -m \"Discuss concerns about Mars' climate for Mummy\"\n\n[master 005937f] Discuss concerns about Mars' climate for Mummy\n 1 file changed, 1 insertion(+)\n현재 상태를 확인하자:\n$ git status\n\nOn branch master\nnothing to commit, working directory clean\n그리고 지금까지 작업한 이력을 살펴보자:\n$ git log\n\ncommit 005937fbe2a98fb83f0ade869025dc2636b4dad5\nAuthor: Vlad Dracula &lt;vlad@tran.sylvan.ia&gt;\nDate:   Thu Aug 22 10:14:07 2013 -0400\n\n    Discuss concerns about Mars' climate for Mummy\n\ncommit 34961b159c27df3b475cfe4415d94a6d1fcd064d\nAuthor: Vlad Dracula &lt;vlad@tran.sylvan.ia&gt;\nDate:   Thu Aug 22 10:07:21 2013 -0400\n\n    Add concerns about effects of Mars' moons on Wolfman\n\ncommit f22b25e3233b4645dabd0d81e651fe074bd8e73b\nAuthor: Vlad Dracula &lt;vlad@tran.sylvan.ia&gt;\nDate:   Thu Aug 22 09:51:46 2013 -0400\n\n    Start notes on Mars as a base\n\n\n\n\n\n\n힌트단어 단위 차이분석(Word-based diffing)\n\n\n\n경우에 따라서는 줄 단위로 텍스트 차이 분석이 너무 자세하지 않을 수도 있다. git diff 명령어에 --color-words 선택옵션이 유용할 수 있는데 이유는 색상을 사용해서 변경된 단어를 강조해서 표시해 주기 때문이다.\n\n\n\n\n\n\n\n\n힌트로그 페이지별 보기\n\n\n\n화면에 git log 출력결과가 너무 긴 경우, git에 화면 크기에 맞춰 페이지 단위로 쪼개주는 프로그램이 제공된다. 페이지별 쪼개보기(“pager”)가 호출되면, 화면 마지막 줄에 프롬프트 대신에 :이 나타난다.\n\n페이저(pager)에서 나오려면, Q를 타이핑한다.\n다음 페이지로 이동하려면, Spacebar를 타이핑한다.\n전체 페이지에서 특정 단어를 검색하려면,\n/를 타이핑하고, 특정 단어를 검색하는 검색어를 타이핑한다. 검색에 매칭되는 단어를 따라가려면 N을 타이핑한다.\n\n\n\n\n\n\n\n\n\n힌트로그 크기 제한걸기\n\n\n\ngit log가 전체 터미널 화면을 뒤덮는 것을 피하려면, -N 선택옵션을 적용해서 Git이 화면에 출력하는 커밋 숫자에 제한을 건다. 여기서 -N은 보고자 하는 커밋 개수가 된다. 예를 들어 가장 마지막 커밋만 보려고 한다면 다음과 같이 타이핑한다:\n$ git log -1\n\ncommit 005937fbe2a98fb83f0ade869025dc2636b4dad5\nAuthor: Vlad Dracula &lt;vlad@tran.sylvan.ia&gt;\nDate:   Thu Aug 22 10:14:07 2013 -0400\n   Discuss concerns about Mars' climate for Mummy\n--oneline 선택옵션을 사용해서 출력되는 로그 메시지 크기를 줄일 수도 있다:\n$ git log --oneline\n\n* 005937f Discuss concerns about Mars' climate for Mummy\n* 34961b1 Add concerns about effects of Mars' moons on Wolfman\n* f22b25e Start notes on Mars as a base\n--oneline 선택옵션과 다른 선택옵션을 조합할 수도 있다. 유용한 조합 사례로 다음이 있다:\n$ git log --oneline --graph --all --decorate\n\n* 005937f Discuss concerns about Mars' climate for Mummy (HEAD, master)\n* 34961b1 Add concerns about effects of Mars' moons on Wolfman\n* f22b25e Start notes on Mars as a base\n\n\n\n\n\n\n\n\n힌트디렉토리\n\n\n\nGit에서 디렉토리에 관해서 알아두면 좋을 두 가지 사실. \n\nGit은 그 자체로 디렉토리를 추적하지 않고, 디렉토리에 담긴 파일만 추적한다. 믿지 못하겠다면, 직접 다음과 같이 시도해 본다:\n\n$ mkdir directory\n$ git status\n$ git add directory\n$ git status\n새로 생성된 directory 이름을 갖는 디렉토리가 git add 명령어로 명시적으로 추가했음에도 불구하고 untracked files 목록에 나오지 않고 있다. 이런 이유로 인해서 가끔 .gitkeep 파일을 보게 된다. .gitignore와 달리, 특별하지는 않고 유일한 목적은 디렉토리를 만들어 내어 Git이 저장소에 추가하도록 하는 역할만 수행한다. 사실 원하는 이름으로 파일명을 붙일 수 있다.\n\nGit 저장소에 디렉토리를 생성하고 파일로 채워넣으면, 다음과 같이 디렉토리의 모든 파일을 추가할 수 있다:\n\ngit add &lt;directory-with-files&gt;\n직접 실습해보자.\n$ touch spaceships/apollo-11 spaceships/sputnik-1\n$ git status\n$ git add spaceships\n$ git status\n다음으로 넘어가기 전에, 이러한 변경사항을 커밋한다.\n$ git commit -m \"Add some initial thoughts on spaceships\"\n\n\n요약하면, 변경사항을 저장소에 추가하고자 할 때, 먼저 변경된 파일을 준비 영역(Staging)에 git add 명령어로 추가하고 나서, 준비 영역의 변경사항을 저장소에 git commit 명령어로 최종 커밋한다.\n\n\n\n\n\n\n그림 10.2: Git 커밋(Commit) 작업흐름\n\n\n\n\n\n\n\n\n\n힌트커밋 메시지 고르기\n\n\n\n\n다음 중 어떤 커밋 메시지가 mars.txt 파일의 마지막 커밋으로 가장 적절할까요?\n\n“Changes”\n“Added line ‘But the Mummy will appreciate the lack of humidity’ to mars.txt”\n“Discuss effects of Mars’ climate on the Mummy”\n\n\n\n\n\n\n\n노트해답\n\n\n\n\n\n1번은 충분히 기술되어 있지 못하고 커밋 목적이 불확실하다;\n2번은 “git diff” 명령어를 사용한 것과 불필요하게 중복된다;\n3번이 좋다: 짧고, 기술이 잘되어 있고, 피할 수 없게 명백하다(imperative).\n\n\n\n\n\n\n\n\n\n\n\n힌트Git에 변경사항 커밋하기\n\n\n\n다음 중 어떤 명령어가 로컬 Git 저장소에 myfile.txt 파일 변경사항을 저장시키는걸까?\n\n   $ git commit -m \"my recent changes\"\n   $ git init myfile.txt\n   $ git commit -m \"my recent changes\"\n   $ git add myfile.txt\n   $ git commit -m \"my recent changes\"\n   $ git commit -m myfile.txt \"my recent changes\"\n\n\n\n\n\n\n\n노트해답\n\n\n\n\n\n\n파일이 이미 준비영역(staging)에 올라온 경우만 커밋이 생성된다.\n신규 저장소를 생성하게 된다.\n정답: 파일을 준비영역에 추가하고 나서, 커밋하게 된다.\nmyfile.txt 파일에 “my recent changes” 메시지를 갖는 커밋을 생성한다.\n\n\n\n\n\n\n\n\n\n\n\n\n힌트파일 다수를 커밋\n\n\n\n준비영역(staging area)은 스냅샷 한 번에 원하는 만큼 파일의 변경사항을 담아낼 수 있다. 1. mars.txt 파일에 전진기지로 생각하는 금성(Venus)를 고려하고 있다는 결정을 담은 텍스트를 추가한다. 2. venus.txt 파일을 새로 생성해서 본인과 친구들에게 금성에 관한 첫생각을 담아낸다. 3. 파일 두 개에 변경사항을 준비영역에 추가하고 커밋한다.\n\n\n\n\n\n\n노트해답\n\n\n\n\n\n먼저, mars.txt, venus.txt 파일에 변경사항을 기록한다:\n$ nano mars.txt\n$ cat mars.txt\n\nMaybe I should start with a base on Venus.\n$ nano venus.txt\n$ cat venus.txt\n\nVenus is a nice planet and I definitely should consider it as a base.\n준비영역에 파일 두 개를 추가한다. 한 줄로 추가 작업을 수행할 수 있다:\n$ git add mars.txt venus.txt\n혹은 명령어를 여러 번 타이핑하면 된다:\n$ git add mars.txt\n$ git add venus.txt\n이제 파일을 커밋할 준비가 되었다. git status를 사용해서 확인하면, 커밋을 할 준비가 되었다:\n$ git commit -m \"Write plans to start a base on Venus\"\n[master cc127c2]\nWrite plans to start a base on Venus\n2 files changed, 2 insertions(+)\ncreate mode 100644 venus.txt\n\n\n\n\n\n\n\n\n\n\n\n힌트bio 저장소\n\n\n\n\nbio라는 새로운 Git 저장소를 본인 로컬 컴퓨터에 생성한다.\nme.txt라는 파일로 본인에 대한 3줄 이력서를 작성한다. 변경사항을 커밋한다.\n\n그리고 나서 한 줄을 바꾸고, 네 번째 줄을 추가하고 나서,\n원래 상태와 갱신된 상태의 차이를 화면에 출력한다.\n\n\n\n\n\n\n\n노트해답\n\n\n\n\n\n필요하다면, planets 폴더에서 빠져나온다:\n$ cd ..\nbio 폴더를 새로 생성하고 bio 폴더로 이동한다:\n$ mkdir bio\n$ cd bio\ngit 명령어로 초기화한다:\n$ git init\nnano 혹은 선호하는 편집기를 사용해서 me.txt 파일에 본인 일대기를 작성한다. 파일을 추가하고 나서, 저장소에 커밋한다:\n$ git add me.txt\n$ git commit -m'Adding biography file'\n기술된 것(한 줄 변경하고, 4번째 줄을 추가한다)처럼 파일을 변경한다. 원본 상태와 수정된 상태를 git diff 명령어를 사용해서 화면에 출력한다:\n$ git diff me.txt\n\n\n\n\n\n\n\n\n\n\n\n힌트저자(Author)와 커미터(Committer)\n\n\n\n매번 커밋을 할 때마다, Git은 이름을 두 번 저장한다. 본인 이름이 저자(Author)와 커미터(Committer)로 기록된다. 마지막 커밋에 추가 정보를 Git에게 요구하면 확인이 가능하다:\n$ git log --format=full\n커밋할 때, 저자를 다른 누군가로 바꿀 수 있다:\n$ git commit --author=\"Vlad Dracula &lt;vlad@tran.sylvan.ia&gt;\"\n커밋을 두 개 생성한다. 하나는 --author 옵션을 갖는 것으로 저자로 동료 이름을 반영한다. git log와 git log --format=full 명령어를 실행한다. 이를 통해 동료들과 어떻게 협업할 수 있을지 생각해 보자.\n\n\n\n\n\n\n노트해답\n\n\n\n\n\n$ git add me.txt\n$ git commit -m \"Update Vlad's bio.\" --author=\"Frank N. Stein &lt;franky@monster.com&gt;\"\n\n[master 4162a51] Update Vlad's bio.\nAuthor: Frank N. Stein &lt;franky@monster.com&gt;\n1 file changed, 2 insertions(+), 2 deletions(-)\n\n$ git log --format=full\ncommit 4162a51b273ba799a9d395dd70c45d96dba4e2ff\nAuthor: Frank N. Stein &lt;franky@monster.com&gt;\nCommit: Vlad Dracula &lt;vlad@tran.sylvan.ia&gt;\n\nUpdate Vlad's bio.\n\ncommit aaa3271e5e26f75f11892718e83a3e2743fab8ea\nAuthor: Vlad Dracula &lt;vlad@tran.sylvan.ia&gt;\nCommit: Vlad Dracula &lt;vlad@tran.sylvan.ia&gt;\nVlad's initial bio.",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>변경사항 추적</span>"
    ]
  },
  {
    "objectID": "git-history.html",
    "href": "git-history.html",
    "title": "11  이력 탐색",
    "section": "",
    "text": "앞선 학습에서 살펴봤듯이, 식별자로 커밋을 조회할 수 있다. HEAD 식별자를 사용해서 작업 디렉터리의 가장 최근 커밋을 조회할 수 있다.\nmars.txt 파일에 한 번에 한 줄씩 추가했다. 따라서 눈으로 봐도 진행 사항을 쉽게 추적할 수 있다. HEAD를 사용해서 추적 작업을 수행해 보자. 시작 전에 mars.txt 파일에 변경을 가해보자.\n$ nano mars.txt\n$ cat mars.txt\n\nCold and dry, but everything is my favorite color\nThe two moons may be a problem for Wolfman\nBut the Mummy will appreciate the lack of humidity\nAn ill-considered change\n이제, 변경된 사항을 살펴보자.\n$ git diff HEAD mars.txt\n\ndiff --git a/mars.txt b/mars.txt\nindex b36abfd..0848c8d 100644\n--- a/mars.txt\n+++ b/mars.txt\n@@ -1,3 +1,4 @@\n Cold and dry, but everything is my favorite color\n The two moons may be a problem for Wolfman\n But the Mummy will appreciate the lack of humidity\n+An ill-considered change.\nHEAD만 빼면, 앞서 살펴본 것과 동일하다. 이러한 접근법의 정말 좋은 점은 이전 커밋을 조회할 수 있다는 점이다.\n~1(“~”은 “틸드(tilde)”, 발음기호 [til-duh])을 추가해서 HEAD 이전 첫 번째 커밋을 조회할 수 있다. \n$ git diff HEAD~1 mars.txt\ngit diff 명령어를 사용해서 이전 커밋과 차이난 점을 보고자 한다면,\nHEAD~1, HEAD~2 표기법을 사용해서 조회를 쉽게 할 수 있다:\n$ git diff HEAD~2 mars.txt\n\ndiff --git a/mars.txt b/mars.txt\nindex df0654a..b36abfd 100644\n--- a/mars.txt\n+++ b/mars.txt\n@@ -1 +1,4 @@\n Cold and dry, but everything is my favorite color\n+The two moons may be a problem for Wolfman\n+But the Mummy will appreciate the lack of humidity\n+An ill-considered change\ngit show를 사용해서도 커밋 메시지뿐만 아니라 이전 커밋과 변경사항을 보여준다.\ngit diff는 작업 디렉터리와 커밋 사이 차이나는 부분을 보여준다. \n$ git show HEAD~2 mars.txt\n\ncommit 34961b159c27df3b475cfe4415d94a6d1fcd064d\nAuthor: Vlad Dracula &lt;vlad@tran.sylvan.ia&gt;\nDate:   Thu Aug 22 10:07:21 2013 -0400\n\n    Start notes on Mars as a base\n\ndiff --git a/mars.txt b/mars.txt\nnew file mode 100644\nindex 0000000..df0654a\n--- /dev/null\n+++ b/mars.txt\n@@ -0,0 +1 @@\n+Cold and dry, but everything is my favorite color\n이런 방식으로, 연쇄 커밋 사슬을 구성할 수 있다.\n가장 최근 사슬의 끝값은 HEAD로 조회된다.\n~ 표기법을 사용하여 이전 커밋을 조회할 수 있다.\n그래서 HEAD~1(“head 마이너스 1”으로 읽는다.)은 “바로 앞선 커밋”을 의미하고,\nHEAD~123은 지금 있는 위치에서 123번째 이전 수정으로 간다는 의미가 된다.\n커밋된 것을 git log 명령어로 화면에 출력되는 숫자와 문자로 구성된 긴 문자열을 사용하여 조회할 수도 있다.\n변경사항에 대해서 중복되지 않는 ID로, “중복되지 않는(unique)”의 의미는 정말 유일하다는 의미다.\n특정 컴퓨터에 있는 임의 파일 집합에 대한 모든 변경사항은 중복되지 않는 40-문자 식별자가 붙어있다.\n첫 번째 커밋은 ID로 f22b25e3233b4645dabd0d81e651fe074bd8e73b가 주어졌다.\n그래서 다음과 같이 시도해 보자:\n$ git diff f22b25e3233b4645dabd0d81e651fe074bd8e73b mars.txt\n\ndiff --git a/mars.txt b/mars.txt\nindex df0654a..93a3e13 100644\n--- a/mars.txt\n+++ b/mars.txt\n@@ -1 +1,4 @@\n Cold and dry, but everything is my favorite color\n+The two moons may be a problem for Wolfman\n+But the Mummy will appreciate the lack of humidity\n+An ill-considered change\n올바른 정답이지만, 40-문자로 된 난수 문자열을 타이핑하는 것은 매우 귀찮은 일이다.\n그래서 Git은 앞의 몇 개 문자만으로도 사용할 수 있게 했다:\n$ git diff f22b25e mars.txt\n\ndiff --git a/mars.txt b/mars.txt\nindex df0654a..93a3e13 100644\n--- a/mars.txt\n+++ b/mars.txt\n@@ -1 +1,4 @@\n Cold and dry, but everything is my favorite color\n+The two moons may be a problem for Wolfman\n+But the Mummy will appreciate the lack of humidity\n+An ill-considered change\n좋았어요!\n파일에 변경사항을 저장할 수 있고 변경된 것을 확인할 수 있다.\n어떻게 옛 버전 파일을 되살릴 수 있을까?\n우연히 파일을 덮어썼다고 가정하자:\n$ nano mars.txt  \n$ cat mars.txt\n\nWe will need to manufacture our own oxygen\n이제 git status를 통해서 파일이 변경되었다고 하지만,\n변경사항은 아직 준비영역(Staging area)에 옮겨지지 않은 것으로 확인된다: \n$ git status\n\nOn branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n\n    modified:   mars.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\ngit checkout 명령어를 사용해서 과거에 있던 상태로 파일을 되돌릴 수 있다:\n$ git checkout HEAD mars.txt\n$ cat mars.txt\n\nCold and dry, but everything is my favorite color\nThe two moons may be a problem for Wolfman\nBut the Mummy will appreciate the lack of humidity\n이름에서 유추할 수 있듯이, git checkout 명령어는 파일의 옛 버전을 확인하고 가져온다. 즉, 되살린다.\n이 경우 HEAD에 기록된 가장 최근에 저장된 파일 버전을 되살린다.\n더 오래된 버전을 되살리고자 한다면, 대신에 커밋 식별자를 사용한다: \n$ git checkout f22b25e mars.txt\n$ cat mars.txt\n\nCold and dry, but everything is my favorite color\n$ git status\n\n# On branch master\nChanges to be committed:\n  (use \"git reset HEAD &lt;file&gt;...\" to unstage)\n# Changes not staged for commit:\n#   (use \"git add &lt;file&gt;...\" to update what will be committed)\n#   (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n#\n#   modified:   mars.txt\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n변경사항은 준비영역에 머물러 있는 것에 주목한다. 다시, git checkout 명령어를 사용해서 이전 버전으로 되돌아간다:\n$ git checkout HEAD mars.txt\n\n\n\n\n\n\n힌트헤드(HEAD)를 잃지 말자\n\n\n\nf22b25e 커밋 상태로 mars.txt 파일을 되돌리는데 앞서 다음 명령어를 사용했다.\n$ git checkout f22b25e mars.txt\n하지만 주의하자! checkout 명령어는 다른 중요한 기능을 가지고 있어서, 만약 타이핑에 오류가 있다면 Git이 의도를 오해할 수 있다.\n예를 들어, 앞선 명령에서 mars.txt를 빼먹게 되면…\n$ git checkout f22b25e\n\nNote: checking out 'f22b25e'.\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by performing another checkout.\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -b with the checkout command again. Example:\n git checkout -b &lt;new-branch-name&gt;\nHEAD is now at f22b25e Start notes on Mars as a base\n“detached HEAD”는 “보기는 하지만 건드리지는 마시오”와 같다. 따라서 현재 상태에서 어떤 변경도 만들지 말아야 한다. 저장소의 지난 상태를 살펴본 후에 git checkout master 명령어로 HEAD를 다시 붙인다.\n\n\n실행 취소를 하는 변경사항을 만들기 전에 저장소 상태를 확인하는 커밋 번호를 사용해야 한다는 것을 기억하는 것이 중요하다.\n흔한 실수는 커밋 번호를 사용하지 않는 것이다. 아래 예제에서는 커밋 번호가 f22b25e인 가장 최신 커밋(HEAD~1) 이전의 상태로 다시 되돌리고자 한다:\n\n\n\n\n\n\n그림 11.1: Git 복원(Checkout)\n\n\n\n그래서, 모두 한데 모아보자.\n\n\n\n\n\n\n그림 11.2: Git 동작방식 도식화\n\n\n\n\n\n\n\n\n\n힌트흔한 사례 단순화\n\n\n\ngit status 출력 결과를 주의 깊게 읽게 되면, 힌트가 포함된 것을 볼 수 있다.\n(use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n출력 결과가 언급하는 바는, 버전 식별자 없이 git checkout 명령어를 실행하게 되면\nHEAD에 저장된 상태로 파일을 원복시킨다는 것이다. 더블 대시 --가 필요한 경우는 명령어 자체로부터 복구해야 되는 파일명을 구별할 때다. 없는 경우, Git은 커밋 식별자에 파일명을 사용한다.\n\n\n파일이 하나씩 옛 상태로 되돌린다는 사실이 사람들이 작업을 조직하는 방식에 변화를 주는 경향이 있다.\n모든 것이 하나의 큰 문서로 되어 있다면,\n나중에 결론 부분에 변경사항을 실행 취소하지 않고, 소개 부분에 변경을 다시 되돌리기가 쉽지 않다(하지만 불가능하지는 않다).\n다른 한편으로 만약 소개 부분과 결론 부분이 다른 파일에 저장되어 있다면,\n시간 앞뒤로 이동하기가 훨씬 쉽다.\n\n\n\n\n\n\n힌트파일의 이전 버전 복구하기\n\n\n\n정훈이가 몇 주 동안 작업한 파이썬 스크립트에 변경을 했고, 오늘 아침 정훈이가 작업한 변경 사항이 스크립트를 “망가뜨려서” 더 이상 실행이 되지 않는다. 백업도 없이, 버그를 고치는 데 1시간 이상 소모했다…\n다행스럽게도, Git을 사용한 프로젝트 버전을 추적하고 있었다! 다음 아래 명령어 중 어떤 것이 data_cruncher.py로 불리는 파이썬 스크립트의 가장 최근 버전을\n복구하게 할까?\n\n$ git checkout HEAD\n$ git checkout HEAD data_cruncher.py\n$ git checkout HEAD~1 data_cruncher.py\n$ git checkout &lt;unique ID of last commit&gt; data_cruncher.py\n2번과 4번 모두\n\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n정답은 (5)-2번과 4번 모두.\ncheckout 명령어는 저장소에서 파일을 복원하여 작업 디렉토리에 있는 파일을 덮어쓴다. 답안 2번과 4번은 모두 저장소에 있는 data_cruncher.py 파일의 최신 버전을 복원한다. 답안 2번은 최신 버전을 나타내기 위해 HEAD를 사용하고, 답안 4번은 HEAD가 의미하는 바로 그 마지막 커밋의 고유한 ID를 사용한다는 차이만 있다.\n답안 3번은 HEAD 이전 커밋에서 data_cruncher.py의 버전을 가져오는데, 이는 원하는 바가 아니다.\n답안 1번은 위험할 수 있다! 파일명이 없으면 git checkout은 현재 디렉토리(및 그 아래의 모든 디렉토리)에 있는 모든 파일을 지정된 커밋 상태로 복원한다. 이 명령어는 data_cruncher.py를 최신 커밋 버전으로 복원하지만, 변경된 다른 모든 파일도 해당 버전으로 복원하여 해당 파일들에 대해 수행했을 수 있는 모든 변경 사항을 지워버린다! 위에서 논의했듯이, HEAD가 분리된 상태로 남게 되는데, 그런 상태에 놓여지는 것은 위험하다.\n\n\n\n\n\n\n\n\n\n\n\n힌트커밋 되돌리기\n\n\n\n정훈이는 동료와 함께 파이썬 코드를 협업해서 작성하고 있다. 그룹 저장소에 마지막으로 커밋한 것이 잘못된 것을 알게 되어서, 실행 취소하여 원복하고자 한다.\n정훈이는 실행 취소를 올바르게 해서 그룹 저장소를 사용하는\n모든 구성원이 제대로 된 변경사항을 가지고 작업을 계속하길 원한다. git revert [잘못된 커밋 ID] 명령어는 정훈이가 이전에 잘못 커밋했던 작업에 대해 실행 취소하는 커밋을 새로 생성시킨다. \n따라서, git revert는 git checkout [커밋 ID]와 다른데 이유는 checkout이 그룹 저장소에 커밋되지 않는 로컬 변경사항에\n대해서 적용된다는 점에서 차이가 난다. 정훈이가 git revert를 사용할 올바른 절차와 설명이 아래에 나와 있다. 빠진 명령어가 무엇일까?\n\n`________ # 커밋 ID를 찾을 수 있도록 Git 프로젝트 이력을 살펴본다.\nID를 복사한다. (ID의 첫 문자 몇 개만 사용한다. 예를 들어, 0b1d055).\ngit revert [커밋 ID]\n새로운 커밋 메시지를 타이핑한다.\n저장하고 종료한다.\n\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n명령어 git log는 커밋 ID와 함께 프로젝트 이력을 나열한다.\n명령어 git show HEAD는 최신 커밋에서 이루어진 변경 사항을 보여주고, 커밋 ID를 나열한다. 그러나 정훈이는 그것이 정확한 커밋인지, 그리고 다른 누군가 저장소에 변경 사항을 커밋하지 않았는지 다시 한 번 확인해야 한다.\n\n\n\n\n\n\n\n\n\n\n\n힌트작업흐름과 이력 이해하기\n\n\n\n다음 마지막 명령의 출력 결과는 무엇일까?\n$ cd planets\n$ echo \"Venus is beautiful and full of love\" &gt; venus.txt\n$ git add venus.txt\n$ echo \"Venus is too hot to be suitable as a base\" &gt;&gt; venus.txt\n$ git commit -m \"Comment on Venus as an unsuitable base\"\n$ git checkout HEAD venus.txt\n$ cat venus.txt #this will print the contents of venus.txt to the screen\n\n   Venus is too hot to be suitable as a base\n   Venus is beautiful and full of love\n   Venus is beautiful and full of love\n   Venus is too hot to be suitable as a base\n   Error because you have changed venus.txt without committing the changes\n\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n정답은 2번이다.\ngit add venus.txt 명령어는 venus.txt의 현재 버전을 준비 영역에 올려놓는다. 두 번째 echo 명령어로 인한 파일의 변경사항은 작업 복사본에만 적용되고, 준비 영역에 있는 버전에는 적용되지 않는다.\n따라서 git commit -m \"Comment on Venus as an unsuitable base\"가 실행될 때, 저장소에 커밋되는 venus.txt의 버전은 준비 영역에 있는 것으로, 한 줄만 가지고 있다.\n이때 작업 복사본은 여전히 두 번째 줄을 가지고 있고(git status는 파일이 수정되었음을 보여줄 것이다). 그러나 git checkout HEAD venus.txt는 작업 복사본을 venus.txt의 가장 최근에 커밋된 버전으로 대체한다.\n그래서 cat venus.txt는 다음과 같이 출력될 것이다.\nVenus is beautiful and full of love.\n\n\n\n\n\n\n\n\n\n\n\n힌트git diff 이해 확인하기\n\n\n\ngit diff HEAD~3 mars.txt 명령어를 고려해 보자. 이 명령어를 실행하게 되면 실행 결과로 예상하는 바를 말해보자. 명령어를 실행하게 되면 어떤 일이 발생하는가? 그리고 이유는 무엇인가? \n또 다른 명령어 git diff [ID] mars.txt를 시도해 보자.\n여기서, [ID]를 가장 최근 커밋 식별자로 치환한다. 무슨 일이 생길까? 그리고 실제로 생긴 일은 무엇인가?\n\n\n\n\n\n\n\n\n힌트준비 단계 변경사항(Staged Changes) 제거하기\n\n\n\ngit checkout 명령어를 통해서 준비영역으로 올라오지 않은 변경사항이 있을 때, 이전 커밋을 복구할 수 있었다. 하지만, git checkout은 준비영역에 올라왔지만, 커밋되지 않는 변경사항에 대해서도 동작한다. mars.txt 파일에 변경사항을 만들고, 변경사항을 추가하고 나서,\ngit checkout 명령어를 사용하게 되면 변경사항이 사라졌는지 살펴보자.\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n변경사항을 추가한 후에는 git checkout을 직접 사용할 수 없다. git status 출력 결과를 살펴보자.\nOn branch main\nChanges to be committed:\n  (use \"git reset HEAD &lt;file&gt;...\" to unstage)\n\n        modified:   mars.txt\n동일한 출력 결과가 나오지 않는다면 파일 변경을 잊었거나, 추가하고 커밋까지 한 상태일 수 있다.\n이 상태에서 git checkout -- mars.txt 명령어를 사용하면 오류는 발생하지 않지만, 파일도 복원되지 않는다. Git은 파일을 unstage하기 위해 먼저 git reset을 사용해야 한다고 친절하게 알려준다. \n$ git reset HEAD mars.txt\n\nUnstaged changes after reset:\nM   mars.txt\n이제 git status를 실행하면 다음과 같은 결과가 화면에 출력된다.\n$ git status\n\nOn branch main\nChanges not staged for commit:\n(use \"git add &lt;file&gt;...\" to update what will be committed)\n(use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n\n      modified:   mars.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n이는 이제 git checkout을 사용하여 파일을 이전 커밋 상태로 복원할 수 있다는 것을 의미하게 된다.\n\n$ git checkout -- mars.txt\n$ git status\n\nOn branch main\nnothing to commit, working tree clean\n\n\n\n\n\n\n\n\n\n\n\n힌트변경 이력 탐색과 요약\n\n\n\n변경 이력 탐색은 Git에 있어 중요한 부분 중의 하나로,\n특히 커밋이 수개월 전에 이뤄졌다면, 올바른 커밋 ID를 찾는 것이 종종 크나큰 도전과제가 된다. planets 프로젝트가 50개 파일 이상으로 구성되었다고 상상해 보자.\nmars.txt 파일에 특정 텍스트가 변경된 커밋을 찾고자 한다. git log를 타이핑하게 되면 매우 긴 목록이 출력된다. 어떻게 하면 검색 범위를 좁힐 수 있을까? git diff 명령어가 특정 파일만 탐색할 수 있다는 점을 상기하자.\n예를 들어, git diff mars.txt. 이 문제에 유사한 아이디어를 적용해 보자.\n$ git log mars.txt\n불행하게도 커밋 메시지 일부는 매우 애매모호하다. 예를 들어, update files. 어떻게 하면 파일을 잘 검색할 수 있을까? git diff, git log 명령어 모두 매우 유용하다. 두 명령어 모두 변경 이력의 다른 부분을 요약해 준다. 둘을 조합하는 것은 가능할까? 다음 명령어를 실행해 보자:\n$ git log --patch mars.txt\n엄청 긴 출력 목록이 나타난다. 각 커밋마다 커밋 메시지와 차이가 쭉 출력된다. 질문: 다음 명령어는 무슨 작업을 수행할까요?\n$ git log --patch HEAD~3 *.txt",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>이력 탐색</span>"
    ]
  },
  {
    "objectID": "git-ignore.html",
    "href": "git-ignore.html",
    "title": "12  추적 대상에서 제외",
    "section": "",
    "text": "만약 Git이 추적하기를 원하지 않는 파일이 있다면 어떻게 해야 할까요? 편집기에서 자동 생성되는 백업 파일이나 자료 분석 중에 생성되는 임시 파일들이 좋은 예가 될 수 있다. 몇 개의 더미(dummy) 파일을 생성해 보자. \n$ mkdir results\n$ touch a.dat b.dat c.dat results/a.out results/b.out\n그러면 Git은 다음과 같이 보여줍니다:\n$ git status\n\nOn branch master\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n\n    a.dat\n    b.dat\n    c.dat\n    results/\nnothing added to commit but untracked files present (use \"git add\" to track)\n버전 관리 아래 이런 파일들을 두는 것은 디스크 공간 낭비다. 더 나쁜 것은, 이런 파일들을 모두 관리 목록에 넣으면 실제로 중요한 변경 사항을 관리하는 데 집중하기 어렵다는 점이다. 그래서 Git에게 이런 중요하지 않은 파일들은 무시하라고 알려준다.\n프로젝트의 루트 디렉터리에 .gitignore라는 파일을 생성하고 무시할 파일들을 명시함으로써 해당 작업을 수행할 수 있다. \n$ nano .gitignore\n$ cat .gitignore\n\n*.dat\nresults/\n위의 패턴은 .dat 확장자를 가진 모든 파일과 results 디렉터리에 있는 모든 것을 무시하라는 의미다. (하지만 이들 중 일부 파일이 이미 추적되고 있다면, Git은 계속해서 추적할 것이다.)\n.gitignore 파일을 생성하자마자, git status 출력 결과는 훨씬 깔끔해졌다.\n$ git status\n\nOn branch master\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n\n    .gitignore\nnothing added to commit but untracked files present (use \"git add\" to track)\n이제 Git이 알아차리는 유일한 것은 새로 생성된 .gitignore 파일뿐이다. 이 파일들을 추적하여 관리하지 않아도 된다고 생각할 수 있지만, 저장소를 공유하는 다른 모든 사람들도 우리가 추적하지 않는 것과 동일한 파일들을 무시하기를 원할 것이다. 그러므로 .gitignore를 추가하고 커밋한다.\n$ git add .gitignore\n$ git commit -m \"Ignore data files and the results folder.\"\n$ git status\n\n# On branch master\nnothing to commit, working directory clean\n보너스로, .gitignore는 실수로 추적하고 싶지 않은 파일이 저장소에 추가되는 것을 방지하는 데 도움이 된다.\n$ git add a.dat\n\nThe following paths are ignored by one of your .gitignore files:\na.dat\nUse -f if you really want to add them.\n만약 .gitignore 설정에도 불구하고 파일을 추가하려면, git add -f를 사용하여 강제로 Git에 파일을 추가할 수 있다. 예를 들어, git add -f a.dat와 같이 사용한다. 추적되지 않는 파일의 상태를 항상 보려면 다음 명령어를 사용하면 된다.\n$ git status --ignored\n\nOn branch master\nIgnored files:\n (use \"git add -f &lt;file&gt;...\" to include in what will be committed)\n\n        a.dat\n        b.dat\n        c.dat\n        results/\n\nnothing to commit, working directory clean\n\n\n\n\n\n\n힌트중첩된 파일 추적하지 않기\n\n\n\n디렉터리 구조가 다음과 같다고 가정해 보자.\nresults/data\nresults/plots\nresults/plots만 추적하지 않고, results/data 디렉터리는 추적하려면 어떻게 해야 할까?\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n대부분의 프로그래밍 이슈와 마찬가지로, 이 문제를 해결하는 몇 가지 방법이 있다. results/plots 디렉터리의 내용만 추적하지 않기로 한다면, .gitignore 파일에서 /plots/ 폴더만 추적하지 않도록 다음과 같이 수정하면 된다.\nresults/plots/\n반대로 /results/ 디렉터리의 모든 것을 추적하지 않되, results/data만 예외로 추적하고 싶다면,\n.gitignore 파일에 results/를 추가하고 results/data/에 대해서는 예외 처리를 해주면 된다. 다음 도전 과제에서 이러한 유형의 해법을 다루게 될 것이다.\n종종 ** 패턴이 사용하기 편리한데, 이는 다수의 디렉터리와 매칭될 수 있기 때문이다. 예를 들어, **/results/plots/*은 루트 디렉터리 아래 어디에 있든 results/plots 디렉터리를 추적하지 않는다.\n\n\n\n\n\n\n\n\n\n\n\n힌트특정 파일만 포함시키기\n\n\n\nfinal.data 파일만 제외하고 모든 .data 파일을 추적하지 않으려면 어떻게 하면 될까? 힌트: ! (느낌표 연산자)가 수행하는 작업을 알아본다. \n\n\n\n\n\n\n주의해답\n\n\n\n\n\n.gitignore 파일에 다음 두 줄을 추가한다:\n*.data           # 모든 data 파일을 추적하지 않는다.\n!final.data      # final.data 파일은 예외로 추적한다. \n느낌표 연산자는 앞서 제외된 항목을 다시 포함시키는 역할을 한다.\n\n\n\n\n\n\n\n\n\n\n\n힌트중첩된 파일 무시하기: 변형\n\n\n\n앞선 중첩 파일 연습과 유사하지만, 약간 다른 디렉터리 구조를 가진 디렉터리 구조가 있다고 하자.\nresults/data\nresults/images\nresults/plots\nresults/analysis\nresults/data는 제외하고 results 폴더의 모든 내용을 무시하려면 어떻게 해야 할까?\n힌트: 이전에 ! 연산자로 예외를 만든 방법에 대해 조금 생각해 보자.\n\n\n\n\n\n\n주의해답\n\n\n\n\n\nresults/의 내용은 무시하되 results/data/의 내용은 무시하지 않으려면, .gitignore에서 results 폴더의 내용을 무시하되 results/data 하위 폴더의 내용에 대해서는 예외를 만들면 된다. 이 경우 .gitignore 파일은 다음과 같이 작성하면 된다.\nresults/*               # results 폴더의 모든 내용을 무시한다\n!results/data/          # results/data/의 내용은 무시하지 않는다\n\n\n\n\n\n\n\n\n\n\n\n힌트디렉터리의 모든 파일 추적하지 않기\n\n\n\n디렉터리 구조가 다음과 같다고 가정해 본다.\nresults/data/position/gps/a.data\nresults/data/position/gps/b.data\nresults/data/position/gps/c.data\nresults/data/position/gps/info.txt\nresults/plots\nresults/data/position/gps 디렉터리의 모든 .data 파일을 추적하지 않도록 .gitignore 파일에 규칙을 작성한다면, 가장 간결한 규칙은 무엇일까? 단, info.txt 파일은 추적되어야 한다.\n\n\n\n\n\n\n주의해답\n\n\n\n\n\nresults/data/position/gps/*.data 규칙은 results/data/position/gps 디렉터리에서 .data로 끝나는 모든 파일과 매칭된다. results/data/position/gps/info.txt 파일은 확장자가 다르기 때문에 계속 추적될 것다.\n\n\n\n\n\n\n\n\n\n\n\n힌트특정 디렉토리의 모든 파일 추적하지 않기\n\n\n\n저장소의 여러 하위 디렉터리에 많은 .csv 파일이 있다고 가정해 본다. 예를 들어 다음과 같은 파일 디렉토리 구조를 가질 수 있다.\nresults/a.csv\ndata/experiment_1/b.csv \ndata/experiment_2/c.csv\ndata/experiment_2/variation_1/d.csv\n해당 폴더의 이름을 명시적으로 나열하지 않고, 모든 .csv 파일을 무시하려면 어떻게 해야 할까?\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n.gitignore 파일에 다음과 같이 작성한다.\n**/*.csv\n이렇게 하면 디렉토리 트리에서의 위치에 상관없이 모든 .csv 파일이 무시된다. 느낌표 연산자를 사용하여 특정 파일은 예외로 여전히 포함시킬 수도 있다.\n\n\n\n\n\n\n\n\n\n\n\n힌트적용 규칙 순서\n\n\n\n.gitignore 파일에 다음 내용이 포함되어 있다고 가정해 본다.\n*.csv\n!*.csv\n어떤 결과가 나올까?\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n! 연산자는 이전에 정의된 제외 패턴을 부정한다. 따라서 .gitignore 파일에서 !*.csv 규칙은 앞서 제외했던 모든 .csv 파일을 다시 포함시킨다. 결과적으로 어떤 파일도 제외되지 않고, 모든 .csv 파일이 추적 대상이 된다.\n\n\n\n\n\n\n\n\n\n\n\n힌트로그 파일\n\n\n\n여러분이 작성한 스크립트가 log_01, log_02, log_03과 같은 형식의 중간 로그 파일을 다수 생성한다고 가정해 본다. 로그 파일들은 보관하고 싶지만, git으로 추적하고 싶지는 않은 경우가 있다.\n\nlog_01, log_02 등의 형식을 가진 모든 파일을 추적에서 제외하는 .gitignore 규칙을 하나 작성한다.\nlog_01 형식의 더미 파일을 생성하여 “제외 패턴”을 테스트한다.\n결국 log_01 파일이 매우 중요하다는 것을 알게 되어, .gitignore 파일은 변경하지 않고 해당 파일만 추적 대상에 포함시킨다.\n.gitignore를 통해 추적을 제외하고 싶은, 디렉터리에 있을 수 있는 다른 유형의 파일에는 어떤 것이 있을지 옆 사람과 의논해 보자.\n\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n\nlog_* 혹은 log* 규칙을 .gitignore 파일에 추가한다.\ngit add -f log_01 명령어를 사용하여 log_01 파일만 강제로 추적한다.",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>추적 대상에서 제외</span>"
    ]
  },
  {
    "objectID": "git-github.html",
    "href": "git-github.html",
    "title": "13  GitHub 원격 작업",
    "section": "",
    "text": "13.1 원격 저장소 생성\n버전 제어(version control)는 다른 사람과 협업할 때 진정으로 그 가치를 발휘한다. 우리는 이미 버전 제어를 위해 필요한 대부분의 작업을 수행했다. 한 가지 빠진 것은 한 저장소에서 다른 저장소로 변경 사항을 복사하는 것이다.\nGit 같은 시스템은 임의 두 저장소 사이에 작업 내용을 옮길 수 있는 기능을 제공한다. 하지만 실무에서는 다른 사람의 노트북이나 PC보다는 중앙 허브에 웹 방식으로 하나의 원본을 두고 사용하는 것이 가장 쉽다.\n대부분의 프로그래머는 프로그램 마스터 원본을 GitHub, BitBucket, GitLab 호스팅 서비스에 두고 사용한다. 이번 장 마지막 부분에서 이러한 접근법의 장점과 단점을 살펴본다.\n세상 사람들과 현재 프로젝트에서 변경한 사항을 공유하는 것에서부터 시작해보자. GitHub에 로그인하고 나서, 우측 상단 아이콘을 클릭해서 planets 이름으로 신규 저장소를 생성한다.\n저장소 이름을 “planets”으로 만들고 “Create Repository”를 클릭한다.\n저장소가 생성되자마자, GitHub는 URL을 가진 페이지와 로컬 저장소 환경 설정 방법에 대한 정보를 화면에 출력한다.\n다음 명령어가 실제로 GitHub 서버에서 자동으로 수행된다.\nmars.txt 파일을 추가하고 커밋한 이전 장을 상기한다면, 로컬 저장소는 다음과 같이 도식적으로 표현할 수 있다.\n이제 저장소가 두 개로 늘어서, 도식적으로 표현하면 다음과 같다.\n현재 로컬 저장소는 여전히 mars.txt 파일에 대한 이전 작업 정보를 담고 있다. 하지만 GitHub의 원격 저장소에는 아직 어떠한 파일도 담고 있지 않다.",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>GitHub 원격 작업</span>"
    ]
  },
  {
    "objectID": "git-github.html#원격-저장소-생성",
    "href": "git-github.html#원격-저장소-생성",
    "title": "13  GitHub 원격 작업",
    "section": "",
    "text": "그림 13.1: (1단계) GitHub 저장소 생성\n\n\n\n\n\n\n\n\n\n\n그림 13.2: (2단계) GitHub 저장소 생성\n\n\n\n\n\n\n\n\n\n\n그림 13.3: (3단계) GitHub 저장소 생성\n\n\n\n\n$ mkdir planets\n$ cd planets\n$ git init\n\n\n\n\n\n\n\n그림 13.4: Git 준비영역(Staging) 로컬 저장소\n\n\n\n\n\n\n\n\n\n\n그림 13.5: 신선한 신규 GitHub 저장소",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>GitHub 원격 작업</span>"
    ]
  },
  {
    "objectID": "git-github.html#로컬-저장소를-원격-저장소-연결",
    "href": "git-github.html#로컬-저장소를-원격-저장소-연결",
    "title": "13  GitHub 원격 작업",
    "section": "13.2 로컬 저장소를 원격 저장소 연결",
    "text": "13.2 로컬 저장소를 원격 저장소 연결\n 다음 단계는 두 저장소를 연결하는 것이다. 로컬 저장소를 위해서 GitHub 저장소를 원격(remote)으로 만들어 두 저장소를 연결한다. GitHub의 저장소 홈페이지에 식별하는 데 필요한 문자열이 포함되어 있다.\n\n\n\nGitHub 저장소 URL 발견장소\n\n\nHTTPS에서 SSH로 프로토콜(protocol)을 변경하려면 ‘SSH’ 링크를 클릭한다.\n브라우저에서 해당 URL을 복사한 후, 로컬 planets 저장소로 이동하여 아래 명령어를 실행한다.\n$ git remote add origin git@github.com:vlad/planets.git\n단, Vlad의 저장소 URL이 아닌 여러분의 저장소 URL을 사용해야 한다. 유일한 차이점은 vlad 대신 사용자 이름을 사용해야 한다는 것이다.\norigin이라는 이름은 원격 저장소에 대한 로컬 별명이다. 원한다면 다른 명칭을 사용할 수도 있지만, origin이 가장 일반적인 선택이다.\ngit remote -v 명령어를 실행해서 명령어가 제대로 작동했는지 확인한다.\n$ git remote -v\n\norigin   git@github.com:vlad/planets.git (fetch)\norigin   git@github.com:vlad/planets.git (push)\n\n\n\n\n\n\n힌트HTTPS vs. SSH\n\n\n\n \n여기서 SSH를 사용하는데, 이는 추가적인 설정이 필요하지만 많은 애플리케이션에서 널리 사용되는 보안 프로토콜이기 때문이다. 아래 단계는 GitHub를 위한 최소한의 수준에서 SSH를 설명하고 있다.\n\n\n\n\n\nGitHub 저장소 URL 변경",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>GitHub 원격 작업</span>"
    ]
  },
  {
    "objectID": "git-github.html#ssh-배경과-환경설정",
    "href": "git-github.html#ssh-배경과-환경설정",
    "title": "13  GitHub 원격 작업",
    "section": "13.3 SSH 배경과 환경설정",
    "text": "13.3 SSH 배경과 환경설정\n드라큘라가 원격 저장소에 연결하기 전에, 컴퓨터가 GitHub와 인증할 수 있는 방법을 설정해야 한다. 이를 통해 GitHub은 원격 저장소에 연결하려는 사람이 바로 드라큘라 본인임을 알 수 있다.\n많은 다양한 서비스에서 명령줄(command line)에 대한 접근을 인증하기 위해 일반적으로 사용되는 방법을 설정할 것이다. 방법을 보안 쉘 프로토콜(Secure Shell Protocol, SSH)이라고 한다. SSH는 보안이 안전하지 않은 네트워크에서도 컴퓨터 간에 안전한 통신을 가능하게 하는 암호화 네트워크 프로토콜이다.\nSSH는 키 쌍(key pair)이라는 것을 사용한다. 접근을 검증하기 위해 함께 작동하는 두 개의 키이다. 하나의 키는 공개적으로 알려져 있으며 공개 키(public key)라고 하고, 다른 키는 비밀 키(private key)라고 하며 비공개로 유지된다. 명칭 자체가 매우 직관적이다. \n공개 키는 자물쇠로, 본인만이 자물쇠를 열 수 있는 키(비밀 키)를 가지고 있다고 생각할 수 있다. GitHub 계정과 같이 안전한 통신 방법이 필요한 곳에 공개 키를 사용한다. 이 자물쇠, 즉 공개 키를 GitHub에 제공하고 “내 계정에 대한 통신을 이 키로 잠그세요. 그러면 내 비밀 키를 가진 컴퓨터만이 통신을 잠금 해제하고 내 GitHub 계정으로 git 명령을 보낼 수 있습니다.”라고 말한다.\n지금 할 일은 SSH 키를 설정하고 공개 키를 GitHub 계정에 추가하는 데 필요한 최소한의 작업이다. 가장 먼저 할 일은 현재 사용 중인 컴퓨터에서 해당 작업이 이미 완료되었는지 확인하는 것이다. 일반적으로 이러한 설정은 한 번만 이루어지면 되고, 이후에는 신경 쓸 필요가 없기 때문이다.\n\n\n\n\n\n\n힌트키 안전하게 보관하기\n\n\n\nSSH 키는 계정을 안전하게 유지하기 때문에 실제로 잊어서는 안 된다. 특히 여러 컴퓨터에서 계정에 접속하는 경우라면 가끔씩 보안 쉘 키를 점검하는 것이 좋은 습관이다.\n\n\n컴퓨터에 이미 존재하는 키 쌍을 확인하기 위해 ls 명령어를 실행한다.\nls -al ~/.ssh\n출력 결과는 사용자별로 사용 중인 컴퓨터에 SSH가 설정되어 있는지 여부에 따라 약간 달라진다. 드라큘라는 컴퓨터에 SSH를 설정한 적이 전혀 없으므로, 출력 결과는 다음과 같다.\nls: cannot access '/c/Users/Vlad Dracula/.ssh': No such file or directory\n만약 사용 중인 컴퓨터에 SSH가 설정되어 있다면, 공개 키와 비밀 키 쌍이 나열될 것이다. 파일 이름은 키 쌍이 설정된 방식에 따라 id_ed25519/id_ed25519.pub 또는 id_rsa/id_rsa.pub 중 하나다. 드라큘라 컴퓨터에는 이러한 키 쌍이 존재하지 않으므로, 다음 명령어를 사용하여 키 쌍을 생성한다.\n\n13.3.1 SSH 키 쌍 생성\nSSH 키 쌍을 생성하기 위해 드라큘라는 다음 명령어를 사용하는데, 여기서 -t 옵션은 사용할 알고리즘 유형을 지정하고 -C는 키에 주석(여기서는 드라큘라의 이메일)을 첨부한다.\n$ ssh-keygen -t ed25519 -C \"vlad@tran.sylvan.ia\"\n만약 Ed25519 알고리즘을 지원하지 않는 레거시 시스템을 사용 중이라면, 다음 명령어(ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\")를 사용한다.\nGenerating public/private ed25519 key pair.\nEnter file in which to save the key (/c/Users/Vlad Dracula/.ssh/id_ed25519):\n기본 파일을 사용하고 싶으므로, 그냥 Enter 키를 누른다.\nCreated directory '/c/Users/Vlad Dracula/.ssh'.\nEnter passphrase (empty for no passphrase):\n이제 드라큘라에게 암호 문구(Passphrase)1를 입력하라고 요청한다. 다른 사람들이 가끔 접근할 수 있는 연구실 노트북을 사용하고 있기 때문에, 암호 문구를 생성하고 싶어 한다. “비밀번호 재설정” 옵션이 없으므로 기억하기 쉬운 것을 사용하거나 암호 문구를 어딘가에 저장해 두어야 한다.\nEnter same passphrase again:\n동일한 패스프레이즈를 한 번 더 입력한 후, 확인 메시지를 받는다.\nYour identification has been saved in /c/Users/Vlad Dracula/.ssh/id_ed25519\nYour public key has been saved in /c/Users/Vlad Dracula/.ssh/id_ed25519.pub\nThe key fingerprint is:\nSHA256:SMSPIStNyA00KPxuYu94KpZgRAYjgt9g4BA4kFy3g1o vlad@tran.sylvan.ia\nThe key's randomart image is:\n+--[ED25519 256]--+\n|^B== o.          |\n|%*=.*.+          |\n|+=.E =.+         |\n| .=.+.o..        |\n|....  . S        |\n|.+ o             |\n|+ =              |\n|.o.o             |\n|oo+.             |\n+----[SHA256]-----+\n“identification”은 실제로 비밀 키이다. 비밀 키를 절대 공유해서는 안 된다. 공개 키는 적절하게 명명되었다. “key fingerprint”는 공개 키의 짧은 버전이다.\n이제 SSH 키를 생성했으므로, 확인해 보면 SSH 파일들을 찾을 수 있다.\n$ ls -al ~/.ssh\n\ndrwxr-xr-x 1 Vlad Dracula 197121   0 Jul 16 14:48 ./\ndrwxr-xr-x 1 Vlad Dracula 197121   0 Jul 16 14:48 ../\n-rw-r--r-- 1 Vlad Dracula 197121 419 Jul 16 14:48 id_ed25519\n-rw-r--r-- 1 Vlad Dracula 197121 106 Jul 16 14:48 id_ed25519.pub\n\n\n13.3.2 공개 키를 GitHub에 복사\n이제 SSH 키 쌍을 가지고 있고, GitHub 인증여부를 확인하기 위해 다음 명령어를 실행할 수 있다.\n$ ssh -T git@github.com\n\nThe authenticity of host 'github.com (192.30.255.112)' can't be established.\nRSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? y\nPlease type 'yes', 'no' or the fingerprint: yes\nWarning: Permanently added 'github.com' (RSA) to the list of known hosts.\ngit@github.com: Permission denied (publickey).\n맞다. GitHub에 공개 키를 제공해야 한다는 것을 잊었다!\n먼저, 공개 키를 복사해야 한다. 그렇지 않으면 비밀 키와 혼동을 일으킬 수 있기 때문에 .pub 확장자를 반드시 포함해야 한다.\n$ cat ~/.ssh/id_ed25519.pub\n\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDmRA3d51X0uu9wXek559gfn6UFNF69yZjChyBIU2qKI vlad@tran.sylvan.ia\n이제 &lt;GitHub.com&gt;에 접속하여, 오른쪽 상단 모서리에 있는 프로필 아이콘을 클릭하여 드롭다운 메뉴를 연다. “Settings”를 클릭하고, 설정 페이지에서 왼쪽의 “Account settings” 메뉴에서 “SSH and GPG keys”를 클릭한다. 오른쪽의 “New SSH key” 버튼을 클릭한다. 이제 제목을 추가할 수 있다(드라큘라는 원래 키 쌍 파일이 어디에 있는지 기억할 수 있도록 “Vlad’s Lab Laptop”이라는 제목을 사용한다). SSH 키를 필드에 붙여넣고 “Add SSH key”를 클릭하여 설정을 완료한다.\n이제 설정이 완료되었으니, 명령줄에서 인증을 다시 확인해 보자.\n$ ssh -T git@github.com\n\nHi Vlad! You've successfully authenticated, but GitHub does not provide shell access.\n좋다! 출력결과는 SSH 키가 의도한 대로 작동함을 확인해 준다. 이제 작업 내용을 원격 저장소에 푸시할 준비가 되었다.",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>GitHub 원격 작업</span>"
    ]
  },
  {
    "objectID": "git-github.html#로컬-변경사항-원격-저장소-푸시",
    "href": "git-github.html#로컬-변경사항-원격-저장소-푸시",
    "title": "13  GitHub 원격 작업",
    "section": "13.4 로컬 변경사항 원격 저장소 푸시",
    "text": "13.4 로컬 변경사항 원격 저장소 푸시\n이제 인증 설정이 완료되었으므로, 원격 저장소로 돌아갈 수 있다. 다음 명령어는 로컬 저장소에서의 변경 사항을 GitHub 저장소로 푸시한다.\n$ git push origin main\n드라큘라가 암호 문구(passphrase)를 설정했기 때문에, 명령어 실행 시 암호 문구를 입력하라는 프롬프트가 표시된다. 만약 인증에 대한 고급 설정을 완료했다면, 암호 문구를 요구하지 않을 것이다.\nEnumerating objects: 16, done.\nCounting objects: 100% (16/16), done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (11/11), done.\nWriting objects: 100% (16/16), 1.45 KiB | 372.00 KiB/s, done.\nTotal 16 (delta 2), reused 0 (delta 0)\nremote: Resolving deltas: 100% (2/2), done.\nTo https://github.com/vlad/planets.git\n * [new branch]      main -&gt; main\n\n\n\n\n\n\n힌트프록시(Proxy)\n\n\n\n만약 연결된 네트워크가 프록시를 사용한다면, “Could not resolve hostname” 오류 메시지로 인해서 마지막 명령어가 실패할 가능성이 있다. 이 문제를 해결하기 위해서는 프록시에 대한 정보를 Git에 전달할 필요가 있다.\n$ git config --global http.proxy http://user:password@proxy.url\n$ git config --global https.proxy http://user:password@proxy.url\n프록시를 사용하지 않는 또 다른 네트워크에 연결될 때는 Git에게 프록시 기능을 사용하지 않도록 다음 명령어를 사용하여 알려준다.\n$ git config --global --unset http.proxy\n$ git config --global --unset https.proxy\n\n\n\n\n\n\n\n\n힌트비밀번호 관리자\n\n\n\n운영체제에 비밀번호 관리자(password manager)가 설정되어 있다면, 사용자이름(username)과 비밀번호(password)가 필요할 때, git push 명령어가 이를 사용하려 한다. “Git Bash on Windows”를 사용하면 기본 디폴트 행동이다. 관리자 비밀번호를 사용하는 대신에, 터미널에서 사용자이름과 비밀번호를 입력하려면, git push를 실행하기 전에 터미널에서 다음과 같이 타이핑한다.\n$ unset SSH_ASKPASS\ngit uses SSH_ASKPASS for all credential entry에도 불구하고, SSH나 HTTPS를 경유하여 Git을 사용하든 SSH_ASKPASS를 설정하고 싶지 않을 수도 있다.\n~/.bashrc 파일 하단에 unset SSH_ASKPASS를 추가해서 Git으로 하여금 사용자명과 비밀번호를 사용하도록 기본설정으로 둘 수도 있다.\n\n\n이제 로컬 저장소와 원격 저장소는 다음과 같은 상태가 된다. \n\n\n\n\n\n\n그림 13.6: 첫 번째 푸시(Push) 다음 GitHub 저장소\n\n\n\n\n\n\n\n\n\n힌트‘-u’ 플래그(flag)\n\n\n\nGit 문서에서 git push와 함께 사용되는 -u 옵션을 볼 수 있다. git branch 명령어에 대한 --set-upstream-to 옵션과 동의어에 해당되는 옵션이다. 원격 브랜치를 현재 브랜치와 연결시키는 데 사용된다. 그래서 git pull 명령어가 아무런 인자 없이 사용될 수 있다. 원격 저장소가 설정되면, git push -u origin master 명령어만 실행시키면 연결 작업이 완료된다.\n\n\n또한, 원격 저장소에서 로컬 저장소로 변경 사항을 풀(pull)해서 가져올 수도 있다.\n$ git pull origin main\n\nFrom https://github.com/vlad/planets\n * branch            master     -&gt; FETCH_HEAD\nAlready up-to-date.\n이 경우 가져오기 하는 풀(pull)은 아무런 결과가 없는데, 이유는 두 저장소가 이미 동기화되어 있기 때문이다. 하지만 만약 누군가 GitHub 저장소에 변경 사항을 푸시했다면, 상기 명령어는 변경된 사항을 로컬 저장소로 다운로드한다.\n\n\n\n\n\n\n힌트GitHub 브라우저에서 직접 파일 업로드\n\n\n\nGitHub에서는 명령줄(command line)을 거치지 않고 브라우저를 떠나지 않은 채로 파일을 직접 저장소에 업로드할 수 있는 기능도 제공한다. 두 가지 방법이 있다. 첫째는 파일 트리 상단의 툴바에 있는 “Upload files” 버튼을 클릭하는 것이고, 둘째는 데스크톱에서 파일을 파일 트리로 드래그 앤 드롭하는 것이다. 이에 대한 자세한 내용은 GitHub 페이지에서 확인할 수 있다.\n\n\n\n\n\n\n\n\n힌트GitHub GUI\n\n\n\n\nGitHub 웹사이트에서 planets 저장소를 찾아간다. Code 탭 아래 “XX commits”(“XX”는 숫자) 텍스트를 클릭한다. 각 커밋 우측의 버튼 세 개를 여기저기 둘러보고, 클릭해 본다. 버튼을 눌러서 어떤 정보를 모을 수 있거나 탐색할 수 있는가? 쉘에서 동일한 정보를 어떻게 얻을 수 있을까?\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n(클립보드 그림을 갖는) 가장 좌측 버튼은 클립보드에 커밋 식별자 전체를 복사한다. 쉘에서 git log 명령어가 각 커밋에 대한 전체 커밋 식별자를 보여준다.\n중간 버튼을 클릭하게 되면, 특정 커밋으로 변경한 내용 전체를 확인할 수 있다. 녹색 음영선은 추가를 붉은색 음영선은 삭제를 의미한다. 쉘에서 동일한 작업을 git diff로 할 수 있다. 특히, git diff ID1..ID2(ID1와 ID2는 커밋 식별자다) 명령어(즉, git diff a3bf1e5..041e637)는 두 커밋 사이 차이를 보여준다.\n가장 우측 버튼은 커밋 당시에 저장소의 모든 파일을 보여준다. 쉘로 이런 작업을 수행하려면, 해당 시점의 저장소를 checkout 해야 한다. 쉘에서 git checkout ID(여기서 ID는 살펴보려고 하는 커밋 식별자) 명령어를 실행하면 된다. checkout 하게 되면, 나중에 저장소를 올바른 상태로 되돌려 놓아야 된다는 것을 기억해야 한다.\n\n\n\n\n\n\n\n\n\n\n\n힌트GitHub 시간도장\n\n\n\n\nGitHub에 원격 저장소를 생성한다. 로컬 저장소의 콘텐츠를 원격 저장소로 푸시한다. 로컬 저장소에 변경 사항을 만들고, 변경 사항을 푸시한다.\n방금 생성한 GitHub 저장소로 가서 GitHub 변경 사항에 대한 시간도장(timestamps)을 살펴본다. GitHub이 시간 정보를 어떻게 기록하는가? 왜 그런가?\n\n\n\n\n\n\n주의해답\n\n\n\n\n\nGitHub은 시간도장을 사람이 읽기 쉬운 형태로 표시한다(즉, “22 hours ago” 혹은 “three weeks ago”). 하지만 시간도장을 이리저리 살펴보면, 파일의 마지막 변경이 발생된 정확한 시간을 볼 수 있다.\n\n\n\n\n\n\n\n\n\n\n\n힌트푸시(Push) vs. 커밋(Commit)\n\n\n\n \n이번 장에서, “git push” 명령어를 소개했다. “git push” 명령어가 “git commit” 명령어와 어떻게 다른가?\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n변경 사항을 푸시하면, 로컬에서 변경한 사항을 원격 저장소와 상호 협의하여 최신 상태로 갱신한다. (흔히 다른 사람이 변경시킨 것을 공유하는 것도 이에 해당된다.) 커밋은 로컬 저장소만 갱신한다는 점에서 차이가 난다.\n\n\n\n\n\n\n\n\n\n\n\n힌트원격 설정 고치기\n\n\n\n원격 URL에 오탈자가 발생되는 일이 실무에서 흔히 발생한다. 이번 연습문제는 이런 유형의 이슈를 어떻게 고칠 수 있는지에 대한 것이다. 먼저 잘못된 URL을 원격(remote)에 추가하면서 시작해 보자.\ngit remote add broken https://github.com/this/url/is/invalid\ngit remote로 추가할 때 오류를 받았나요? 원격 URL이 적법한지 확인해 주는 명령어를 생각해 낼 수 있나요? URL을 어떻게 수정할 수 있을까요? (팁: git remote -h를 사용한다.) 이번 연습문제를 수행한 다음에 원격(remote)을 지워버리는 것을 잊지 말자.\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n원격(remote)를 추가할 때 어떤 오류 메시지도 볼 수 없다. (원격 remote를 추가하는 것은 Git에게 알려주기만 할 뿐 아직 사용하지는 않았기 때문이다.)\ngit push 명령어를 사용하자마자, 오류 메시지를 보게 된다. git remote set-url 명령어를 통해서 잘못된 원격 URL을 바꿔 문제를 해결하게 된다.\n\n\n\n\n\n\n\n\n\n\n\n힌트GitHub 라이선스와 README 파일\n\n\n\n이번 장에서 GitHub에 원격 저장소를 생성하는 방법을 배웠다. 하지만 GitHub 저장소를 초기화할 때 README.md 혹은 라이선스 파일을 추가하지 않았다. 로컬 저장소와 원격 저장소를 연결시킬 때 두 파일을 갖게 되면 무슨 일이 발생될 것으로 생각하는가?\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n\n이런 경우, 관련없는 이력 때문에 병합 충돌(merge conflict)이 발생한다. GitHub에서 README.md 파일을 생성시키고 원격 저장소에서 커밋 작업을 수행한다. 로컬 저장소로 원격 저장소를 풀(pull)로 땡겨오면, Git이 origin과 공유되지 않는 이력을 탐지하고 병합(merge)을 거부해 버린다.\n$ git pull origin master\n \nFrom https://github.com/vlad/planets\n  * branch            master     -&gt; FETCH_HEAD\n  * [new branch]      master     -&gt; origin/master\nfatal: refusing to merge unrelated histories\n--allow-unrelated-histories 옵션으로 두 저장소를 강제로 병합(merge)시킬 수 있다. 이런 옵션을 사용할 때는 주의해야 한다. 병합하기 전에 로컬 저장소와 원격 저장소의 콘텐츠를 면밀히 조사해야 한다.\n$ git pull --allow-unrelated-histories origin master\n \nFrom https://github.com/vlad/planets\n  * branch            master     -&gt; FETCH_HEAD\nMerge made by the 'recursive' strategy.\nREADME.md | 1 +\n1 file changed, 1 insertion(+)\ncreate mode 100644 README.md",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>GitHub 원격 작업</span>"
    ]
  },
  {
    "objectID": "git-github.html#footnotes",
    "href": "git-github.html#footnotes",
    "title": "13  GitHub 원격 작업",
    "section": "",
    "text": "암호 문구는 암호(Password)와 비슷하지만 띄어쓰기를 포함한 더 긴 문장이나 구절을 사용한다는 점이 다르다. 예를 들어, “I love coding with ChatGPT!”와 같이 여러 단어로 이루어진 문장을 Passphrase로 사용할 수 있다. 암호 문구가 암호보다 길기 때문에 보안성이 더 높다고 여겨진다. 추측하거나 무작위로 대입하기가 더 어렵기 때문으로 IT 분야에서 암호 대신 Passphrase를 사용하는 것이 점점 더 일반화되고 있는 추세다.↩︎",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>GitHub 원격 작업</span>"
    ]
  },
  {
    "objectID": "git-open.html",
    "href": "git-open.html",
    "title": "14  공개 과학과 협업",
    "section": "",
    "text": "14.1 협업\n정보의 자유로운 공유는 과학에서 이상적일지 모르지만, 현실은 좀 더 복잡하다. 현재 일반적인 현장 상황은 다음과 같다.\n하지만 점점 더 많은 과학자들이 진행하는 연구 프로세스는 다음과 같다.\n이러한 공개 연구 모형은 발견을 가속화한다. 연구 작업이 더 많이 공개될수록 더 많이 인용되고 재사용된다. 하지만 이런 방식으로 작업하고 연구하고자 하는 사람들은 실무에서 “공개(open)”가 정확히 무엇을 의미하는지에 대해 몇 가지 결정을 내릴 필요가 있다. 공개 과학(Open Science)에 관한 다른 측면에 대해서는 Opening Science를 참고한다.\n이것이 버전 제어(version control)를 가르치는 (많은) 이유 중 하나다. 버전 제어가 꾸준히 사용될 때, 컴퓨터 작업에 대한 공유 가능한 전자 연구 노트로 활용되어 “방법”에 대한 질문에 답을 한다.\n앞서 개인 이력관리와 GitHub 저장소에 중점을 두었다면 공개과학을 위한 협업방법을 살펴보자. 먼저, 짝을 이룬다. 한 사람이 “소유자”(연습을 시작하는 데 사용될 GitHub 저장소 주인)가 되고, 다른 사람이 “협력자”(소유자 저장소를 복제해서 변경을 하는 사람)가 된다. 목표는 협력자가 변경 사항을 소유자 저장소에 추가하는 것이다. 마지막에는 역할을 바꿔서 두 사람 모두 소유자와 협력자의 역할을 수행한다.\n소유자가 협력자에게 접근 권한을 부여할 필요가 있다. GitHub에서 오른쪽에 ‘setting’ 버튼을 클릭해서 협력자(Collaborators)를 선택하고 파트너 이름을 입력한다.\n소유자 저장소에 접근 권한이 부여되면 협력자(Collaborator)는 https://github.com/notifications으로 이동한다. 그곳에서 소유자 저장소의 접근을 수락하면 된다.\n다음으로 협력자(Collaborator)는 소유자 저장소 사본을 본인 컴퓨터로 내려받는다. 이런 작업을 “저장소 복제(cloning a repo)”라고 부른다. 소유자의 저장소를 본인 바탕화면(Desktop) 폴더에 클론하려면 협력자는 다음 명령어를 입력한다.\n’vlad’를 소유자 사용자 이름(저장소를 소유하고 있는 사람)으로 바꾼다.\n앞서 작업했던 것과 정확히 동일한 방식으로 협력자는 이제 소유자의 저장소 클론에서 변경을 마음대로 할 수 있다:\n그리고 나서 변경 사항을 GitHub의 소유자 저장소로 푸시한다:\n주목할 점은 origin이라는 원격 저장소를 생성할 필요가 없다는 것이다. 저장소를 복제(clone)할 때 Git이 자동으로 origin 이름을 붙여준다. (수작업으로 원격 설정을 할 때 앞에서 왜 origin 이름을 사용한 것이 현명한 선택인지 이해할 수 있다.)\n이제 GitHub 웹사이트에서 소유자 저장소를 살펴본다(아마도 웹 브라우저를 새로 고침해야 할 수 있다). 협력자가 신규 커밋을 한 것을 확인할 수 있다.\n소유자 로컬 컴퓨터로 GitHub 원본 저장소의 변경 사항을 다운로드하려면 소유자는 다음과 같이 입력한다.\n저장소를 복제(clone)할 때 Git이 자동으로 origin 이름을 붙여주기 때문에, origin이라는 원격 저장소를 따로 생성할 필요가 없다는 점에 주목한다. (이것이 앞에서 원격 설정을 수작업으로 할 때 origin 이름을 사용한 것이 현명한 선택이었던 이유다.)\nGitHub에서 소유자 저장소를 다시 살펴보면, 협력자가 만든 새로운 커밋을 볼 수 있다. 새로운 커밋을 보려면 브라우저를 새로 고침해야 할 수도 있다.\nGitHub에서 협력자의 변경 사항을 다운로드하기 위해 이제 소유자는 다음을 입력한다.\n이제 저장소 3개(소유자 로컬 저장소, 협력자 로컬 저장소, GitHub의 소유자 저장소) 모두 동기화되었다.",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>공개 과학과 협업</span>"
    ]
  },
  {
    "objectID": "git-open.html#git-collab",
    "href": "git-open.html#git-collab",
    "title": "14  공개 과학과 협업",
    "section": "",
    "text": "힌트혼자 훈련하기\n\n\n\n혼자서 쭉 진행해 왔다면 두 번째 터미널을 열어서 계속 실습을 진행할 수 있다. 두 번째 윈도우가 여러분의 협력자를 나타내고 다른 컴퓨터에서 작업하고 있는 것으로 볼 수 있다. GitHub 접근 권한을 다른 사람에게 줄 필요가 없어졌다. 왜냐하면 두 협렵 ‘파트너’ 모두 본인이기 때문이다.\n\n\n\n\n\n\n\n\n\n그림 14.1: GitHub에 협업자(collaborators) 추가\n\n\n\n\n\n$ git clone https://github.com/vlad/planets.git ~/Desktop/vlad-planets\n\n\n\n\n\n\n\n그림 14.2: 저장소 클론한 후 모습\n\n\n\n\n$ cd ~/Desktop/vlad-planets\n$ nano pluto.txt\n$ cat pluto.txt\n\nIt is so a planet!\n$ git add pluto.txt\n$ git commit -m \"Add notes about Pluto\"\n\n 1 file changed, 1 insertion(+)\n create mode 100644 pluto.txt\n\n$ git push origin main\n\nCounting objects: 4, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 306 bytes, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://github.com/vlad/planets.git\n   9272da5..29aba7c  master -&gt; master\n\n\n\n$ git pull origin main\n\nremote: Counting objects: 4, done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 3 (delta 0)\nUnpacking objects: 100% (3/3), done.\nFrom https://github.com/vlad/planets\n * branch            master     -&gt; FETCH_HEAD\nUpdating 9272da5..29aba7c\nFast-forward\n pluto.txt | 1 +\n 1 file changed, 1 insertion(+)\n create mode 100644 pluto.txt\n\n\n\n\n\n\n\n\n힌트원격 저장소에 대해 좀 더 알아보기\n\n\n\n이제 로컬 저장소는 origin이라고 불리는 하나의 “원격 저장소”를 가지고 있었다. 원격 저장소는 다른 곳에 호스팅되어 있는 저장소의 복사본으로, 푸시(push)하고 풀(pull)할 수 있는 곳이며, 하나의 원격 저장소로만 작업해야 할 이유는 없다. 예를 들어, 대규모 프로젝트에서는 자신의 GitHub 계정에 자신만의 복사본을 가질 수도 있고(이를 origin이라고 부를 수 있음), 또한 메인 “상위” 프로젝트 저장소(예제에서는 이를 upstream이라고 부르겠음)를 가질 수 있다. 다른 사람들이 커밋한 최신 업데이트를 받기 위해 수시로 upstream에서 풀을 받을 것이다.\n원격 저장소에 부여하는 이름은 로컬에서만 존재한다는 것을 기억하자. 임의로 선택한 원격 저장소 별칭으로 origin이든, upstream이든, fred든 - 원격 저장소 고유한 것은 절대 아니다.\ngit remote 명령어 계열은 저장소와 연결된 원격 저장소를 설정하고 변경하는 데 사용된다. 가장 유용한 명령어들로 다음과 같은 것들이 있다.\n\ngit remote -v: 설정된 모든 원격 저장소를 나열한다.\ngit remote add [name] [url]: 새로운 원격 저장소를 추가하는 데 사용된다.\ngit remote remove [name]: 원격 저장소를 제거한다. 원격 저장소 자체에는 전혀 영향을 미치지 않고, 단지 로컬 저장소에서 그것에 대한 링크만 제거한다는 점에 유의한다.\ngit remote set-url [name] [newurl]: 원격 저장소와 연결된 URL을 변경한다. 원격 저장소가 이동했을 때 (예: 다른 GitHub 계정으로, 혹은 GitHub에서 다른 호스팅 서비스로), 또는 원격 저장소를 추가할 때 오타를 냈을 때 유용하다!\ngit remote rename [oldname] [newname]: 원격 저장소의 로컬 별칭, 즉 이름을 변경한다. 예를 들어, 이를 사용하여 upstream을 fred로 변경할 수 있다.\n\n\n\n\n$ git pull origin main\n\nremote: Enumerating objects: 4, done.\nremote: Counting objects: 100% (4/4), done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 3 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), done.\nFrom https://github.com/vlad/planets\n * branch            main     -&gt; FETCH_HEAD  \n   9272da5..29aba7c  main     -&gt; origin/main\nUpdating 9272da5..29aba7c\nFast-forward\n pluto.txt | 1 +\n 1 file changed, 1 insertion(+)\n create mode 100644 pluto.txt\n\n\n\n\n\n\n\n힌트기본적인 협업 작업 흐름\n\n\n\n\n실무에서 협업하는 저장소의 가장 최신 버전을 갖도록 확인하는 것이 좋다. 어떤 변경을 가하기 전에 git pull 명령어를 먼저 실행해야 한다. 기본적인 협업 작업 흐름은 다음과 같다:\n\ngit pull origin main 명령어로 본인 로컬 저장소를 최신 상태로 갱신한다.\n변경 작업을 수행하고 git add 명령어로 준비 단계(staging area)로 보낸다.\ngit commit -m 명령어로 변경 사항을 커밋한다.\nGitHub에 git push origin main 명령어로 변경 사항을 업로드한다.\n\n상당한 변경 사항을 포함한 단 한 번의 커밋보다는 작은 변화를 준 커밋을 많이 하는 것이 좋다. 작은 커밋이 가독성도 좋고 리뷰하기도 더 편하다.\n\n\n\n\n\n\n\n\n힌트역할을 바꾸고 반복한다\n\n\n\n역할을 바꿔서 전체 과정을 반복한다.\n\n\n\n\n\n\n\n\n힌트변경 사항 리뷰\n\n\n\n협력자에게 어떤 정보도 주지 않고 소유자가 저장소에 커밋을 푸시했다. 협력자는 명령줄(command line)을 통해 무엇이 변경되었는지 어떻게 알 수 있을까?\n\n\n\n\n\n\n주의해답\n\n\n\n\n\n명령줄에서 협력자는 로컬 저장소에 원격 저장소 변경 사항을 git fetch origin master 명령어를 사용해서 가져올 수 있다. 하지만 그 자체로 병합(merge)되는 것은 아니다. git diff master origin/main 명령어를 실행해서 협력자는 터미널에 변경 사항을 확인할 수 있다. \nGitHub에서도 협력자는 포크된 저장소로 가서 “This branch is 1 commit behind Our-Repository:master.” 메시지를 볼 수 있다. Compare 아이콘과 링크가 걸려 있다. Compare 페이지에서 협력자는 base fork를 본인 저장소로 변경하고 나서 “compare across forks” 위에 링크를 클릭한다. 마지막으로 head fork를 주 저장소로 변경한다. 이 작업을 하게 되면 차이 나는 모든 커밋을 볼 수 있게 된다.\n\n\n\n\n\n\n\n\n\n\n\n힌트GitHub에서 변경 사항 주석 달기\n\n\n\n협력자는 소유자가 변경한 한 줄에 대해 질문을 가질 수 있고 일부 제안 사항도 있다.\nGitHub으로 커밋 차이에 대해 주석을 다는 것도 가능하다. 파란색 주석 아이콘(comment icon)을 클릭하면 주석 윈도우(comment window)를 열 수 있다.\n협력자는 GitHub 인터페이스를 사용해서 코멘트와 제안을 남길 수 있다.\n\n\n\n\n\n\n\n\n힌트버전 이력, 백업, 그리고 버전 제어\n\n\n\n일부 백업 소프트웨어는 파일 버전에 대한 이력을 기록하고 있다. 또한 특정 버전을 복구하는 기능도 제공하고 있다. 이러한 기능이 버전 제어와 어떻게 다른가? 버전 제어, Git, GitHub을 사용하는 좋은 점은 무엇인가?",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>공개 과학과 협업</span>"
    ]
  },
  {
    "objectID": "git-open.html#git-licensing",
    "href": "git-open.html#git-licensing",
    "title": "14  공개 과학과 협업",
    "section": "14.2 라이선싱",
    "text": "14.2 라이선싱\n\n14.2.1 소프트웨어 라이선스\n\n소스 코드, 원고, 다른 창의적 저작물을 갖는 저장소가 공개될 때는 저장소 기반 디렉터리에 LICENSE 혹은 LICENSE.txt 파일을 포함해서 콘텐츠가 어떤 라이선스로 이용 가능한지를 명확히 기술해야 한다. 이유는 소스 코드가 창의적 저작물로서 자동적으로 지적재산(따라서 저작권) 보호 대상에 부합되기 때문이다. 자유로이 이용 가능한 것으로 보여지거나 명시적으로 광고되는 코드라고 해서 그런 보호가 유예되는 것은 아니다. 따라서 라이선스 문장이 없는 코드를 (재)사용하는 누구나 스스로 위험에 처하게 된다. 왜냐하면 소프트웨어 코드 저자가 언제라도 일방적으로 재사용을 불법화할 수 있기 때문이다. 즉, 저작권 소유자가 당신을 저작권법 위반으로 고소할 수 있다.\n라이선스는 그렇지 않다면 보유하지 못할 권리를 다른 사람(라이선스 허여자, licensee)에게 부여함으로써 이 문제를 해결한다. 어떤 조건 아래서 무슨 권리가 부여될지는 라이선스마다 다소 차이가 난다. 독점적 라이선스와 대조적으로, Open Source Initiative에서 공인된 오픈 라이선스(open licences)는 최소한 다음에 나온 권리를 모두 부여한다. 이런 권리를 오픈 소스 정의(Open Source Definition)로 부른다: \n\n소스 코드는 제약 없이 이용 가능하고 사용되고 재배포될 수 있다. 종합 배포의 일부로서도 포함된다.\n변형 혹은 다른 파생 저작물도 허락되고 또한 재배포될 수 있다.\n\n이런 권리를 누가 받느냐의 질문이 차별의 조건이 되지 않는다. 예를 들어 상업적 혹은 학술적처럼 노력 분야에 의해서가 아님도 포함된다.\n\n특히 지금까지 라이선스 몇 개가 인기를 얻고 있는데, choosealicense.com 웹사이트에서 본인 상황에 적합한 일반적인 라이선스를 선택하는 데 도움이 된다. 주요한 고려사항에는 다음이 포함된다:\n\n특허권을 주장하고자 하는가?\n파생 저작물을 배포하는 데 다른 사람도 소스 코드를 배포하도록 강제할 것인가?\n라이선싱하는 콘텐츠가 소스 코드인가?\n이왕이면 소스 코드도 라이선스할 것인가?\n\n적절한 라이선스를 가장 잘 선택하는 것이 상당히 많은 가능한 조합이 있어 주눅이 들 수도 있다. 실무에서 일부 라이선스만 지금까지 가장 인기가 있고 다음이 그 범주에 포함된다: \n\nGNU 일반공중 라이선스 (GPL),\nMIT 라이선스,\nBSD 라이선스,\n아파치 라이선스, 버전 2.0.\n\nGPL은 다른 대부분의 공개 소스 라이선스와 다른데, 전염성이 있는(infective) 특징이 있다. 코드의 수정된 버전을 배포하는 누구나 혹은 GPL 코드를 포함한 어느 것이든지 자신의 코드도 동일하게 자유로이 공개 가능하게 만들어야 한다.\n흔히 사용되는 라이선스를 선택하는 것이 기여자나 사용자의 삶을 편하게 한다. 왜냐하면 기여자나 사용자 모두 해당 라이선스에 친숙해서 사용할 때 상당한 양의 전문 용어를 꼼꼼히 살펴볼 필요가 없기 때문이다.\nOpen Source Initiative와 Free Software Foundation 모두 좋은 선택이 될 수 있는 라이선스 목록을 유지 관리하고 있다.\n코드를 작성하는 과학자 관점에서 라이선싱과 라이선싱 선택지에 대한 전반적인 정보를 [@Morin2012] 에서 살펴볼 수 있다.\n결국 가장 중요한 것은 라이선스가 무엇인지에 대해 분명한 문장이 있는지와 라이선스가 OSI와 FSF에서 승인되고 이미 검증된 것인지 여부다. 또한 저장소에 공개된 것이 아닐지라도, 처음부터 최선으로 라이선스를 선택해야 한다. 결정을 미루는 것은 나중에 더 복잡해진다. 왜냐하면 매번 새로운 협력자가 기여하기 시작하면 협력자도 저작권을 갖게 되기 때문이다. 따라서 라이선스를 고르자마자 승인을 득해야 할 필요가 있다.\n\n\n\n\n\n\n힌트본인이 오픈 라이선스를 사용할 수 있나요?\n\n\n\n여러분이 작성하고 있는 소프트웨어에 오픈 소스 소프트웨어 라이선스를 적용할 수 있는지 알아본다. 여러분이 라이선스 적용을 일방적으로 할 수 있는가? 혹은 여러분의 기관이나 조직의 다른 사람에게서 허락이 필요한가? 만약 그렇다면 누구일까?\n\n\n\n\n\n\n\n\n힌트본인은 어떤 라이선스를 이미 승인했나요?\n\n\n\n매일 사용하는 대다수 소프트웨어는 오픈 소스 소프트웨어로 출시되었다. 아래 목록 혹은 본인이 직접 고른 GitHub 사이트에서 프로젝트를 하나 고른다. 라이선스를 찾아(보통 LICENSE 혹은 COPYING 이름이 붙은 파일)보고, 소프트웨어 사용을 어떻게 제약하는지 살펴본다. 이번 절에서 논의된 라이선스 중 하나인가? 차이점은 어떻게 나는가?\n\nGit, 소스 코드 관리 도구\nCPython, 파이썬 언어 구현\nJupyter, 웹 기반 파이썬 노트북 프로젝트\nEtherPad, 실시간 협업 편집기\n\n\n\n\n\n14.2.2 콘텐츠 라이선스\n \n만약 저장소 콘텐츠가 소프트웨어가 아닌 데이터, 창의적 저작물(매뉴얼, 기술 보고서, 원고) 같은 연구 제품이 포함되면, 소프트웨어를 위해 설계된 라이선스 대부분은 적합하지 않다.\n\n데이터: 대부분 국가 사법권에서 데이터 유형 대부분은 자연에 대한 사실로 간주된다. 그러므로 저작권 보호를 받을 자격이 없다. (단, 아마도 사진과 의료 영상 정보 등은 예외) 따라서 저작자 표시로 사회적 혹은 학자적 기대치를 알리려고 저작권을 정의로 주장하는 방식으로 라이선스를 사용하는 것은 단지 법적으로 혼탁한 상황만 조장할 뿐이다. 크리에이티브 커먼즈 제로(Creative Commons Zero, CC0) 처럼 공중 도메인 권리 포기를 지지하는법적 표시를 분명히 하는 것이 더 낫다. Dryad 데이터 저장소는 사실 이를 요구하고 있다.\n창의적 저작물(Creative works): 매뉴얼, 보고서, 원고, 기타 창의적 저작물은 지적재산 보호 대상이 된다. 따라서 소프트웨어와 마찬가지로 자동으로 저작권으로 보호된다. 크리에이티브 커먼즈(Creative Commons) 조직이 기본 제약사항 4개를 조합해서 라이선스 집합을 마련했다:\n\n저작자 표시(Attribution): 파생 저작물에 대해서 최초 저작자의 이름, 출처 등의 정보를 반드시 표시해야 한다.\n변경 금지(No Derivative): 저작물을 복사할 수도 있으나 저작물을 변경하거나 저작물을 이용하여 2차적 저작물 제작을 금한다.\n동일조건변경허락(Share Alike): 2차적 저작물을 제작할 수 있으나, 2차적 저작물은 원래 저작물과 동일한 라이선스를 적용한다.\n비영리(Noncommercial): 저작물을 영리 목적으로 사용할 수 없다. 영리 목적을 위해서는 별도의 계약이 필요하다.\n\n\n출처 표시 (CC-BY)와 동일조건변경허락(CC-BY-SA) 라이선스만이 “오픈 라이선스”로 간주된다. \n소프트웨어 카펜트리는 가능하면 폭넓게 재사용될 수 있도록 수업 자료에 대해서는 CC-BY, 코드에는 MIT 라이선스를 사용한다. 다시 한번, 가장 중요한 것은 프로젝트 루트 디렉터리에 있는 LICENSE 파일에 라이선스가 무엇인지 분명하게 언급하는 것이다. 본인 프로젝트를 참조하는 방법을 기술하는 데 CITATION 혹은 CITATION.txt 파일을 포함할 수도 있다. 소프트웨어 카펜트리 사례는 다음과 같다:\nTo reference Software Carpentry in publications, please cite both of the following:\n\nGreg Wilson: \"Software Carpentry: Lessons Learned\". arXiv:1307.5448, July 2013.\n\n@online{wilson-software-carpentry-2013,\n  author      = {Greg Wilson},\n  title       = {Software Carpentry: Lessons Learned},\n  version     = {1},\n  date        = {2013-07-20},\n  eprinttype  = {arxiv},\n  eprint      = {1307.5448}\n}",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>공개 과학과 협업</span>"
    ]
  },
  {
    "objectID": "git-open.html#git-hosting",
    "href": "git-open.html#git-hosting",
    "title": "14  공개 과학과 협업",
    "section": "14.3 호스팅",
    "text": "14.3 호스팅\n\n저작물이나 작업을 공개하고자 하는 그룹에서 가지는 두 번째 큰 질문은 코드와 데이터를 어디에 호스팅할지 정하는 것이다. 방법 중 하나는 연구실, 학과, 혹은 대학이 서버를 제공하여 계정 관리와 백업 등을 관리하는 것이다. 주된 장점은 누가 무엇을 소유하는지 명확하다는 점이다. 특히 민감한 정보(예를 들어, 사람에 대한 실험 정보 혹은 특허 출원에 사용될 수도 있는 정보)가 있다면 중요하다. 큰 단점은 서비스 제공 비용과 수명이다. 데이터를 수집하는 데 10년을 보낸 과학자가 지금부터 10년 후에도 여전히 이용 가능하기를 원하지만, 학교 인프라를 지원하는 대부분의 연구 기금의 수명이 턱없이 짧다.\n또 다른 선택지는 도메인을 구입하고 호스팅하는 데 ISP(인터넷 서비스 제공자, Internet Service Provider)에 비용을 지불하는 것이다. 이 접근법은 개인이나 그룹에게 좀 더 많은 제어권을 주고 학교나 기관을 바꿀 때 생기는 문제도 비켜갈 수 있다. 하지만 위나 아래 선택지보다 초기 설정하는 데 더 많은 시간과 노력이 요구된다.\n세 번째 선택지는 GitHub, BitBucket, 혹은 SourceForge 같은 공개 호스팅 서비스를 채용하는 것이다. 웹 인터페이스를 통해서 저장소 코드를 생성하고, 보고, 편집할 수 있게 한다. 이러한 서비스는 이슈 추적, 위키 페이지, 이메일 통보, 코드 리뷰를 포함한 커뮤니케이션과 프로젝트 관리 도구도 제공한다. 이러한 서비스는 규모의 경제와 네트워크 효과로 모두 이익을 볼 수 있다. 즉, 동일한 표준을 갖는 작은 많은 서비스를 실행하는 것보다 큰 서비스 하나를 실행하는 것이 더 쉽다. 또한 사람들이 협업하기도 더 쉽다. 대중적인 서비스를 사용하면 이미 동일한 서비스를 사용하는 커뮤니티와 본인 프로젝트를 연결하는 데 도움이 된다.\n예를 들어 소프트웨어 카펜트리는 GitHub에 있어서 해당 페이지에 대한 소스 코드를 찾아볼 수 있다. GitHub 계정을 갖는 누구나 해당 페이지에 변경 사항을 제안할 수 있다.\nGitHub 저장소에서 Zenodo에 릴리스(release)를 연결하면 DOI를 부여할 수도 있다. 예를 들어 10.5281/zenodo.57467이 “Git 소개”에 대해 주조된 DOI다.\n규모가 크고 잘 정립된 서비스를 사용하는 것이 빠르게 강력한 도구의 장점을 흡수하는 데 도움을 줄 수도 있다. 지속적 통합(Continuous Integration, CI)이 그런 도구 중 하나로 자동으로 소프트웨어 빌드를 돌리고 코드가 커밋되거나 풀 요청이 제출될 때마다 실행된다. 온라인 호스팅 서비스와 CI의 직접 통합은 어떤 풀 요청에도 해당 정보가 존재해서 코드 완결성과 품질 표준을 유지하는 데 도움을 준다는 것을 의미한다. 여전히 CI가 자가 구축한 호스팅 환경에서도 이용 가능하지만 온라인 서비스 사용과 연계되면 초기 설정과 유지 보수 업무를 줄일 수 있다. 더욱이 이러한 도구가 오픈 소스 프로젝트에 무료로 제공되기도 한다. 사설 저장소에 대해서만 비용 일부를 지불하고 이용 가능하다.\n\n\n\n\n\n\n노트제도적 장벽\n\n\n\n공유가 과학에는 이상적이지만 많은 기관에서 공유에 제약을 가한다. 예를 들어 잠재적으로 특허 가능한 지적재산을 보호하는 데 말이다. 만약 여러분이 그런 제약과 마주한다면 특정 프로젝트 혹은 도메인에 예외를 요청하거나, 제도 혁파를 통해서 더 공개된 과학을 지지하도록 좀 더 앞서 나가는 데 근본적인 동기에 관해 질의하는 것이 더 생산적일 수 있다.\n\n\n\n\n\n\n\n\n힌트본인 작업을 공개할 수 있을까?\n\n\n\n본인 작업을 공개 저장소에 공개할 수 있는지 알아보자. 공개 작업을 일방적으로 할 수 있을까? 혹은 속한 조직의 누군가로부터 허락이 필요한가? 만약 그렇다면 조직의 누구일까?\n\n\n\n\n\n\n\n\n힌트본인 작업을 어디에 공개할 수 있을까?\n\n\n\n본인 논문, 데이터, 소프트웨어를 공유하려면 이용 가능한 저장소가 소속 기관에 갖추어져 있는가? 소속 기관 저장소는 arXiV, figshare, GitHub, GitLab과 같은 데이터 저장소 서비스와 비교하여 어떤 차이점이 있는가?",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>공개 과학과 협업</span>"
    ]
  },
  {
    "objectID": "git-conflict.html",
    "href": "git-conflict.html",
    "title": "15  충돌",
    "section": "",
    "text": "사람들이 병렬로 작업을 할 수 있게 됨에 따라, 누군가 다른 사람 작업 영역에 발을 들여 넣을 가능성이 생겼다. 혼자서 작업할 경우에도 이런 현상이 발생한다. 만약 개인 노트북과 연구실 서버에서 소프트웨어 개발을 한다면, 각 작업본에 다른 변경 사항을 만들 수 있다. 버전 제어(version control)는 겹치는 변경 사항을 해결(resolve)하는 도구를 제공함으로써 이러한 충돌(conflicts)을 관리할 수 있게 돕는다.\n충돌을 어떻게 해소할 수 있는지 확인하기 위해서 먼저 파일을 하나 생성하자. mars.txt 파일은 현재 두 협업하는 사람의 planets 저장소 사본에서 다음과 같이 보인다.\n파트너 사본에만 한 줄을 추가하자.\n그리고 나서 변경 사항을 GitHub에 푸시하자.\n이제 다른 파트너가 GitHub에서 갱신(update)하지 않고 본인 사본에 다른 변경 사항을 작업한다.\n로컬 저장소에 변경 사항을 커밋할 수 있다.\n하지만 Git이 GitHub에는 푸시할 수 없게 한다.\nGit이 푸시를 거절한다. 이유는 로컬 브랜치로 반영되지 않는 새로운 업데이트가 원격 저장소에 있음을 Git이 탐지했기 때문이다. 즉, 본인이 작업한 변경 사항이 다른 사람이 작업한 변경 사항과 중첩되는 것을 Git이 탐지해서 앞에서 작업한 것을 덮어쓰지 않도록 정지시킨다. 이제 해야 할 작업은 GitHub에서 변경 사항을 풀(Pull)해서 가져오고 현재 작업 중인 작업본과 병합(merge)해서 푸시한다. 풀(Pull)부터 시작하자.\ngit pull 명령어는 로컬 저장소를 갱신할 때 원격 저장소에 이미 반영된 변경 사항을 포함시키도록 한다. 원격 저장소 브랜치에서 변경 사항을 가져온(fetch) 후에 로컬 저장소 사본의 변경 사항이 원격 저장소 사본과 겹치는 것을 탐지해냈다. 따라서 앞서 작업한 것이 덮어쓰지 않도록 서로 다른 두 버전의 병합(merge)을 승인하지 않고 거절한 것이다. 해당 파일에 충돌나는 부분을 다음과 같이 표시해 놓는다.\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD로 시작되는 부분에 본인 변경 사항이 나와 있다. Git이 자동으로 =======를 넣어 충돌나는 변경 사항 사이에 구분자로 넣고, &gt;&gt;&gt;&gt;&gt;&gt;&gt;기호는 GitHub에서 다운로드된 파일 내용의 마지막을 표시한다. (&gt;&gt;&gt;&gt;&gt;&gt;&gt;표시자 다음에 문자와 숫자로 구성된 문자열은 방금 다운로드한 커밋 번호의 식별자다.)\n파일을 편집해서 표시자/구분자를 제거하고 변경 사항을 일치시키는 것은 전적으로 여러분에게 달려 있다. 원하는 것이면 무엇이든 할 수 있다. 예를 들어 로컬 저장소의 변경 사항을 반영하든, 원격 저장소의 변경 사항을 반영하든, 로컬과 원격 저장소의 내용을 대체하는 새로운 것을 작성하든, 혹은 변경 사항을 완전히 제거하는 것도 가능하다. 로컬과 원격 모두 교체해서 다음과 같이 파일이 보이도록 하자.\n병합을 마무리하기 위해 병합으로 생성된 변경 사항을 mars.txt 파일에 추가하고 커밋한다.\n이제 변경 사항을 GitHub에 푸시할 수 있다.\nGit이 병합하면서 수행한 것을 모두 추적하고 있어서 수작업으로 다시 고칠 필요는 없다. 처음 변경 사항을 만든 협력자 프로그래머가 다시 풀하게 되면 다음과 같은 결과를 얻는다.\n병합된 파일을 얻게 된다.\n다시 병합할 필요는 없는데, 다른 누군가 작업을 했다는 것을 Git이 알기 때문이다.\n충돌을 해소하는 Git 기능은 매우 유용하지만, 충돌 해소에는 시간과 노력이 수반되고, 충돌이 올바르게 해소되지 않으면 오류가 스며들게 된다. 프로젝트 와중에 상당량의 충돌을 해소하는 데 시간을 쓰고 있다고 생각되면, 충돌을 줄일 수 있는 기술적인 접근법도 고려해 보는 것이 좋다.\n좀 더 자주 upstream을 풀(Pull)하기, 특히 새로운 작업을 시작하기 전이라면 더욱 그렇다. 작업을 구별하기 위해 토픽 브랜치를 사용해서 작업을 완료하면 메인(main) 브랜치에 병합시킨다. 좀 더 작게 원자 수준 커밋을 한다. 논리적으로 적절하다면 큰 파일을 좀 더 작은 것으로 쪼갠다. 그렇게 함으로써 두 저자가 동시에 동일한 파일을 변경하는 것을 줄일 수 있을 듯 싶다. 프로젝트 관리 전략으로 충돌을 최소화할 수도 있다.\n프로젝트 관리 전략으로 충돌을 최소화할 수도 있다.",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>충돌</span>"
    ]
  },
  {
    "objectID": "git-conflict.html#footnotes",
    "href": "git-conflict.html#footnotes",
    "title": "15  충돌",
    "section": "",
    "text": "AFK는 “Away From Keyboard”의 약자로 사용자가 컴퓨터 앞에 있지 않다는 것을 나타내는 데 사용된다. 주로 게임이나 채팅 중에 자리를 비울 때 다른 사용자들에게 알리기 위해 사용된다.↩︎",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>충돌</span>"
    ]
  },
  {
    "objectID": "git-ide.html",
    "href": "git-ide.html",
    "title": "16  통합개발환경",
    "section": "",
    "text": "연습문제\n데이터 분석 스크립트를 개발할 때 버전 제어(version control)는 매우 유용하다. R 프로그래밍 언어 전용 통합개발환경 RStudio는 Git과 통합이 잘 되어 있다. 몇몇 고급 Git 기능은 명령 프롬프트가 필요하지만, 일반적인 Git 작업에 대한 좋은 인터페이스를 RStudio을 통해 무료로 제공되고 있다.\nRStudio에서 작업 디렉토리와 연관된 프로젝트를 생성을 통해 관련 파일을 추적한다. Rstudio 프로젝트에 Git을 통합시켜 버전 제어를 함으로써 프로젝트 개발과정을 시간에 따라 추적하고, 이전 버전으로 되돌릴 수 있으며, 다른 사람과 협업을 원활히 수행할 수 있다. RStudio에서 Git을 이용하여 시작하려면, 새 프로젝트를 생성한다.\n프로젝트 생성 방식에 대해 묻는 대화 상자가 열린다. 여기서 몇 가지 옵션을 선택할 수 있다. 가령 이미 만들어 둔 planets 저장소를 RStudio에서 Git 저장소로 재사용한다고 가정해 보자. 저장소가 컴퓨터 디렉토리에 있으므로, “Existing Directory”(“기존 디렉토리”) 옵션을 선택한다.\n다음으로, RStudio가 기존 디렉토리를 사용하려는지 물어보게 된다. “Browse…”(“찾아보기…”)를 클릭해서 원하는 디렉토리로 이동한 후 “Create Project”(“프로젝트 생성”)를 클릭한다.\n이제 완성! 기존 planets 저장소에 RStudio 새 프로젝트를 생성했다. 메뉴에서 “Git” 메뉴를 확인해 보자. RStudio는 현재 디렉토리를 Git 저장소로 올바르게 인식했다. 또한, RStudio를 통해 Git을 자유롭게 활용할 수 있도록 필요한 다양한 도구들이 제공된다.\n저장소 기존 파일들을 편집하기 위해서, 우측 하단 “Files”(“파일”) 패널에서 클릭할 수 있다. 이제 명왕성에 대한 몇 가지 추가 정보를 명왕성 파일(pluto.txt)에 추가한다.\n파일을 편집한 후 Git 메뉴에서 “Commit…”(“커밋…”)을 클릭하여 변경사항을 저장할 수 있다.\n이제 어떤 파일을 커밋할지 선택할 수 있게 하고, 커밋 메시지를 입력할 수 있는 대화 상자를 연다. (“Staged” 적절한 상자를 체크하면 된다). “Status” 아이콘은 각 파일의 현재 상태를 나타낸다. 파일을 클릭하면 변경사항에 대한 정보가 아래 패널에 펼쳐진다.(git diff 출력이 활용된다). 모든 것이 원하는 대로 되면, “커밋”(“Commit”)을 클릭한다.\n변경사항을 푸시하려면 Git 메뉴에서 “브랜치 푸시”(“Push Branch”)를 선택하면 된다. 원격 저장소에서 가져오는(“Pull Branch”) 옵션도 있으며, 커밋 이력(“History”)을 보는 옵션도 있다:\n만약 “이력”(“History”)을 클릭한다면, 콘솔에서 git log 출력과 것과 같은 그래픽 버전으로 확인할 수 있다.\nRStudio는 프로젝트를 추적하는 데 많은 파일들을 자동생성한다. 종종 추적하고 싶지 않은 경우가 있다. 예를 들어, 데이터 크기가 큰 파일이나 API 인증키 같은 보안 정보가 포함된 파일들이다. 이럴 경우 파일명 혹은 디렉토리명을 .gitignore 파일에 추가한다.",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>통합개발환경</span>"
    ]
  },
  {
    "objectID": "git-ide.html#연습문제",
    "href": "git-ide.html#연습문제",
    "title": "16  통합개발환경",
    "section": "",
    "text": "새로운 디렉토리를 만들어서 프로젝트에 graphs 디렉토리를 추가하시오.\n.gitignore 파일을 수정하여 graphs 디렉토리를 버전 제어에서 제외시키시오.",
    "crumbs": [
      "**2부** 버전제어와 협업",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>통합개발환경</span>"
    ]
  },
  {
    "objectID": "tool-regex.html",
    "href": "tool-regex.html",
    "title": "17  정규 표현식",
    "section": "",
    "text": "17.1 문자 매칭\n지금까지 파일을 훑어서 패턴을 찾고, 관심 있는 라인에서 다양한 비트(bits)를 뽑아냈다. stringr 패키지 str_split, str_detect 같은 문자열 함수를 사용하였고, 라인에서 일정 부분을 뽑아내기 위해서 리스트와 문자열 슬라이싱(slicing)을 사용했다.\n검색하고 추출하는 작업은 너무 자주 있는 일이어서 R과 파이썬 모두 상기와 같은 작업을 매우 우아하게 처리하는 정규 표현식(regular expressions)으로 불리는 매우 강력한 라이브러리를 제공한다. 정규 표현식을 책의 앞부분에 소개하지 않은 이유는 정규 표현식 라이브러리가 매우 강력하지만, 약간 복잡하고, 구문에 익숙해지는 데 시간이 필요하기 때문이다.\n정규표현식은 문자열을 검색하고 파싱하는 데 그 자체가 작은 프로그래밍 언어다. 사실, 책 전체가 정규 표현식을 주제로 쓰여진 책이 몇 권 있다. 이번 장에서는 정규 표현식의 기초만을 다룰 것이다. 정규 표현식의 좀 더 자세한 사항은 다음을 참조한다.\nR에서 정규표현식을 지원하는 패키지는 많지만, 대표적으로 stringr 패키지가 활용 사례도 많고 문서화도 충실하다. 정규 표현식 패키지를 사용하기 전에 패키지를 가져와야 한다. 정규 표현식 패키지의 가장 간단한 쓰임은 str_detect() 검색 함수다. 다음 프로그램은 검색 함수의 사소한 사용 예를 보여준다.\n파일을 열고, 각 라인을 루프로 반복해서 정규 표현식 stringr::str_detect() 함수를 호출하여 문자열 “From”이 포함된 라인만 출력한다. 상기 프로그램에는 진정으로 강력한 정규 표현식 기능이 사용되지 않았다. 왜냐하면, 다른 함수를 가지고도 동일한 결과를 쉽게 구현할 수 있기 때문이다.\n정규 표현식의 강력한 기능은 문자열에 해당하는 라인을 좀 더 정확하게 제어하기 위해서 검색 문자열에 특수문자를 추가할 때 확인될 수 있다. 매우 적은 코드를 작성할지라도, 정규 표현식에 특수 문자를 추가하는 것만으로도 정교한 일치(matching)와 추출이 가능하게 한다.\n예를 들어, 탈자 기호(caret)는 라인의 “시작”과 일치하는 정규 표현식에 사용된다. 다음과 같이 “From:”으로 시작하는 라인만 일치하도록 응용프로그램을 변경할 수 있다.\n“From:” 문자열로 시작하는 라인만 일치할 수 있다. 여전히 매우 간단한 프로그램으로 다른 패키지에서도 다양한 함수로 동일하게 수행할 수 있다. 하지만, 무엇을 정규 표현식과 매칭하는가에 대해서 특수 액션 문자(^)를 담아 강력한 제어를 수행하는 정규 표현식 개념을 소개하기에는 충분하다.\n좀 더 강력한 정규 표현식을 작성할 수 있는 다른 특수문자는 많이 있다. 가장 자주 사용되는 특수 문자는 임의 문자를 매칭하는 마침표다.\n다음 예제에서 정규 표현식 “F..m:”은 “From:”, “Fxxm:”, “F12m:”, “F!@m:’ 같은 임의 문자열을 매칭한다. 왜냐하면 정규 표현식 마침표 문자가 임의의 문자와 매칭되기 때문이다.\n정규 표현식에 “*”, “+’ ’ 문자를 사용하여 문자가 원하는 만큼 반복을 나타내는 기능과 결합되었을 때는 더욱 강력해진다.”*“,”+’ ’ 특수 문자가 검색 문자열에 문자 하나만을 매칭하는 대신에 별표 기호인 경우 0회 혹은 그 이상의 매칭, 더하기 기호인 경우 1회 혹은 그 이상의 문자의 매칭을 의미한다.\n다음 예제에서 반복 와일드 카드(wild card) 문자를 사용하여 매칭하는 라인을 좀 더 좁힐 수 있다.\n검색 문자열 “^From:.+@” 은 “From:” 으로 시작하고, “.+” 하나 혹은 그 이상의 문자들, 그리고 @ 기호와 매칭되는 라인을 성공적으로 찾아낸다. 그래서 다음 라인은 매칭이 될 것이다.\nFrom: stephen.marquard@uct.ac.za\n콜론(:)과 @ 기호 사이의 모든 문자들을 매칭하도록 확장하는 것으로 “.+” 와일드 카드를 간주할 수 있다.\nFrom:.+ @\n더하기와 별표 기호를 “밀어내기(pushy)” 문자로 생각하는 것이 좋다. 예를 들어, 다음 문자열은 “.+” 특수문자가 다음에 보여주듯이 밖으로 밀어내는 것처럼 문자열 마지막 @ 기호를 매칭한다.\nFrom:stephen.marquard@uct.ac.za, csev@umich.edu, and cwen @iupui.edu\n다른 특수문자를 추가함으로써 별표나 더하기 기호가 너무 “탐욕(greedy)”스럽지 않게 만들 수 있다. 와일드 카드 특수문자의 탐욕스러운 기능을 끄는 것에 대해서는 자세한 정보를 참조하기 바란다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>정규 표현식</span>"
    ]
  },
  {
    "objectID": "tool-regex.html#regex-char-matching",
    "href": "tool-regex.html#regex-char-matching",
    "title": "17  정규 표현식",
    "section": "",
    "text": "R\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>정규 표현식</span>"
    ]
  },
  {
    "objectID": "tool-regex.html#regex-extraction",
    "href": "tool-regex.html#regex-extraction",
    "title": "17  정규 표현식",
    "section": "\n17.2 데이터 추출",
    "text": "17.2 데이터 추출\nR stringr 패키지로 문자열에서 데이터를 추출하려면, str_extract_all() 함수를 사용해서 정규 표현식과 매칭되는 모든 부속 문자열을 추출할 수 있다. 형식에 관계없이 임의 라인에서 전자우편 주소 같은 문자열을 추출하는 예제를 사용해보자. 예를 들어, 다음 각 라인에서 전자우편 주소를 뽑아내고자 한다.\nFrom stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008\nReturn-Path: &lt;postmaster@collab.sakaiproject.org&gt;\n          for &lt;source@collab.sakaiproject.org&gt;;\nReceived: (from apache@localhost)\nAuthor: stephen.marquard@uct.ac.za\n각각의 라인에 대해서 다르게 쪼개고, 슬라이싱하면서 라인 각각의 형식에 맞추어 코드를 작성하고는 싶지는 않다. 다음 프로그램은 str_extract_all() 함수를 사용하여 전자우편 주소가 있는 라인을 찾아내고 하나 혹은 그 이상의 주소를 뽑아낸다. \n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nstr_extract_all() 함수는 두 번째 인자 패턴을 갖는 문자열을 찾아서 전자우편 주소처럼 보이는 모든 문자열을 리스트로 반환한다. 공백이 아닌 문자(\\\\S)와 매칭되는 가운데 @을 갖는 두 문자열 시퀀스(sequence)를 매칭한다.\n프로그램의 출력은 다음과 같다.\n\n[[1]]\n[1] \"csev@umich.edu\" \"cwen@iupui.edu\"\n\n정규 표현식을 해석하면, 적어도 하나의 공백이 아닌 문자, @과 적어도 하나 이상의 공백이 아닌 문자를 가진 부속 문자열을 찾는다. 또한, “\\\\S+” 특수 문자는 가능한 많이 공백이 아닌 문자를 매칭한다. (정규 표현식에서 “탐욕(greedy)” 매칭이라고 부른다.)\n정규 표현식은 두 번 매칭(csev@umich.edu, cwen@iupui.edu)하지만, 문자열 “@2PM”은 매칭을 하지 않는다. 왜냐하면, @ 기호 앞에 공백이 아닌 문자가 하나도 없기 때문이다. 프로그램의 정규 표현식을 사용해서 파일의 모든 라인을 읽고 다음과 같이 전자우편 주소처럼 보이는 모든 문자열을 출력한다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n각 라인을 읽어 들이고, 정규 표현식과 매칭되는 모든 부속 문자열을 추출한다. str_extract() 함수는 문자 벡터를 반환하기 때문에, 전자우편 처럼 보이는 부속 문자열을 적어도 하나 찾아서 출력하기 위해서 반환 리스트 요소 숫자가 NA 여부를 간단히 확인한다.\n원데이터 mbox.txt 파일에 프로그램을 실행하면, 다음 출력을 얻는다.\n[1] \"stephen.marquard@uct.ac.za\"\n[1] \"&lt;postmaster@collab.sakaiproject.org&gt;\"\n[1] \"&lt;200801051412.m05ECIaH010327@nakamura.uits.iupui.edu&gt;\"\n[1] \"&lt;source@collab.sakaiproject.org&gt;;\"\n[1] \"&lt;source@collab.sakaiproject.org&gt;;\"\n[1] \"&lt;source@collab.sakaiproject.org&gt;;\"\n[1] \"apache@localhost)\"\n[1] \"source@collab.sakaiproject.org;\"\n[1] \"stephen.marquard@uct.ac.za\"\n[1] \"source@collab.sakaiproject.org\"\n[1] \"stephen.marquard@uct.ac.za\"\n[1] \"stephen.marquard@uct.ac.za\"\n[1] \"louis@media.berkeley.edu\"\n...\n전자우편 주소 몇몇은 “&lt;”, “;” 같은 잘못된 문자가 앞과 뒤에 붙어있다. 문자나 숫자로 시작하고 끝나는 문자열 부분만 관심 있다고 하자.\n그러기 위해서, 정규 표현식의 또 다른 기능을 사용한다. 매칭하려는 다중 허용 문자 집합을 표기하기 위해서 꺾쇠 괄호를 사용한다. 그런 의미에서 “\\S”은 공백이 아닌 문자 집합을 매칭하게 한다. 이제 매칭하려는 문자에 관해서 좀 더 명확해졌다.\n여기 새로운 정규 표현식이 있다.\n[a-zA-Z0-9]\\\\S*@\\\\S*[a-zA-Z]\n약간 복잡해졌다. 왜 정규 표현식이 자신만의 언어인가에 대해서 이해할 수 있다. 이 정규 표현식을 해석하면, 0회 혹은 그 이상의 공백이 아닌 문자(“\\\\S*”)로 하나의 소문자, 대문자 혹은 숫자(“[a-zA-Z0-9]”)를 가지며, @ 다음에 0회 혹은 그 이상의 공백이 아닌 문자(“\\\\S*”)로 하나의 소문자, 대문자 혹은 숫자(“[a-zA-Z0-9]”)로 된 부속 문자열을 찾는다. 0회 혹은 그 이상의 공백이 아닌 문자를 나타내기 위해서 “+”에서 “”으로 바꿨다. 왜냐하면 ”[a-zA-Z0-9]” 자체가 이미 하나의 공백이 아닌 문자이기 때문이다. ””, “+”는 단일 문자에 별표, 더하기 기호 왼편에 즉시 적용됨을 기억한다. \n프로그램에 정규 표현식을 사용하면, 데이터가 훨씬 깔끔해진다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n[1] \"stephen.marquard@uct.ac.za\"\n[1] \"postmaster@collab.sakaiproject.org\"\n[1] \"200801051412.m05ECIaH010327@nakamura.uits.iupui.edu\"\n[1] \"source@collab.sakaiproject.org\"\n[1] \"source@collab.sakaiproject.org\"\n[1] \"source@collab.sakaiproject.org\"\n[1] \"apache@localhost\"\n[1] \"source@collab.sakaiproject.org\"\n[1] \"stephen.marquard@uct.ac.za\"\n[1] \"source@collab.sakaiproject.org\"\n[1] \"stephen.marquard@uct.ac.za\"\n[1] \"stephen.marquard@uct.ac.za\"\n[1] \"louis@media.berkeley.edu\"\n[1] \"postmaster@collab.sakaiproject.org\"\n[1] \"200801042308.m04N8v6O008125@nakamura.uits.iupui.edu\"\n[1] \"source@collab.sakaiproject.org\"\n[1] \"source@collab.sakaiproject.org\"\n[1] \"source@collab.sakaiproject.org\"\n[1] \"apache@localhost\"\n...\n“source@collab.sakaiproject.org” 라인에서 문자열 끝에 “&gt;” 문자를 정규 표현식으로 제거한 것을 주목한다. 정규 표현식 끝에 “[a-zA-Z]”을 추가하여서 정규 표현식 파서가 찾는 임의 문자열은 문자로만 끝나야 되기 때문이다. 그래서, “sakaiproject.org&gt;;”에서 “&gt;”을 봤을 때, “g”가 마지막 맞는 매칭이 되고, 거기서 마지막 매칭을 마치고 중단한다.\nstr_extract_all() 프로그램의 출력은 리스트의 단일 요소를 가진 문자열로 R 리스트이다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>정규 표현식</span>"
    ]
  },
  {
    "objectID": "tool-regex.html#regex-search-extraction",
    "href": "tool-regex.html#regex-search-extraction",
    "title": "17  정규 표현식",
    "section": "\n17.3 검색과 추출 조합",
    "text": "17.3 검색과 추출 조합\n다음과 같은 “X-” 문자열로 시작하는 라인의 숫자를 찾고자 한다면,\nX-DSPAM-Confidence: 0.8475\nX-DSPAM-Probability: 0.0000  \n임의의 라인에서 임의 부동 소수점 숫자가 아니라 상기 구문을 가진 라인에서만 숫자를 추출하고자 한다.\n라인을 선택하기 위해서 다음과 같이 정규 표현식을 구성한다.\n^X-.*: [0-9.]+\n정규 표현식을 해석하면, ^에서 “X-”으로 시작하고, “.*“에서 0회 혹은 그 이상의 문자를 가지며, 콜론(”:“)이 나오고 나서 공백을 만족하는 라인을 찾는다. 공백 뒤에”[0-9.]+“에서 숫자(0-9) 혹은 점을 가진 하나 혹은 그 이상의 문자가 있어야 한다. 꺾쇠 기호 사이에 마침표는 실제 마침표만 매칭함을 주목하기 바란다. (즉, 꺾쇠 기호 사이는 와일드 카드 문자가 아니다.)\n관심을 가지고 있는 특정한 라인과 매우 정확하게 매칭이 되는 매우 빠듯한 정규 표현식으로 다음과 같다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n프로그램을 실행하면, 잘 걸러져서 찾고자 하는 라인만 볼 수 있다.\n[1] \"X-DSPAM-Confidence: 0.8475\"\n[1] \"X-DSPAM-Probability: 0.0000\"\n[1] \"X-DSPAM-Confidence: 0.6178\"\n[1] \"X-DSPAM-Probability: 0.0000\"\n[1] \"X-DSPAM-Confidence: 0.6961\"\n...\n하지만, 이제 str_split() 함수를 사용해서 숫자를 뽑아내는 문제를 해결해야 한다. str_split()을 사용하는 것이 간단해 보이지만, 동시에 라인을 검색하고 파싱하기 위해서 정규 표현식의 또 다른 기능을 사용할 수 있다.\n\n괄호는 정규 표현식의 또 다른 특수 문자다. 정규 표현식에 괄호를 추가한다면, 문자열이 매칭될 때, 무시된다. 하지만, str_match()를 사용할 때, 매칭할 전체 정규 표현식을 원할지라도, 정규 표현식을 매칭하는 부속 문자열의 부분만을 뽑아낸다는 것을 괄호가 표시한다.\n \n그래서, 프로그램을 다음과 같이 수정한다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nstr_detect()을 호출하는 대신에, 매칭 문자열의 부동 소수점 숫자만 뽑아내는 데 str_match()에 원하는 부동 소수점 숫자를 표현하는 정규 표현식 부분에 괄호를 추가한다.\n프로그램의 출력은 다음과 같다.\n[1] \"0.8475\"\n[1] \"0.0000\"\n[1] \"0.6178\"\n[1] \"0.0000\"\n[1] \"0.6961\"\n[1] \"0.0000\"\n[1] \"0.7565\"\n[1] \"0.0000\"\n[1] \"0.7626\"\n...\n숫자가 여전히 리스트에 있어서 문자열에서 부동 소수점으로 변환할 필요가 있지만, 흥미로운 정보를 찾아 뽑아내기 위해서 정규 표현식의 강력한 힘을 사용했다.\n이 기술을 활용한 또 다른 예제로, 파일을 살펴보면, 폼(form)을 가진 라인이 많다.\nDetails: http://source.sakaiproject.org/viewsvn/?view=rev&rev=39772\n상기 언급한 동일한 기법을 사용하여 모든 변경 번호(라인의 끝에 정수 숫자)를 추출하고자 한다면 다음과 같이 프로그램을 작성할 수 있다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n작성한 정규 표현식을 해석하면, “Details:”로 시작하는 “.*“에 임의의 문자들로,”rev=“을 포함하고 나서, 하나 혹은 그 이상의 숫자를 가진 라인을 찾는다. 전체 정규 표현식을 만족하는 라인을 찾고자 하지만, 라인 끝에 정수만을 추출하기 위해서”[0-9]+“을 괄호로 감쌌다.\n프로그램을 실행하면, 다음 출력을 얻는다.\n[1] \"39772\"\n[1] \"39771\"\n[1] \"39770\"\n[1] \"39769\"\n[1] \"39766\"\n[1] \"39765\"\n[1] \"39764\"\n...\n“[0-9]+”은 “탐욕(greedy)”스러워서, 숫자를 추출하기 전에 가능한 큰 문자열 숫자를 만들려고 한다는 것을 기억하라. 이런 “탐욕(greedy)”스러운 행동으로 인해서 왜 각 숫자로 모두 5자리 숫자를 얻은 이유가 된다. 정규 표현식 라이브러리는 양방향으로 파일 처음이나 끝에 숫자가 아닌 것을 마주칠 때까지 뻗어 나간다.\n이제 정규 표현식을 사용해서 각 전자우편 메시지의 요일에 관심이 있었던 책 앞의 연습 프로그램을 다시 작성한다. 다음 형식의 라인을 찾는다.\nFrom stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008\n그리고 나서, 각 라인의 요일의 시간을 추출하고자 한다. 앞에서 str_split를 두 번 호출하여 작업을 수행했다. 첫 번째는 라인을 단어로 쪼개고, 다섯 번째 단어를 뽑아내서, 관심 있는 두 문자를 뽑아내기 위해서 콜론 문자에서 다시 쪼갰다.\n작동을 할지 모르지만, 실질적으로 정말 부서지기 쉬운 코드로 라인이 잘 짜여져 있다고 가정하에 가능하다. 잘못된 형식의 라인이 나타날 때도 결코 망가지지 않는 프로그램을 담보하기 위해서 충분한 오류 검사 기능을 추가하거나 커다란 try/except 블록을 넣으면, 참 읽기 힘든 10-15 라인 코드로 커질 것이다.\n다음 정규 표현식으로 훨씬 간결하게 작성할 수 있다.\n^From .* [0-9][0-9]:\n상기 정규 표현식을 해석하면, 공백을 포함한 “From”으로 시작해서, “.*“에 임의 개수의 문자, 그리고 공백, 두 개의 숫자”[0-9][0-9]” 뒤에 콜론(:) 문자를 가진 라인을 찾는다. 일종의 찾고 있는 라인에 대한 정의다.\nstr_extract() 함수를 사용해서 단지 시간만 뽑아내기 위해서, 두 숫자에 괄호를 다음과 같이 추가한다.\n^From .* ([0-9][0-9]):\n작업 결과는 다음과 같이 프로그램에 반영한다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n프로그램을 실행하면, 다음 출력 결과가 나온다.\n[1] \"09\"\n[1] \"18\"\n[1] \"16\"\n[1] \"15\"\n[1] \"15\"\n[1] \"14\"\n[1] \"11\"\n[1] \"11\"\n...",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>정규 표현식</span>"
    ]
  },
  {
    "objectID": "tool-regex.html#regex-escape",
    "href": "tool-regex.html#regex-escape",
    "title": "17  정규 표현식",
    "section": "\n17.4 이스케이프 문자",
    "text": "17.4 이스케이프 문자\n라인의 처음과 끝을 매칭하거나, 와일드 카드를 명세하기 위해서 정규 표현식의 특수 문자를 사용했기 때문에, 정규 표현식에 사용된 문자가 “정상(normal)”적인 문자임을 표기할 방법이 필요하고 달러 기호와 탈자 기호(^) 같은 실제 문자를 매칭하고자 한다.\n역슬래시(\\)을 가진 문자를 앞에 덧붙여서 문자를 단순히 매칭하고자 한다고 나타낼 수 있다. 예를 들어, 다음 정규표현식으로 금액을 찾을 수 있다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n역슬래시 달러 기호를 앞에 덧붙여서(\\\\$), 실제로 “라인 끝(end of line)” 매칭 대신에 입력 문자열의 달러 기호와 매칭한다. 정규 표현식 나머지 부분은 하나 혹은 그 이상의 숫자 혹은 소수점 문자를 매칭한다. 주목: 꺾쇠 괄호 내부에 문자는 “특수 문자”가 아니다. 그래서 “[0-9.]”은 실제 숫자 혹은 점을 의미한다. 꺾쇠 괄호 외부에 점은 “와일드 카드(wild-card)” 문자이고 임의의 문자와 매칭한다. 꺾쇠 괄호 내부에서 점은 점일 뿐이다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>정규 표현식</span>"
    ]
  },
  {
    "objectID": "tool-regex.html#regex-summary",
    "href": "tool-regex.html#regex-summary",
    "title": "17  정규 표현식",
    "section": "\n17.5 요약",
    "text": "17.5 요약\n지금까지 정규 표현식의 표면을 긁은 정도지만, 정규 표현식 언어에 대해서 조금 학습했다. 정규 표현식은 특수 문자로 구성된 검색 문자열로 “매칭(matching)”을 정의하고 매칭된 문자열로부터 추출된 결과물을 정규 표현식 시스템과 프로그래머가 의도한 바를 의사소통하는 것이다. 다음에 특수 문자 및 문자 시퀀스의 일부가 있다.\n\n\n^ 라인의 처음을 매칭.\n\n$ 라인의 끝을 매칭.\n\n. 임의의 문자를 매칭(와일드 카드)\n\ns 공백 문자를 매칭.\n\nS 공백이 아닌 문자를 매칭.(s 의 반대).\n\n* 바로 앞선 문자에 적용되고 0회 혹은 그 이상의 앞선 문자와 매칭을 표기.\n\n*? 바로 앞선 문자에 적용되고 0회 혹은 그 이상의 앞선 문자와 매칭을 “탐욕적이지 않은(non-greedy) 방식”으로 표기.\n\n+ 바로 앞선 문자에 적용되고 1회 혹은 그 이상의 앞선 문자와 매칭을 표기.\n\n+? 바로 앞선 문자에 적용되고 1회 혹은 그 이상의 앞선 문자와 매칭을 “탐욕적이지 않은(non-greedy) 방식”으로 표기.\n\n[aeiou] 명세된 집합 문자에 존재하는 단일 문자와 매칭. 다른 문자는 안 되고, “a”, “e”, “i”, “o”, “u” 문자만 매칭되는 예제.\n\n[a-z0-9] 음수 기호로 문자 범위를 명세할 수 있다. 소문자이거나 숫자인 단일 문자만 매칭되는 예제.\n\n[^A-Za-z] 집합 표기의 첫 문자가 ^인 경우, 로직을 거꾸로 적용한다. 대문자나 혹은 소문자가 아닌 임의 단일 문자만 매칭하는 예제.\n\n( ) 괄호가 정규표현식에 추가될 때, 매칭을 무시한다. 하지만 str_extract()을 사용할 때 전체 문자열보다 매칭된 문자열의 상세한 부속 문자열을 추출할 수 있게 한다.\n\n\\b 빈 문자열을 매칭하지만, 단어의 시작과 끝에만 사용.\n\n\\B 빈 문자열을 매칭하지만, 단어의 시작과 끝이 아닌 곳에 사용.\n\n\\d 임의 숫자와 매칭하여 [0-9] 집합에 상응.\n\n\\D 임의 숫자가 아닌 문자와 매칭하여 [^0-9] 집합에 상응.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>정규 표현식</span>"
    ]
  },
  {
    "objectID": "tool-regex.html#unix-users",
    "href": "tool-regex.html#unix-users",
    "title": "17  정규 표현식",
    "section": "\n17.6 유닉스 사용자 보너스",
    "text": "17.6 유닉스 사용자 보너스\n\n정규 표현식을 사용하여 파일을 검색하는 기능은 1960년대 이래로 유닉스 운영 시스템에 내장되어 여러 가지 형태로 거의 모든 프로그래밍 언어에서 이용 가능하다.\n사실, str_detect() 예제에서와 거의 동일한 기능을 하는 grep (Generalized Regular Expression Parser)으로 불리는 유닉스 내장 명령어 프로그램이 있다. 그래서, 맥킨토시나 리눅스 운영 시스템을 가지고 있다면, 명령어 창에서 다음 명령어를 시도할 수 있다.\n$ grep '^From:' mbox-short.txt\nFrom: stephen.marquard@uct.ac.za\nFrom: louis@media.berkeley.edu\nFrom: zqian@umich.edu\nFrom: rjlowe@iupui.edu\ngrep을 사용하여, mbox-short.txt 파일 내부에 “From:” 문자열로 시작하는 라인을 보여준다. grep 명령어를 가지고 약간 실험을 하고 grep에 대한 문서를 읽는다면, 파이썬에서 지원하는 정규표현식과 grep에서 지원되는 정규 표현식과 차이를 발견할 것이다. 예를 들어, grep은 공백이 아닌 문자 “\\S”을 지원하지 않는다. 그래서 약간 더 복잡한 집합 표기 “[^ ]”을 사용해야 한다. “[^ ]”은 간단히 정리하면, 공백을 제외한 임의의 문자와 매칭한다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>정규 표현식</span>"
    ]
  },
  {
    "objectID": "tool-regex.html#regex-debugging",
    "href": "tool-regex.html#regex-debugging",
    "title": "17  정규 표현식",
    "section": "\n17.7 디버깅",
    "text": "17.7 디버깅\nR에 대해서 도움을 어떻게 받을 수 있을까?\n만약 특정 함수의 정확한 이름을 기억해 내기 위해서 빠르게 생각나게 하는 것이 필요하다면 도움이 많이 될 수 있는 간단하고 초보적인 내장 문서가 R에 포함되어 있다. 내장 문서 도움말은 인터랙티브 모드의 R 인터프리터에서 볼 수 있다.\n\n17.7.1 도움말 파일 읽어오기\n특정한 패키지를 사용하고자 한다면, ls(pos = \"package:패키지명) 명령어를 사용하여 다음과 같이 패키지에 포함된 함수를 찾을 수 있다. stringr 패키지는 파이썬 내장 re 패키지와 유사한 패턴 매칭 함수를 제공한다.\n\nls(pos = \"package:stringr\")\n#&gt;  [1] \"%&gt;%\"               \"boundary\"          \"coll\"             \n#&gt;  [4] \"fixed\"             \"fruit\"             \"invert_match\"     \n#&gt;  [7] \"regex\"             \"sentences\"         \"str_c\"            \n#&gt; [10] \"str_conv\"          \"str_count\"         \"str_detect\"       \n#&gt; [13] \"str_dup\"           \"str_ends\"          \"str_equal\"        \n#&gt; [16] \"str_escape\"        \"str_extract\"       \"str_extract_all\"  \n#&gt; [19] \"str_flatten\"       \"str_flatten_comma\" \"str_glue\"         \n#&gt; [22] \"str_glue_data\"     \"str_interp\"        \"str_length\"       \n#&gt; [25] \"str_like\"          \"str_locate\"        \"str_locate_all\"   \n#&gt; [28] \"str_match\"         \"str_match_all\"     \"str_order\"        \n#&gt; [31] \"str_pad\"           \"str_rank\"          \"str_remove\"       \n#&gt; [34] \"str_remove_all\"    \"str_replace\"       \"str_replace_all\"  \n#&gt; [37] \"str_replace_na\"    \"str_sort\"          \"str_split\"        \n#&gt; [40] \"str_split_1\"       \"str_split_fixed\"   \"str_split_i\"      \n#&gt; [43] \"str_squish\"        \"str_starts\"        \"str_sub\"          \n#&gt; [46] \"str_sub_all\"       \"str_sub&lt;-\"         \"str_subset\"       \n#&gt; [49] \"str_to_lower\"      \"str_to_sentence\"   \"str_to_title\"     \n#&gt; [52] \"str_to_upper\"      \"str_trim\"          \"str_trunc\"        \n#&gt; [55] \"str_unique\"        \"str_view\"          \"str_view_all\"     \n#&gt; [58] \"str_which\"         \"str_width\"         \"str_wrap\"         \n#&gt; [61] \"word\"              \"words\"\n\nR과 모든 팩키지는 함수에 대한 도움말 파일을 제공한다. 네임스페이스(인터랙티브 R 세션)에 적재된 팩키지에 있는 특정 함수에 대한 도움말은 다음과 같이 찾는다.\n\n?function_name\nhelp(function_name)\n\nRStudio에 도움말 페이지에 도움말이 표시된다. (혹은 R 자체로 일반 텍스트로 표시된다) 각 도움말 페이지는 절(section)로 구분된다:\n\n기술(Description): 함수가 어떤 작업을 수행하는가에 대한 충분한 기술\n사용법(Usage): 함수 인자와 기본 디폴트 설정값\n인자(Arguments): 각 인자가 예상하는 데이터 설명\n상세 설명(Details): 알고 있어야 되는 중요한 구체적인 설명\n값(Value): 함수가 반환하는 데이터\n함께 보기(See Also): 유용할 수 있는 연관된 함수.\n예제(Examples): 함수 사용법에 대한 예제들.\n\n함수마다 상이한 절을 갖추고 있다. 하지만, 상기 항목이 알고 있어야 하는 핵심 내용이다.\n\n\n\n\n\n\n힌트도움말 파일 불러 읽어오기\n\n\n\nR에 대해 가장 기죽게 되는 한 측면이 엄청난 함수 갯수다. 모든 함수에 대한 올바른 사용법을 기억하지 못하면, 엄두가 나지 않을 것이다. 운 좋게도, 도움말 파일로 인해 기억할 필요가 없다!\n\n\n\n17.7.2 특수 연산자\n특수 연산자에 대한 도움말을 찾으려면, 인용부호를 사용한다:\n\n?\"&lt;-\"\n\n\n17.7.3 팩키지 도움말 얻기\n많은 팩키지에 “소품문(vignettes)”이 따라온다. 활용법과 풍부한 예제를 담은 문서. 어떤 인자도 없이, vignette() 명령어를 입력하면 설치된 모든 팩키지에 대한 모든 소품문 목록이 출력된다, vignette(package=\"package-name\") 명령어는 package-name 팩키지명에 대한 이용 가능한 모든 소품문 목록을 출력하고, vignette(\"vignette-name\") 명령어는 특정된 소품문을 연다.\n팩키지에 어떤 소품문도 포함되지 않는다면, 일반적으로 help(\"package-name\") 명령어를 타이핑해서 도움말을 얻는다.\n\n17.7.4 함수가 기억나지 않을 때\n함수가 어느 팩키지에 있는지 확신을 못하거나, 구체적인 철자법을 모르는 경우, 퍼지 검색(fuzzy search)을 실행한다.\n\n??function_name\n\n\n17.7.5 시작조차 난감할 때\n어떤 함수 혹은 팩키지가 필요한지 모르는 경우, CRAN Task Views 사이트가 좋은 시작점이 된다. 유지 관리되는 팩키지 목록이 필드로 묶여 잘 정리되어 있다.\n\n17.7.6 코드가 동작하지 않을 때\n동료로부터 도움을 구해 코드가 동작하지 않는 이슈를 해결한다. 함수 사용에 어려움이 있는 경우, 10 에 9 경우에 찾는 정답이 이미 Stack Overflow에 답글이 달려 있다. 검색할 때 [r] 태그를 사용한다.\n원하는 답을 찾지 못한 경우, 동료에게 질문을 만드는 데 몇 가지 유용한 함수가 있다.\n\n?dput\n\ndput() 함수는 작업하고 있는 데이터를 텍스트 파일 형식으로 덤프해서 저장한다. 그래서 다른 사람 R 세션으로 복사해서 붙여넣기 좋게 돕는다.\n\nsessionInfo()\n#&gt; R version 4.5.0 (2025-04-11)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sonoma 14.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] ko_KR.UTF-8/ko_KR.UTF-8/ko_KR.UTF-8/C/ko_KR.UTF-8/ko_KR.UTF-8\n#&gt; \n#&gt; time zone: Asia/Seoul\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] showtext_0.9-7   showtextdb_3.0   sysfonts_0.8.9   JuliaCall_0.17.6\n#&gt;  [5] rvest_1.0.4      gtExtras_0.6.0   gt_1.1.0         lubridate_1.9.4 \n#&gt;  [9] forcats_1.0.0    stringr_1.5.2    dplyr_1.1.4      purrr_1.1.0     \n#&gt; [13] readr_2.1.5      tidyr_1.3.1      tibble_3.3.0     ggplot2_4.0.0   \n#&gt; [17] tidyverse_2.0.0 \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] generics_0.1.4     xml2_1.4.0         stringi_1.8.7     \n#&gt;  [4] extrafontdb_1.1    hms_1.1.3          digest_0.6.37     \n#&gt;  [7] magrittr_2.0.4     evaluate_1.0.5     grid_4.5.0        \n#&gt; [10] timechange_0.3.0   RColorBrewer_1.1-3 fastmap_1.2.0     \n#&gt; [13] jsonlite_2.0.0     processx_3.8.6     chromote_0.5.1    \n#&gt; [16] rematch2_2.1.2     ps_1.9.1           promises_1.3.3    \n#&gt; [19] httr_1.4.7         scales_1.4.0       cli_3.6.5         \n#&gt; [22] rlang_1.1.6        withr_3.0.2        yaml_2.3.10       \n#&gt; [25] tools_4.5.0        tzdb_0.5.0         paletteer_1.6.0   \n#&gt; [28] vctrs_0.6.5        R6_2.6.1           lifecycle_1.0.4   \n#&gt; [31] fs_1.6.6           htmlwidgets_1.6.4  fontawesome_0.5.3 \n#&gt; [34] pkgconfig_2.0.3    pillar_1.11.1      later_1.4.4       \n#&gt; [37] gtable_0.3.6       glue_1.8.0         Rcpp_1.1.0        \n#&gt; [40] xfun_0.53          tidyselect_1.2.1   knitr_1.50        \n#&gt; [43] extrafont_0.20     farver_2.1.2       websocket_1.4.4   \n#&gt; [46] htmltools_0.5.8.1  rmarkdown_2.30     Rttf2pt1_1.3.14   \n#&gt; [49] compiler_4.5.0     S7_0.2.0\n\nsessionInfo()는 R 현재 버전 정보와 함께 적재된 팩키지 정보를 출력한다. 이 정보가 다른 사람이 여러분 문제를 재현하고 디버그하는 데 유용할 수 있다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>정규 표현식</span>"
    ]
  },
  {
    "objectID": "tool-regex.html#r-regex-terminology",
    "href": "tool-regex.html#r-regex-terminology",
    "title": "17  정규 표현식",
    "section": "\n17.8 용어 정의",
    "text": "17.8 용어 정의\n\n\n부서지기 쉬운 코드(brittle code):입력 데이터가 특정한 형식일 경우에만 작동하는 코드. 하지만 올바른 형식에서 약간이도 벗어나게 되면 깨지기 쉽다. 쉽게 부서지기 때문에 “부서지기 쉬운 코드(brittle code)”라고 부른다.\n\n욕심쟁이 매칭(greedy matching):정규 표현식의 “+”, “*” 문자는 가능한 큰 문자열을 매칭하기 위해서 밖으로 확장하는 개념. \n\ngrep: 정규 표현식에 매칭되는 파일을 탐색하여 라인을 찾는데 대부분의 유닉스 시스템에서 사용 가능한 명령어. “Generalized Regular Expression Parser”의 약자. \n\n정규 표현식(regular expression): 좀 더 복잡한 검색 문자열을 표현하는 언어. 정규 표현식은 특수 문자를 포함해서 검색 라인의 처음 혹은 끝만 매칭하거나 많은 비슷한 것을 매칭한다. \n\n와일드 카드(wild card): 임의 문자를 매칭하는 특수 문자. 정규 표현식에서 와일드 카드 문자는 마침표 문자다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>정규 표현식</span>"
    ]
  },
  {
    "objectID": "tool-regex.html#r-regex-ex",
    "href": "tool-regex.html#r-regex-ex",
    "title": "17  정규 표현식",
    "section": "연습문제",
    "text": "연습문제\n\n유닉스의 grep 명령어를 모사하는 간단한 프로그램을 작성한다. 사용자가 정규 표현식을 입력하고 정규 표현식에 매칭되는 라인 수를 세는 프로그램이다.\n\n$ Rscript grep.R\nEnter a regular expression: ^Author\nmbox.txt had 1798 lines that matched ^Author\n\n$ Rscript grep.R\nEnter a regular expression: ^X-\nmbox.txt had 14368 lines that matched ^X-\n\n$ Rscript grep.R\nEnter a regular expression: java$\nmbox.txt had 4218 lines that matched java$\n\n다음 형식의 라인만을 찾는 프로그램을 작성하세요.\n\nNew Revision: 39772\n그리고, 정규 표현식과 str_extract() 함수를 사용하여 각 라인으로부터 숫자를 추출한다. 숫자들의 평균을 구하고 출력한다.\nEnter file:mbox.txt \n38549.7949721\n\nEnter file:mbox-short.txt\n39756.9259259",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>정규 표현식</span>"
    ]
  },
  {
    "objectID": "tool-network.html",
    "href": "tool-network.html",
    "title": "18  네트워크 프로그램",
    "section": "",
    "text": "18.1 하이퍼텍스트 전송 프로토콜\n지금까지 책의 많은 예제는 파일을 읽고 파일 속 정보를 찾는 데 집중했지만, 다양한 많은 정보의 원천이 인터넷에 있다. 이번 장에서는 웹 브라우저로 가장하고 HTTP 프로토콜(HyperText Transport Protocol, HTTP)을 사용하여 웹페이지를 검색할 것이다. 웹페이지 데이터를 읽고 파싱할 것이다.\n웹에 동력을 공급하는 네트워크 프로토콜(HyperText Transport Protocol - HTTP)은 실제로 매우 단순하다. 파이썬에는 소켓(sockets)이라고 불리는 내장 지원 모듈이 있다. 파이썬 프로그램에서 소켓 모듈을 통해서 네트워크 연결을 하고, 데이터 검색을 매우 용이하게 한다.\n소켓(socket)은 단일 소켓으로 두 프로그램 사이에 양방향 연결을 제공한다는 점을 제외하고 파일과 매우 유사하다. 동일한 소켓에 읽거나 쓸 수 있다. 소켓에 무언가를 쓰게 되면, 소켓의 다른 끝에 있는 응용프로그램에 전송된다. 소켓으로부터 읽게 되면, 다른 응용 프로그램이 전송한 데이터를 받게 된다.\n하지만, 소켓의 다른쪽 끝에 프로그램이 어떠한 데이터도 전송하지 않았는데 소켓을 읽으려고 하면, 단지 앉아서 기다리기만 한다. 만약 어떠한 것도 보내지 않고 양쪽 소켓 끝의 프로그램 모두 기다리기만 한다면, 모두 매우 오랜 시간 동안 기다리게 될 것이다.\n인터넷으로 통신하는 프로그램의 중요한 부분은 특정 종류의 프로토콜을 공유하는 것이다. 프로토콜(protocol)은 정교한 규칙의 집합으로 누가 메시지를 먼저 보내고, 메시지로 무엇을 하며, 메시지에 대한 응답은 무엇이고, 다음에 누가 메시지를 보내는지 등을 포함한다. 이런 관점에서 소켓 끝의 두 응용프로그램이 함께 춤을 추고 있으니, 다른 사람 발을 밟지 않도록 확인해야 한다.\n네트워크 프로토콜을 기술하는 문서가 많이 있다. 하이퍼텍스트 전송 프로토콜(HyperText Transport Protocol)은 다음 문서에 기술되어 있다.\nhttp://www.w3.org/Protocols/rfc2616/rfc2616.txt\n매우 상세한 176페이지나 되는 장문의 복잡한 문서다. 흥미롭다면 시간을 가지고 읽어보기 바란다. RFC2616에 36페이지를 읽어보면, GET 요청(request)에 대한 구문을 발견하게 된다. 꼼꼼히 읽게 되면, 웹서버에 문서를 요청하기 위해서, 80 포트로 www.py4inf.com 서버에 연결을 하고 나서 다음 양식 한 라인을 전송한다.\nGET http://www.py4inf.com/code/romeo.txt HTTP/1.0\n두 번째 매개변수는 요청하는 웹페이지가 된다. 그리고 또한 빈 라인도 전송한다. 웹서버는 문서에 대한 헤더 정보와 빈 라인 그리고 문서 본문으로 응답한다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>네트워크 프로그램</span>"
    ]
  },
  {
    "objectID": "tool-network.html#network-http",
    "href": "tool-network.html#network-http",
    "title": "18  네트워크 프로그램",
    "section": "",
    "text": "경고웹브라우저 실행금지 사유\n\n\n\nwebr pyodide는 웹 브라우저 샌드박스 환경에서 R, 파이썬을 실행하며, 브라우저 보안 제한으로 인해 일반 R, 파이썬 환경에서와 같은 네트워크 소켓에 대한 직접적인 접근이 불가능하다. 접근을 허용하는 것이 중대한 보안 위험을 초래할 수 있기 때문에 웹 브라우저에서 실행이 되지 않게 되어 있어 오류가 나는 것이 정상이다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>네트워크 프로그램</span>"
    ]
  },
  {
    "objectID": "tool-network.html#network-web-browser",
    "href": "tool-network.html#network-web-browser",
    "title": "18  네트워크 프로그램",
    "section": "\n18.2 가장 간단한 웹 브라우저",
    "text": "18.2 가장 간단한 웹 브라우저\n아마도 HTTP 프로토콜이 어떻게 작동하는지 알아보는 가장 간단한 방법은 매우 간단한 파이썬 프로그램을 작성하는 것이다. 웹서버에 접속하고 HTTP 프로토콜 규칙에 따라 문서를 요청하고 서버가 다시 보내주는 결과를 보여주는 것이다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n처음에 프로그램은 www.py4e.com 서버에 80 포트로 연결한다. “웹 브라우저” 역할로 작성된 프로그램이 하기 때문에 HTTP 프로토콜은 GET 명령어를 공백 라인과 함께 보낸다.\n\n\n\n\n\n그림 18.1: 소켓 개념도\n\n\n공백 라인을 보내자마자, 512 문자 덩어리의 데이터를 소켓에서 받아 더 이상 읽을 데이터가 없을 때까지(즉, recv()이 빈 문자열을 반환한다.) 데이터를 출력하는 루프를 작성한다.\n프로그램 실행 결과 다음을 얻을 수 있다.\nHTTP/1.1 200 OK\nDate: Wed, 11 Apr 2018 18:52:55 GMT\nServer: Apache/2.4.7 (Ubuntu)\nLast-Modified: Sat, 13 May 2017 11:22:22 GMT\nETag: \"a7-54f6609245537\"\nAccept-Ranges: bytes\nContent-Length: 167\nCache-Control: max-age=0, no-cache, no-store, must-revalidate\nPragma: no-cache\nExpires: Wed, 11 Jan 1984 05:00:00 GMT\nConnection: close\nContent-Type: text/plain\n\nBut soft what light through yonder window breaks\nIt is the east and Juliet is the sun\nArise fair sun and kill the envious moon\nWho is already sick and pale with grief\n출력결과는 웹서버가 문서를 기술하기 위해서 보내는 헤더(header)로 시작한다. 예를 들어, Content-Type 헤더는 문서가 일반 텍스트 문서(text/plain)임을 표기한다.\n서버가 헤더를 보낸 후에, 빈 라인을 추가해서 헤더 끝임을 표기하고 나서 실제 파일romeo.txt을 보낸다.\n이 예제를 통해서 소켓을 통해서 저수준(low-level) 네트워크 연결을 어떻게 하는지 확인할 수 있다. 소켓을 사용해서 웹서버, 메일 서버 혹은 다른 종류의 서버와 통신할 수 있다. 필요한 것은 프로토콜을 기술하는 문서를 찾고 프로토콜에 따라 데이터를 주고받는 코드를 작성하는 것이다.\n하지만, 가장 흔히 사용하는 프로토콜은 HTTP (즉, 웹) 프로토콜이기 때문에, 파이썬에는 HTTP 프로토콜을 지원하기 위해 특별히 설계된 라이브러리가 있다. 이것을 통해서 웹상에서 데이터나 문서 검색을 쉽게 할 수 있다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>네트워크 프로그램</span>"
    ]
  },
  {
    "objectID": "tool-network.html#socket-images",
    "href": "tool-network.html#socket-images",
    "title": "18  네트워크 프로그램",
    "section": "\n18.3 HTTP 경유 이미지 가져오기",
    "text": "18.3 HTTP 경유 이미지 가져오기\n \n상기 예제에서는 파일에 줄바꿈(newline)이 있는 일반 텍스트 파일을 가져왔다. 그리고 나서, 프로그램을 실행해서 데이터를 단순히 화면에 복사했다. HTTP를 사용하여 이미지를 가져오도록 비슷하게 프로그램을 작성할 수 있다. 프로그램 실행 시에 화면에 데이터를 복사하는 대신에, 데이터를 문자열로 누적하고, 다음과 같이 헤더를 잘라내고 나서 파일에 이미지 데이터를 저장한다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n프로그램을 실행하면, 다음과 같은 출력을 생성한다.\n$ python urljpeg.py\n5120 5120\n5120 10240\n4240 14480\n5120 19600\n...\n5120 214000\n3200 217200\n5120 222320\n5120 227440\n3167 230607\nHeader length 393\nHTTP/1.1 200 OK\nDate: Wed, 11 Apr 2018 18:54:09 GMT\nServer: Apache/2.4.7 (Ubuntu)\nLast-Modified: Mon, 15 May 2017 12:27:40 GMT\nETag: \"38342-54f8f2e5b6277\"\nAccept-Ranges: bytes\nContent-Length: 230210\nVary: Accept-Encoding\nCache-Control: max-age=0, no-cache, no-store, must-revalidate\nPragma: no-cache\nExpires: Wed, 11 Jan 1984 05:00:00 GMT\nConnection: close\nContent-Type: image/jpeg\n상기 url에 대해서, Content-Type 헤더가 문서 본문이 이미지(image/jpeg)를 나타내는 것을 볼 수 있다. 프로그램이 완료되면, 이미지 뷰어로 stuff.jpg 파일을 열어서 이미지 데이터를 볼 수 있다.\n프로그램을 실행하면, recv() 메서드를 호출할 때마다 5120 문자는 전달받지 못하는 것을 볼 수 있다. recv() 호출하는 순간마다 웹서버에서 네트워크로 전송되는 가능한 많은 문자를 받을 뿐이다. 매번 5120 문자까지 요청하지만, 1460 혹은 2920 문자만 전송받는다.\n결과값은 네트워크 속도에 따라 달라질 수 있다. recv()메서드 마지막 호출에는 스트림 마지막인 1681 바이트만 받았고,recv()다음 호출에는 0 길이 문자열을 전송받아서, 서버가 소켓 마지막에close()` 메서드를 호출하고 더 이상의 데이터가 없다는 신호를 준다. \n주석 처리한 time.sleep()을 풀어줌으로써 recv() 연속 호출을 늦출 수 있다. 이런 방식으로 매번 호출 후에 0.25초 기다리게 한다. 그래서, 사용자가 recv() 메서드를 호출하기 전에 서버가 먼저 도착할 수 있어서 더 많은 데이터를 보낼 수가 있다. 정지 시간을 넣어서 프로그램을 다시 실행하면 다음과 같다.\n$ python urljpeg.py\n5120 5120\n5120 10240\n5120 15360\n...\n5120 225280\n5120 230400\n207 230607\nHeader length 393\nHTTP/1.1 200 OK\nDate: Wed, 11 Apr 2018 21:42:08 GMT\nServer: Apache/2.4.7 (Ubuntu)\nLast-Modified: Mon, 15 May 2017 12:27:40 GMT\nETag: \"38342-54f8f2e5b6277\"\nAccept-Ranges: bytes\nContent-Length: 230210\nVary: Accept-Encoding\nCache-Control: max-age=0, no-cache, no-store, must-revalidate\nPragma: no-cache\nExpires: Wed, 11 Jan 1984 05:00:00 GMT\nConnection: close\nContent-Type: image/jpeg\nrecv() 메서드 호출의 처음과 마지막을 제외하고, 매번 새로운 데이터를 요청할 때마다 이제 5120 문자가 전송된다.\n서버 send() 요청과 응용프로그램 recv() 요청 사이에 버퍼가 있다. 프로그램에 지연을 넣어 실행하게 될 때, 어느 지점엔가 서버가 소켓 버퍼를 채우고 응용프로그램이 버퍼를 비울 때까지 잠시 멈춰야 된다. 송신하는 응용프로그램 혹은 수신하는 응용프로그램을 멈추게 하는 행위를 “흐름 제어(flow control)”라고 한다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>네트워크 프로그램</span>"
    ]
  },
  {
    "objectID": "tool-network.html#httr",
    "href": "tool-network.html#httr",
    "title": "18  네트워크 프로그램",
    "section": "\n18.4 httr 웹페이지 가져오기",
    "text": "18.4 httr 웹페이지 가져오기\n수작업으로 소켓 라이브러리를 사용하여 HTTP로 데이터를 주고받을 수 있지만, httr 패키지를 사용하여 R에서 동일한 작업을 수행하는 좀 더 간편한 방식이 있다. httr을 사용하여 파일처럼 웹페이지를 다룰 수가 있다. 단순하게 어느 웹페이지를 가져올 것인지만 지정하면 httr 라이브러리가 모든 HTTP 프로토콜과 헤더 관련 사항을 처리해 준다.\nGET() 함수를 사용하여 웹페이지를 열게 되면, 파일처럼 다룰 수 있고 content() 함수를 사용해서 데이터를 읽을 수 있다. 프로그램을 실행하면, 파일 내용 출력결과만을 볼 수 있다. 헤더 정보는 여전히 전송되었지만, GET() 함수가 헤더를 받아 내부적으로 처리하고, 사용자에게는 단지 데이터만 반환한다. 파이썬으로 웹에서 romeo.txt 파일을 읽도록 urllib를 사용하여 동일한 기능을 구현할 수 있다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n예제로, romeo.txt 데이터를 가져와서 파일의 각 단어 빈도를 계산하는 프로그램을 다음과 같이 작성할 수 있다. 웹페이지를 로컬 텍스트로 변환시킨 후에 공백과 \\n으로 텍스트를 잘게 쪼개고, 단어 빈도수를 table()로 계산한 후 단어 빈도수를 확인한다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n다시 한번, 웹페이지를 열게 되면, 로컬 파일처럼 웹페이지를 읽을 수 있다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>네트워크 프로그램</span>"
    ]
  },
  {
    "objectID": "tool-network.html#html-webscraping",
    "href": "tool-network.html#html-webscraping",
    "title": "18  네트워크 프로그램",
    "section": "\n18.5 HTML 파싱과 웹 스크래핑",
    "text": "18.5 HTML 파싱과 웹 스크래핑\n \nR httr 패키지를 활용하는 일반적인 사례는 웹 스크래핑(scraping)이다. 웹 스크래핑은 웹 브라우저를 가장한 프로그램을 작성하는 것이다. 웹페이지를 가져와서, 패턴을 찾아 페이지 내부의 데이터를 꼼꼼히 조사한다. 예로, 구글 같은 검색엔진은 웹 페이지의 소스를 조사해서 다른 페이지로 가는 링크를 추출하고, 그 해당 페이지를 가져와서 링크 추출하는 작업을 반복한다. 이러한 기법으로 구글은 웹상의 거의 모든 페이지를 거미(spiders)줄처럼 연결한다.\n구글은 또한 발견한 웹페이지에서 특정 페이지로 연결되는 링크 빈도를 사용하여 얼마나 중요한 페이지인지를 측정하고 검색결과에 페이지가 얼마나 높은 순위로 노출되어야 하는지 평가한다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>네트워크 프로그램</span>"
    ]
  },
  {
    "objectID": "tool-network.html#webscarping-regex",
    "href": "tool-network.html#webscarping-regex",
    "title": "18  네트워크 프로그램",
    "section": "\n18.6 정규 표현식 HTML 파싱",
    "text": "18.6 정규 표현식 HTML 파싱\nHTML을 파싱하는 간단한 방식은 정규 표현식을 사용하여 특정한 패턴과 매칭되는 부속 문자열을 반복적으로 찾아 추출하는 것이다.\n여기 간단한 웹페이지가 있다.\n&lt;h1&gt;The First Page&lt;/h1&gt;\n&lt;p&gt;\nIf you like, you can switch to the\n&lt;a href=\"http://www.dr-chuck.com/page2.htm\"&gt;\nSecond Page&lt;/a&gt;.\n&lt;/p&gt;\n모양 좋은 정규표현식을 구성해서 다음과 같이 상기 웹페이지에서 링크를 매칭하고 추출할 수 있다.\nhref=\"http://.+?\"\n작성된 정규 표현식은 “href=http://”로 시작하고, 하나 이상의 문자를 “.+?” 가지고 큰 따옴표를 가진 문자열을 찾는다. “.+?”에 물음표가 갖는 의미는 매칭이 “욕심쟁이(greedy)” 방식보다 “비욕심쟁이(non-greedy)” 방식으로 수행됨을 나타낸다. 비욕심쟁이(non-greedy) 매칭 방식은 가능한 가장 적게 매칭되는 문자열을 찾는 방식이고, 욕심 방식은 가능한 가장 크게 매칭되는 문자열을 찾는 방식이다. \n추출하고자 하는 문자열이 매칭된 문자열의 어느 부분인지를 표기하기 위해서 정규 표현식에 괄호를 추가하여 다음과 같이 프로그램을 작성한다. \n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nstr_extract_all 정규 표현식 함수는 정규 표현식과 매칭되는 모든 문자열 리스트를 추출하여 큰 따옴표 사이에 링크 텍스트만을 반환한다. 프로그램을 실행하면, 다음 출력을 얻게 된다.\n$ Rscript.exe ./code/extract_link.R\n웹사이트 입력 - http://www.dr-chuck.com/page1.htm\n[1] \"href=\\\"http://www.dr-chuck.com/page2.htm\\\"\"\n\n$ Rscript.exe ./code/extract_link.R\n웹사이트 입력 - http://www.py4inf.com/book.htm\n [1] \"href=\\\"http://amzn.to/1KkULF3\\\"\"\n [2] \"href=\\\"http://www.py4e.com/book\\\"\"\n [3] \"href=\\\"http://amzn.to/1KkULF3\\\"\"\n [4] \"href=\\\"http://amzn.to/1hLcoBy\\\"\"\n [5] \"href=\\\"http://amzn.to/1KkV42z\\\"\"\n [6] \"href=\\\"http://amzn.to/1fNOnbd\\\"\"\n [7] \"href=\\\"http://amzn.to/1N74xLt\\\"\"\n [8] \"href=\\\"http://do1.dr-chuck.net/py4inf/EN-us/book.pdf\\\"\"\n [9] \"href=\\\"http://do1.dr-chuck.net/py4inf/ES-es/book.pdf\\\"\"\n[10] \"href=\\\"http://do1.dr-chuck.net/py4inf/PT-br/book.pdf\\\"\"\n[11] \"href=\\\"http://www.xwmooc.net/python/\\\"\"\n[12] \"href=\\\"http://fanwscu.gitbooks.io/py4inf-zh-cn/\\\"\"\n[13] \"href=\\\"http://itunes.apple.com/us/book/python-for-informatics/id554638579?mt=13\\\"\"\n[14] \"href=\\\"http://www-personal.umich.edu/~csev/books/py4inf/ibooks//python_for_informatics.ibooks\\\"\"\n[15] \"href=\\\"http://www.py4inf.com/code\\\"\"\n[16] \"href=\\\"http://www.greenteapress.com/thinkpython/thinkCSpy/\\\"\"\n[17] \"href=\\\"http://allendowney.com/\\\"\"\n정규 표현식은 HTML이 예측 가능하고 잘 구성된 경우에 멋지게 작동한다. 하지만, “망가진” HTML 페이지가 많아서, 정규 표현식만을 사용하는 솔루션은 유효한 링크를 놓치거나 잘못된 데이터만 찾고 끝날 수 있다.\n이 문제는 강건한 HTML 파싱 라이브러리를 사용해서 해결될 수 있다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>네트워크 프로그램</span>"
    ]
  },
  {
    "objectID": "tool-network.html#rvest-scraping",
    "href": "tool-network.html#rvest-scraping",
    "title": "18  네트워크 프로그램",
    "section": "\n18.7 rvest HTML 파싱",
    "text": "18.7 rvest HTML 파싱\n\nHTML을 파싱하여 페이지에서 데이터를 추출할 수 있는 R 패키지는 많이 있다. 패키지 각각은 강점과 약점이 있어서 사용자 필요에 따라 취사선택한다.\n예로, 간단하게 HTML 입력을 파싱하여 rvest 라이브러리를 사용하여 링크를 추출할 것이다.\nHTML이 XML처럼 보이고 몇몇 페이지는 XML로 되도록 꼼꼼하게 구축되었지만, 일반적으로 대부분의 HTML이 깨져서 XML 파서가 HTML 전체 페이지를 잘못 구성된 것으로 간주하고 받아들이지 않는다. rvest 패키지는 결점 많은 HTML 페이지에 내성이 있어서 사용자가 필요로 하는 데이터를 쉽게 추출할 수 있게 한다.\nhttr 패키지를 사용하여 페이지를 읽어들이고, rvest를 사용해서 앵커 태그(a)로부터 href 속성을 추출한다. \n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n프로그램이 웹 주소를 입력받고, 웹페이지를 열고, 데이터를 읽어서 BeautifulSoup 파서에 전달하고, 그리고 나서 모든 앵커 태그를 불러와서 각 태그별로 href 속성을 출력한다. 프로그램을 실행하면, 아래와 같다.\n$ Rscript.exe ./code/extract_link_rvest.R\n웹사이트 입력 - http://www.dr-chuck.com/page1.htm\n[1] \"http://www.dr-chuck.com/page2.htm\"\n\n$ Rscript.exe ./code/extract_link_rvest.R\n웹사이트 입력 - http://www.py4inf.com/book.htm\n [1] \"http://amzn.to/1KkULF3\"\n [2] \"http://www.py4e.com/book\"\n [3] \"http://amzn.to/1KkULF3\"\n [4] \"http://amzn.to/1hLcoBy\"\n [5] \"http://amzn.to/1KkV42z\"\n [6] \"http://amzn.to/1fNOnbd\"\n [7] \"http://amzn.to/1N74xLt\"\n [8] \"http://do1.dr-chuck.net/py4inf/EN-us/book.pdf\"\n [9] \"http://do1.dr-chuck.net/py4inf/ES-es/book.pdf\"\n[10] \"https://twitter.com/fertardio\"\n[11] \"http://do1.dr-chuck.net/py4inf/PT-br/book.pdf\"\n[12] \"https://twitter.com/victorjabur\"\n[13] \"translations/KO/book_009_ko.pdf\"\n[14] \"http://www.xwmooc.net/python/\"\n[15] \"http://fanwscu.gitbooks.io/py4inf-zh-cn/\"\n[16] \"book_270.epub\"\n[17] \"translations/ES/book_272_es4.epub\"\n[18] \"https://www.gitbook.com/download/epub/book/fanwscu/py4inf-zh-cn\"\n[19] \"html-270/\"\n[20] \"html_270.zip\"\n[21] \"http://itunes.apple.com/us/book/python-for-informatics/id554638579?mt=13\"\n[22] \"http://www-personal.umich.edu/~csev/books/py4inf/ibooks//python_for_informatics.ibooks\"\n[23] \"http://www.py4inf.com/code\"\n[24] \"http://www.greenteapress.com/thinkpython/thinkCSpy/\"\n[25] \"http://allendowney.com/\"\nrvest을 사용하여 다음과 같이 각 태그별로 다양한 부분을 뽑아낼 수 있다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n상기 프로그램은 다음을 출력한다.\n$ Rscript.exe ./code/extract_link_tag.R\n웹사이트 입력 - http://www.dr-chuck.com/page1.htm\n경고메시지(들):\nfor (name in names(public_methods)) lockBinding(name, public_bind_env)에서:\n  사용되지 않는 커넥션 3 (stdin)를 닫습니다\nTAG: &lt;a href=\"http://www.dr-chuck.com/page2.htm\"&gt;\nSecond Page&lt;/a&gt;\nURL: http://www.dr-chuck.com/page2.htm\nContent:\nSecond Page\nAttrs: href: http://www.dr-chuck.com/page2.htm\nHTML을 파싱하는 데 rvest가 가진 강력한 기능을 예제로 보여줬다. 좀 더 자세한 사항은 https://rvest.tidyverse.org/에서 문서와 예제를 살펴보기 바란다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>네트워크 프로그램</span>"
    ]
  },
  {
    "objectID": "tool-network.html#GET-binary",
    "href": "tool-network.html#GET-binary",
    "title": "18  네트워크 프로그램",
    "section": "\n18.8 바이너리 파일 읽기",
    "text": "18.8 바이너리 파일 읽기\n이미지나 비디오 같은 텍스트가 아닌 (혹은 바이너리) 파일을 가져올 때가 종종 있다. 일반적으로 이런 파일 데이터를 출력하는 것은 유용하지 않다. 하지만, download.file() 함수를 사용하여, 하드 디스크 로컬 파일에 URL 사본을 쉽게 만들 수 있다.\n\ndownload.file() 함수 내부에 인터넷에 공개된 이미지 url을 적고, destfile에는 로컬 파일에 저장할 파일명을 적어준다. 중요한 것은 텍스트가 아니라 바이너리 이미지라 mode = \"wb\"를 지정한다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n작성된 프로그램은 네트워크로 모든 데이터를 한번에 읽어서 컴퓨터 cover.jpg 파일을 다운로드받아 로컬 디스크에 지정한 디렉터리 파일명으로 데이터를 쓴다. 이 방식은 파일 크기가 사용자 컴퓨터의 메모리 크기보다 작다면 정상적으로 작동한다.\n\n\n\n\n\n그림 18.2: 바이너리 파일 다운로드\n\n\n하지만, 오디오 혹은 비디오 파일 대용량이면, 상기 프로그램은 멈추거나 사용자 컴퓨터 메모리가 부족할 때 극단적으로 느려질 수 있다. 메모리 부족을 회피하기 위해서, 데이터를 블록 혹은 버퍼로 가져와서, 다음 블록을 가져오기 전에 디스크에 각각의 블록을 쓴다. 이런 방식으로 사용자가 가진 모든 메모리를 사용하지 않고 어떠한 크기 파일도 읽어올 수 있다.\nFannie Mae and Freddie Mac은 주택금융공사에 상응하는 기관으로 주택 구매자에게 저리의 장기 고정금리 주택담보대출을 제공하고 주택 모기지를 재융자하는 역할을 통해 주택 금융 시장의 안정을 도모하고 국민의 주거 안정에 기여한다. 웹사이트에 공공데이터로 인터넷에 공개된 주택담보대출 나름 크기가 큰 데이터를 가져온다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nUNIX 혹은 매킨토시 컴퓨터를 가지고 있다면, 다음과 같이 상기 동작을 수행하는 명령어가 운영체제 자체에 내장되어 있다. \ncurl -O https://www.dr-chuck.com/py4inf/cover.jpg",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>네트워크 프로그램</span>"
    ]
  },
  {
    "objectID": "tool-network.html#network-terminology",
    "href": "tool-network.html#network-terminology",
    "title": "18  네트워크 프로그램",
    "section": "\n18.9 용어 정의",
    "text": "18.9 용어 정의\n\n\nBeautifulSoup: 파이썬 라이브러리로 HTML 문서를 파싱하고 브라우저가 일반적으로 생략하는 HTML의 불완전한 부분을 보정하여 HTML 문서에서 데이터를 추출한다. www.crummy.com 사이트에서 BeautifulSoup 코드를 다운로드 받을 수 있다. \n\nrvest: 파이썬 BeautifulSoup에 대응되는 R 크롤링 패키징 \n\n포트(port): 서버에 소켓 연결을 만들 때, 사용자가 무슨 응용프로그램을 연결하는지 나타내는숫자. 예로, 웹 트래픽은 통상 80 포트, 전자우편은 25 포트를 사용한다. \n\n스크래핑(scraping): 프로그램이 웹 브라우저를 가장하여 웹페이지를 가져와서 웹 페이지의 내용을 검색한다. 종종 프로그램이 한 페이지의 링크를 따라 다른 페이지를 찾게 된다. 그래서, 웹페이지 네트워크 혹은 소셜 네트워크 전체를 훑을 수 있다. \n\n소켓(socket): 두 응용프로그램 사이 네트워크 연결. 두 응용프로그램은 양방향으로 데이터를 주고받는다. \n\n스파이더(spider):검색 색인을 구축하기 위해서 한 웹페이지를 검색하고, 그 웹페이지에 링크된 모든 페이지 검색을 반복하여 인터넷에 있는 거의 모든 웹페이지를 가져오기 위해서 사용되는 검색엔진 행동.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>네트워크 프로그램</span>"
    ]
  },
  {
    "objectID": "tool-network.html#network-ex",
    "href": "tool-network.html#network-ex",
    "title": "18  네트워크 프로그램",
    "section": "연습문제",
    "text": "연습문제\n\n소켓 프로그램 socket1.py을 변경하여 임의 웹페이지를 읽을 수 있도록 URL을 사용자가 입력하도록 수정한다. split('/')을 사용하여 URL을 컴포넌트로 쪼개서 소켓 connect 호출에 대해 호스트 명을 추출할 수 있다. 사용자가 적절하지 못한 형식 혹은 존재하지 않는 URL을 입력하는 경우를 처리할 수 있도록 try, except를 사용하여 오류 검사기능을 추가한다.\n소켓 프로그램을 변경하여 전송받은 문자를 계수(count)하고 3000 문자를 출력한 후에 그 이상 텍스트 출력을 멈추게 한다. 프로그램은 전체 문서를 가져와야 하고, 전체 문자를 계수(count)하고, 문서 마지막에 문자 계수(count)결과를 출력해야 한다.\nhttr 패키지를 사용하여 이전 예제를 반복한다. (1) 사용자가 입력한 URL에서 문서 가져오기 (2) 3000 문자까지 화면에 보여주기 (3) 문서의 전체 문자 계수(count)하기. 이 연습문제에서 헤더에 대해서는 걱정하지 말고, 단지 문서 본문에서 첫 3000 문자만 화면에 출력한다.\nurllinks.R 프로그램을 변경하여 가져온 HTML 문서에서 문단(p) 태그를 추출하고 프로그램의 출력물로 문단을 계수(count)하고 화면에 출력한다. 문단 텍스트를 화면에 출력하지 말고 단지 숫자만 센다. 작성한 프로그램을 작은 웹페이지뿐만 아니라 조금 큰 웹 페이지에도 테스트한다.\n(고급) 소켓 프로그램을 변경하여 헤더와 빈 라인 다음에 데이터만 보이도록 개발한다. recv는 라인이 아니라 문자(새줄(newline)과 모든 문자)를 전송받는다는 것을 기억한다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>네트워크 프로그램</span>"
    ]
  },
  {
    "objectID": "tool-web.html",
    "href": "tool-web.html",
    "title": "19  웹서비스 사용하기",
    "section": "",
    "text": "19.1 XML\n프로그램을 사용하여 HTTP상에서 문서를 가져와서 파싱하는 것이 익숙해지면, 다른 프로그램(즉, 브라우저에서 HTML로 보여지지 않는 것)에서 활용되도록 특별히 설계된 문서를 생성하는 것은 그다지 오래 걸리지 않는다.\n웹상에서 데이터를 교환할 때 두 가지 형식이 많이 사용된다. XML(“eXtensible Markup Language”)은 오랜 기간 사용되어 왔고 문서-형식(document-style) 데이터를 교환하는 데 가장 적합하다. 딕셔너리, 리스트 혹은 다른 내부 정보를 프로그램으로 서로 교환할 때, JSON(JavaScript Object Notation, www.json.org)을 사용한다. 두 가지 형식에 대해 모두 살펴볼 것이다.\nXML(eXtensible Markup Language)은 HTML과 매우 유사하지만, XML이 HTML보다 좀 더 구조화되어 있다. 여기 XML 문서 샘플이 있다.\n&lt;person&gt;\n  &lt;name&gt;Chuck&lt;/name&gt;\n  &lt;phone type=\"intl\"&gt;\n     +1 734 303 4456\n   &lt;/phone&gt;\n   &lt;email hide=\"yes\"/&gt;\n&lt;/person&gt;\n종종 XML 문서를 나무 구조(tree structure)로 생각하는 것이 도움이 된다. 최상단 person 태그가 있고, phone 같은 다른 태그는 부모 노드의 자식(children) 노드로 표현된다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>웹서비스 사용하기</span>"
    ]
  },
  {
    "objectID": "tool-web.html#ws-xml",
    "href": "tool-web.html#ws-xml",
    "title": "19  웹서비스 사용하기",
    "section": "",
    "text": "그림 19.1: XML 나무구조 도식화",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>웹서비스 사용하기</span>"
    ]
  },
  {
    "objectID": "tool-web.html#ws-xml-parsing",
    "href": "tool-web.html#ws-xml-parsing",
    "title": "19  웹서비스 사용하기",
    "section": "\n19.2 XML 파싱",
    "text": "19.2 XML 파싱\n \n다음은 XML을 파싱하고 XML에서 데이터 요소를 추출하는 간단한 응용프로그램이다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nxml2 패키지 read_xml() 함수를 사용하여 XML 문자열 표현을 XML 노드 ’나무(tree)’로 변환한다. XML이 나무 구조로 되었을 때, XML에서 데이터 일부분을 추출하기 위해서 호출하는 함수가 있다. xml_find_first() 함수는 XML 나무를 훑어서 XPath 표현을 사용하여 특정한 태그와 매칭되는 노드(node)를 검색한다. 각 노드는 텍스트, 속성(즉, hide 같은), 그리고 “자식(child)” 노드로 구성된다. 각 노드는 노드 나무의 최상단이 될 수 있다.\nName:  Chuck \nAttr:  yes \nxml2 같은 XML 패키지를 사용하는 것은 장점이 있다. 상기 예제의 XML은 매우 간단하지만, 적합한 XML에 관해서 규칙이 많이 있고, XML 구문 규칙에 얽매이지 않고 xml2를 사용해서 XML에서 데이터를 추출할 수 있다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>웹서비스 사용하기</span>"
    ]
  },
  {
    "objectID": "tool-web.html#ws-xml-nodes",
    "href": "tool-web.html#ws-xml-nodes",
    "title": "19  웹서비스 사용하기",
    "section": "\n19.3 노드 반복하기",
    "text": "19.3 노드 반복하기\n\n종종 XML이 다중 노드를 가지고 있어서 모든 노드를 처리하는 루프를 작성할 필요가 있다. 다음 프로그램에서 모든 user 노드를 루프로 반복한다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nxml_find_all() 함수는 R 리스트의 하위 나무를 가져온다. 리스트는 XML 나무에서 user 구조를 표현한다. 그리고 나서, for 루프를 작성해서 각 user 노드 값을 확인하고 name, id 텍스트 요소와 user 노드에서 x 속성도 가져와서 출력한다.\nUser count: 2 \nName:  Chuck \nId:  001 \nAttribute:  2 \nName:  Brent \nId:  009 \nAttribute:  7",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>웹서비스 사용하기</span>"
    ]
  },
  {
    "objectID": "tool-web.html#webservice-json",
    "href": "tool-web.html#webservice-json",
    "title": "19  웹서비스 사용하기",
    "section": "\n19.4 JSON",
    "text": "19.4 JSON\n \nJSON(JavaScript Object Notation) 형식은 자바스크립트 언어에서 사용되는 객체와 배열 형식에서 영감을 얻었다. 하지만 파이썬이 자바스크립트 이전에 개발되어서 딕셔너리와 리스트의 파이썬 구문이 JSON 구문에 영향을 주었다. 그래서 JSON 포맷이 거의 파이썬 리스트와 딕셔너리의 조합과 일치한다.\n상기 간단한 XML에 대략 상응하는 JSON으로 작성한 것이 다음에 있다.\n\n{\n  \"name\" : \"Chuck\",\n  \"phone\" : {\n    \"type\" : \"intl\",\n    \"number\" : \"+1 734 303 4456\"\n   },\n   \"email\" : {\n     \"hide\" : \"yes\"\n   }\n}\n\n몇 가지 차이점에 주목하세요. 첫째로 XML에서는 “phone” 태그에 “intl” 같은 속성을 추가할 수 있다. JSON에서는 단지 키-값 페어(key-value pair)다. 또한 XML “person” 태그는 사라지고 외부 중괄호 세트로 대체되었다.\n일반적으로 JSON 구조가 XML보다 간단하다. 왜냐하면, JSON이 XML보다 적은 역량을 보유하기 때문이다. 하지만 JSON이 딕셔너리와 리스트의 조합에 직접 매핑된다는 장점이 있다. 그리고, 거의 모든 프로그래밍 언어가 파이썬 딕셔너리와 리스트에 상응하는 것을 갖고 있어서, JSON이 협업하는 두 프로그램 사이에서 데이터를 교환하는 매우 자연스러운 형식이 된다.\nXML에 비해서 상대적으로 단순하기 때문에, JSON이 응용프로그램 간 거의 모든 데이터를 교환하는 데 있어 빠르게 선택되고 있다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>웹서비스 사용하기</span>"
    ]
  },
  {
    "objectID": "tool-web.html#webservice-json-parsing",
    "href": "tool-web.html#webservice-json-parsing",
    "title": "19  웹서비스 사용하기",
    "section": "\n19.5 JSON 파싱하기",
    "text": "19.5 JSON 파싱하기\n딕셔너리(객체)와 리스트를 중첩함으로써 JSON을 생성한다. 이번 예제에서, 사용자 리스트를 표현하는데, 각 사용자가 키-값 페어(key-value pair, 즉, 딕셔너리)다. 그래서 리스트 딕셔너리가 있다.\n다음 프로그램에서 내장된 json 라이브러리를 사용하여 JSON을 파싱하여 데이터를 읽어온다. 이것을 상응하는 XML 데이터, 코드와 비교해 보세요. JSON은 조금 덜 정교해서 사전에 미리 리스트를 가져오고, 리스트가 사용자이고, 각 사용자가 키-값 페어 집합임을 알고 있어야 한다. JSON은 좀 더 간략(장점)하고 하지만 좀 더 덜 서술적(단점)이다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nJSON과 XML에서 데이터를 추출하는 코드를 비교하면, jsonlite 패키지 fromJSON() 함수는 JSON 파일을 즉시 정형 데이터프레임으로 변환시킨다. 프로그램 출력은 정확하게 상기 XML 버전과 동일한 정보를 데이터프레임으로 표현하고 있어 후속 작업에 훨씬 유연하게 대응할 수 있다.\n\nUser count: 2\nName Chuck\nId 001\nAttribute 2\nName Brent\nId 009\nAttribute 7\n\n일반적으로 웹서비스에 대해서 XML에서 JSON으로 옮겨가는 산업 경향이 뚜렷하다. JSON이 프로그래밍 언어에서 이미 갖고 있는 네이티브 자료 구조와 좀 더 직접적이며 간단히 매핑되기 때문에, JSON을 사용할 때 파싱하고 데이터 추출하는 코드가 더욱 간단하고 직접적이다. 하지만 XML이 JSON보다 좀 더 자기 서술적이고 XML이 강점을 가지는 몇몇 응용프로그램 분야가 있다. 예를 들어, 대부분의 워드 프로세서는 JSON보다는 XML을 사용하여 내부적으로 문서를 저장한다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>웹서비스 사용하기</span>"
    ]
  },
  {
    "objectID": "tool-web.html#ws-api",
    "href": "tool-web.html#ws-api",
    "title": "19  웹서비스 사용하기",
    "section": "\n19.6 API",
    "text": "19.6 API\n \n이제 HTTP를 사용하여 응용프로그램 간에 데이터를 교환할 수 있게 되었다. 또한, XML 혹은 JSON을 사용하여 응용프로그램 간에도 복잡한 데이터를 주고받을 수 있는 방법을 습득했다.\n다음 단계는 상기 학습한 기법을 사용하여 응용프로그램 간에 “계약(contract)”을 정의하고 문서화한다. 응용프로그램-대-응용프로그램 계약에 대한 일반적 명칭은 API 응용 프로그램 인터페이스(Application Program Interface)다. API를 사용할 때, 일반적으로 하나의 프로그램이 다른 응용 프로그램에서 사용할 수 있는 가능한 서비스 집합을 생성한다. 또한, 다른 프로그램이 서비스에 접근하여 사용할 때 지켜야 하는 API (즉, “규칙”)도 게시한다.\n다른 프로그램에서 제공되는 서비스에 접근을 포함하여 프로그램 기능을 개발할 때, 이러한 개발법을 SOA, Service-Oriented Architecture(서비스 지향 아키텍처)라고 부른다. SOA 개발 방식은 전반적인 응용 프로그램이 다른 응용 프로그램 서비스를 사용하는 것이다. 반대로, SOA가 아닌 개발 방식은 응용 프로그램이 하나의 독립된 응용 프로그램으로 구현에 필요한 모든 코드를 담고 있다.\n웹을 사용할 때 SOA 사례를 많이 찾아볼 수 있다. 웹사이트 하나를 방문해서 비행기표, 호텔, 자동차를 단일 사이트에서 예약 완료한다. 호텔 관련 데이터는 물론 항공사 컴퓨터에 저장되어 있지 않다. 대신에 항공사 컴퓨터는 호텔 컴퓨터와 계약을 맺어 호텔 데이터를 가져와서 사용자에게 보여준다. 항공사 사이트를 통해서 사용자가 호텔 예약을 동의할 경우, 항공사 사이트에서 호텔 시스템의 또 다른 웹서비스를 통해서 실제 예약을 한다. 전체 거래(transaction)를 완료하고 카드 결제를 진행할 때, 다른 컴퓨터가 프로세스에 관여하여 처리한다.\n\n\n\n\n\n그림 19.2: 서비스 지향 아키텍처\n\n\n서비스 지향 아키텍처는 많은 장점이 있다. (1) 항상 단 하나의 데이터만 유지 관리한다. 이중으로 중복 예약을 원치 않는 호텔 같은 경우에 매우 중요하다. (2) 데이터 소유자가 데이터 사용에 대한 규칙을 정한다. 이러한 장점으로, SOA 시스템은 좋은 성능과 사용자 요구를 모두 만족하기 위해서 신중하게 설계되어야 한다.\n응용프로그램이 웹상에 이용 가능한 API로 서비스 집합을 만들 때, 웹서비스(web services)라고 부른다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>웹서비스 사용하기</span>"
    ]
  },
  {
    "objectID": "tool-web.html#google-geocoding",
    "href": "tool-web.html#google-geocoding",
    "title": "19  웹서비스 사용하기",
    "section": "\n19.7 지오코딩 웹서비스",
    "text": "19.7 지오코딩 웹서비스\n \n구글이 자체적으로 구축한 대용량 지리 정보 데이터베이스를 누구나 이용할 수 있게 하는 훌륭한 웹서비스가 있다. “Ann Arbor, MI” 같은 지리 검색 문자열을 지오코딩 API에 넣으면, 검색 문자열이 의미하는 지도상에 위치와 근처 주요 지형지물 정보를 나름 최선을 다해서 예측 제공한다.\n지오코딩 서비스는 무료지만 사용량이 제한되어 있어서, 상업적 응용프로그램에 API를 무제한 사용할 수는 없다. 하지만, 최종 사용자가 자유 형식 입력 박스에 위치 정보를 입력하는 설문 데이터가 있다면, 구글 API를 사용하여 데이터를 깔끔하게 정리하는 데는 유용하다.\n구글 지오코딩 API 같은 무료 API를 사용할 때, 자원 사용에 대한 지침을 준수해야 한다. 너무나 많은 사람이 서비스를 남용하게 되면, 구글은 무료 서비스를 중단하거나, 상당 부분 줄일 수 있다.\n\nWhen you are using a free API like Google’s geocoding API, you need to be respectful in your use of these resources. If too many people abuse the service, Google might drop or significantly curtail its free service.\n\n서비스에 대해서 자세한 사항을 온라인 문서를 정독할 수 있지만, 무척 간단해서 브라우저에 다음 URL을 입력해서 테스트까지 할 수 있다.\nhttp://maps.googleapis.com/maps/api/geocode/json?sensor=false &address=Ann+Arbor%2C+MI\n웹 브라우저에 붙여넣기 전에, URL만 뽑아냈고 URL에서 모든 공백을 제거했는지 확인한다. 그리고, 브라우저에 붙여 넣기한다.\n다음은 간단한 응용 프로그램이다. 사용자가 검색 문자열을 입력하고 구글 지오코딩 API를 호출하여 반환된 JSON에서 정보를 추출한다. 구글 지리정보 API는 상용으로 전환되었기에 다음카카오 지도 API를 대체하여 동일한 개발 작업을 수행한다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n                      주소             경도             위도\n1 서울 강남구 테헤란로 152 127.036508620542 37.5000242405515\n\n프로그램이 사용자로부터 검색 문자열을 받는다. 적절히 인코딩된 매개변수로 검색 문자열을 변환하여 URL을 만든다. 그리고 나서 httr 패키지를 사용하여 카카오 지오코딩 API에서 텍스트를 가져온다. 고정된 웹페이지와 달리, 반환되는 데이터는 전송한 매개변수와 카카오 서버에 저장된 지리정보 데이터에 따라 달라진다.\nJSON 데이터를 가져오면, jsonlite 패키지로 파싱하고 전송받은 데이터가 올바른지 확인하는 몇 가지 절차를 거친 후에 찾고자 하는 정보를 추출한다.\nRscript 프로그램 실행을 위해서 사용자 입력과 API 키 외부 유출 방지를 위한 조치를 취한 후에 일반화를 위해 코드를 일부 수정한다.\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n프로그램 출력결과는 다음과 같다.\n$ Rscript code/api_address.R \n\n주소를 입력하세요 - '서울특별시 강남구 역삼동 737'\n                      주소             경도             위도\n1 서울 강남구 테헤란로 152 127.036508620542 37.5000242405515\n다음카카오 지도 API 외 다른 지오코딩 관련 자세한 사항은 공간통계를 위한 데이터 사이언스 - 지리정보 API - 주소와 위도경도 웹사이트를 참조한다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>웹서비스 사용하기</span>"
    ]
  },
  {
    "objectID": "tool-web.html#ws-security",
    "href": "tool-web.html#ws-security",
    "title": "19  웹서비스 사용하기",
    "section": "\n19.8 보안과 API 사용",
    "text": "19.8 보안과 API 사용\n \n상용 업체 API를 사용하기 위해서는 일종의 “API 키(API key)”가 일반적으로 필요하다. 서비스 제공자 입장에서 누가 서비스를 사용하고 있으며 각 사용자가 얼마나 사용하고 있는지를 알고자 한다. 상용 API 제공 업체는 서비스에 대한 무료 사용자와 유료 사용자에 대한 구분을 두고 있다. 특정 기간 동안 한 개인 사용자가 사용할 수 있는 요청 수에 대해 제한을 두는 정책을 두고 있다.\n때때로 API 키를 얻게 되면, API를 호출할 때 POST 데이터의 일부로 포함하거나 URL의 매개변수로 키를 포함시킨다.\n또 다른 경우에는 업체가 서비스 요청에 대한 보증을 강화해서 공유 키와 비밀번호를 암호화된 메시지 형식으로 보내도록 요구한다. 인터넷을 통해서 서비스 요청을 암호화하는 일반적인 기술을 OAuth라고 한다. http://www.oauth.net 사이트에서 OAuth 프로토콜에 대해 더 많은 정보를 만날 수 있다.\n트위터(현 x.com) API가 점차적으로 가치 있게 됨에 따라 트위터가 공개된 API에서 API를 매번 호출할 때마다 OAuth 인증을 거치도록 API를 바꾸었다. 다행스럽게도 편리한 OAuth 라이브러리가 많이 있다.\n그래서 명세서를 읽고 아무것도 없는 상태에서 OAuth 구현하는 것을 피할 수 있게 되었다. 이용 가능한 라이브러리는 복잡성도 다양한 만큼 기능적으로도 다양하다. OAuth 웹사이트에서 다양한 OAuth 라이브러리 정보를 확인할 수 있다.\nOAuth 보안 요구사항을 충족하기 위해 추가된 다양한 매개변수 의미를 좀 더 자세히 알고자 한다면, OAuth 명세서를 읽어보기 바란다.\n이와 같은 보안 API 키는 누가 트위터 API를 사용하고 어느 정도 수준으로 트위터를 사용하는지에 대해서 트위터가 확고한 신뢰를 갖게 한다. 사용량에 한계를 두고 서비스를 제공하는 방식은 단순히 개인적인 목적으로 데이터 검색을 할 수는 있지만, 하루에 수백만 API 호출로 데이터를 추출하여 제품을 개발하지 못하게 제한하는 기능도 동시에 한다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>웹서비스 사용하기</span>"
    ]
  },
  {
    "objectID": "tool-web.html#ws-term",
    "href": "tool-web.html#ws-term",
    "title": "19  웹서비스 사용하기",
    "section": "\n19.9 용어 정의",
    "text": "19.9 용어 정의\n\n\nAPI: 응용 프로그램 인터페이스(Application Program Interface) - 두 응용 프로그램 컴포넌트 간에 상호작용하는 패턴을 정의하는 응용 프로그램 간의 계약. \n\nElementTree: XML 데이터를 파싱하는 데 사용되는 파이썬 내장 라이브러리. \n\nxml2: XML 데이터를 파싱하는 데 사용되는 R 내장 라이브러리. \n\nJSON: JavaScript Object Notation - 자바스크립트 객체(JavaScript Objects) 구문을 기반으로 구조화된 데이터 마크업(markup)을 허용하는 형식. \n\nREST: REpresentational State Transfer - HTTP 프로토콜을 사용하여 응용 프로그램 내부에 자원에 접근을 제공하는 일종의 웹서비스 스타일. \n\nSOA: 서비스 지향 아키텍처(Service Oriented Architecture) - 응용 프로그램이 네트워크에 연결된 컴포넌트로 구성될 때. \n\nXML: 확장 마크업 언어(eXtensible Markup Language) - 구조화된 데이터의 마크업을 허용하는 형식.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>웹서비스 사용하기</span>"
    ]
  },
  {
    "objectID": "tool-database.html",
    "href": "tool-database.html",
    "title": "20  데이터베이스와 SQL",
    "section": "",
    "text": "20.1 용어정의",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>데이터베이스와 SQL</span>"
    ]
  },
  {
    "objectID": "tool-database.html#db-terminology",
    "href": "tool-database.html#db-terminology",
    "title": "20  데이터베이스와 SQL",
    "section": "",
    "text": "속성(attribute): 튜플 내부에 값의 하나. 좀더 일반적으로 “열”, “칼럼”, “필드”로 불린다. \n\n\n제약(constraint): 데이터베이스가 테이블의 필드나 행에 규칙을 강제하는 것. 일반적인 제약은 특정 필드에 중복된 값이 없도록 하는 것(즉, 모든 값이 유일해야 한다.) \n\n\n커서(cursor): 커서를 사용해서 데이터베이스에서 SQL 명령어를 수행하고 데이터베이스에서 데이터를 가져온다. 커서는 네트워크 연결을 위한 소켓이나 파일의 파일 핸들러와 유사하다. \n\n\n데이터베이스 브라우져(database browser): 프로그램을 작성하지 않고 직접적으로 데이터베이스에 연결하거나 데이터베이스를 조작할 수 있는 소프트웨어. \n\n\n외부 키(foreign key): 다른 테이블에 있는 행의 주키를 가리키는 숫자 키. 외부 키는 다른 테이블에 저장된 행사이에 관계를 설정한다. \n\n\n인텍스(index): 테이블에 행이 추가될 때 정보 검색하는 것을 빠르게 하기 위해서 데이터베이스 소프트웨어가 유지관리하는 추가 데이터. \n\n\n논리 키(logical key): “외부 세계”가 특정 행의 정보를 찾기 위해서 사용하는 키. 사용자 계정 테이블의 예로, 사람의 전자우편 주소는 사용자 데이터에 대한 논리 키의 좋은 후보자가 될 수 있다. \n\n\n정규화(normalization): 어떠한 데이터도 중복이 없도록 데이터 모델을 설계하는 것. 데이터베이스 한 장소에 데이터 각 항목 정보를 저장하고 외부키를 이용하여 다른 곳에서 참조한다. \n\n\n주키(primary key): 다른 테이블에서 테이블의 한 행을 참조하기 위해서 각 행에 대입되는 숫자 키. 종종 데이터베이스는 행이 삽입될 때 주키를 자동 삽입하도록 설정되었다. \n\n\n관계(relation): 튜플과 속성을 담고 있는 데이터베이스 내부 영역. 좀더 일반적으로 “테이블(table)”이라고 한다. \n\n\n튜플(tuple):데이터베이스 테이블에 단일 항목으로 속성 집합이다. 좀더 일반적으로 “행(row)”이라고 한다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>데이터베이스와 SQL</span>"
    ]
  },
  {
    "objectID": "tool-viz.html",
    "href": "tool-viz.html",
    "title": "21  시각화",
    "section": "",
    "text": "21.1 나이팅게일 신화탄생\n코딩에서 데이터 시각화로의 전환은 단순한 기술적 변화를 넘어서, 복잡한 데이터셋을 시각적으로 해석하고 전달하는 새로운 차원으로의 도약을 의미한다. 코딩이 데이터를 처리하고 분석하는 데 필수적이지만, 시각화는 데이터가 전달하는 이야기를 더 잘 이해하고 공유할 수 있게 함으로써, 기술적 능력과 창의적 표현을 결합하는 과정이며, 데이터에 내재된 패턴과 통찰을 명확하고 강력한 시각적 형태로 변환하는 작업이다.\n시각화는 데이터를 보다 인간 중심적으로 만들며, 복잡한 수치와 추세를 누구나 이해할 수 있는 형태로 재구성하는 데 기여한다. 이러한 과정에서 코딩은 도구로서 역할을 하고, 시각적 스토리텔링은 데이터가 지닌 의미를 풍부하게 전달하는 매개체가 된다.\n플로렌스 나이팅게일의 데이터 시각화 작업은 데이터의 힘을 보여주는 역사적인 사례로, 이를 통해 복잡한 데이터를 명확하고 효과적으로 전달하는 방법의 중요성을 인식하게 되었다. 19세기 공중 보건과 군의학 분야에 혁신을 통해 데이터가 단순한 정보의 전달을 넘어 사회 변화와 인류 보건 위생 개선에 중요한 기여를 할 수 있음을 증명했다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>시각화</span>"
    ]
  },
  {
    "objectID": "tool-viz.html#나이팅게일-신화탄생",
    "href": "tool-viz.html#나이팅게일-신화탄생",
    "title": "21  시각화",
    "section": "",
    "text": "21.1.1 배경\n \n크림 전쟁은 1853년부터 1856년까지 일어난 큰 전쟁이었다. 한쪽에는 러시아, 반면 다른 한쪽에는 영국, 프랑스, 오스만 제국(현대 튀르키예), 그리고 나중에 사르디니아(현대 이탈리아의 일부)가 동맹을 구성하여 전쟁을 치렀다. 전쟁이 바로 시작된 이유는 러시아가 오스만 제국 내 정교회 신자들을 보호하려는 명분을 내세웠지만, 사실 더 많은 영토를 차지하기 위함이었다. 양측 간 전쟁은 흑해를 두고 남하하는 러시아에 맞서 동맹군이 크림반도에서 발생하여 “크림전쟁”(Crimean War)으로 불린다. 영화로 소개된 경기병대의 돌격(“Charge of the Light Brigade”), 영국 간호사 플로렌스 나이팅게일의 활약, 전신과 철도의 본격적인 도입으로 큰 의미를 갖는 전쟁이기도 하다. 많은 전투와 많은 사람들이 죽은 후, 1856년 파리 조약으로 전쟁은 마무리되어, 러시아의 확장은 잠시 멈추게 되었고, 오스만 제국도 한숨 돌린 계기가 되었다.\n크림 전쟁 중 스쿠타리 막사는 튀르키예(구 터키)의 스쿠타리 병원(Scutari Hospital, Turkey)이 영국 군 병원으로 개조되었다. 크림전쟁에서 부상을 당한 수많은 병사가 치료를 위해 이곳으로 보내졌지만, 병자와 부상병들을 감당할 수 있도록 설계되지 않았고 제대로 된 역할도 수행하지 못했다. 1854년 나이팅게일이 간호사 일행과 함께 도착했을 때, 비위생적인 환경과 고통받는 병사들을 보고 경악했다. 나이팅게일의 스쿠타리 병원에서의 경험은 병원과 의료 서비스를 개선하여 이와 같은 고통과 비극이 재발하지 않도록 향후 프로젝트의 중요한 동기와 방향이 되었다.\n\n\n\n\n\n그림 21.1: 스쿠타리 병원의 한 병동 석판화 그림 (William Simpson)\n\n\n환자의 사망률을 42%에서 2%로 낮추고 집중치료실(ICU)을 설치하여 상태가 중한 환자를 격리하여 집중 관리하는 등 근대적인 간호체계를 수립하는 데 기여했다.\n\n21.1.2 원본 데이터\n크림 전쟁 중 튀르키예의 스쿠타리 병원에서 몇 년간에 걸쳐 수작업으로 종이에 분석 가능한 형태의 자료를 만들어내는 것은 결코 쉬운 작업이 아니다.\n\n\n\n\n\n그림 21.2: 원본 데이터\n\n\n\n21.1.3 그래프 진화\n복잡한 논거를 제시하는 대신 구체적인 주장에 데이터 시각화와 데이터 스토리텔링(Storytelling)을 통해 청중에 한걸음 더 다가섰다. 나이팅게일의 스토리텔링은 열악한 위생 상태와 과밀로 인해 불필요한 죽음이 얼마나 많이 발생하는지 이해하기 쉬운 비교를 통해 이야기를 구성해서 설득해 나갔다. 예를 들어, 군대 사망률을 민간인 사망률(유사한 환경의 맨체스터)과 비교하는 프레임을 제시하고, 군대 막사에서 생활하는 평시 병사들이 비슷한 연령대 민간인 남성보다 더 높은 비율로 사망하는 것을 제시했다. 이를 통해, 데이터가 보여주는 현실을 부정할 수 없게 만들었고, 군대 행정에 극적인 개혁을 이끌어냈다. 1\n\n\n\n\n\n\n\n\n\n(a) 막대그래프\n\n\n\n\n\n\n\n\n\n(b) 맨체스터 사망\n\n\n\n\n\n\n빅토리아 여왕 보고(I)\n\n\n\n\n\n빅토리아 여왕 보고(II)\n\n\n\n\n\n빅토리아 여왕 보고(III)\n\n\n\n\n\n그림 21.3: 나이팅게일 그래프 진화과정\n\n\n\n21.1.4 설득\n\n나이팅게일은 크림 전쟁 중 병원에서의 위생 문제와 관련된 데이터를 수집하고 분석하여 그 결과를 시각화했고, 병원에서의 사망 원인 중 대부분이 감염성 질병으로 인한 것임을 발견했다. 이러한 감염성 질병은 부적절한 위생 조건과 밀접한 관련이 있음을 확인했다.\n나이팅게일은 병원의 위생 상태 개선을 통해 수많은 생명을 구할 수 있다는 사실을 확인했고, 연구 결과와 권장 사항을 다양한 영국 정부 부처에 제출했으며, 특히 1858년에 영국의 장관들에게 보고서를 제출했다. 이를 통해서 군 병원의 위생 조건을 개선하는 데 큰 영향을 미쳤다.\n\n\n\n\n\n그림 21.4: 나이팅게일과 빅토리아 여왕\n\n\n\n21.1.5 성과와 영향\n\n나이팅게일 캠페인이 민간 공중보건에 미친 가장 큰 영향은 실현되기까지 오랜 기간에 걸쳐 다각도로 검토되었고, 마침내 1875년 영국 공중보건법(British Public Health Act)에 법제화되었다. 이 법에는 잘 정비된 하수도, 깨끗한 수돗물, 건축법 규제 등의 요건이 담겨있다. 질병에 대한 면역력을 강화하는 백신과 농작물 수확량을 획기적으로 늘리는 인공비료 개발과 함께 이 제도적인 노력으로 평균 수명을 두 배로 늘리는 원동력이 되었다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>시각화</span>"
    ]
  },
  {
    "objectID": "tool-viz.html#작업과정",
    "href": "tool-viz.html#작업과정",
    "title": "21  시각화",
    "section": "\n21.2 작업과정",
    "text": "21.2 작업과정\n\n21.2.1 디지털 데이터\n \n스페인 R-ladies GitHub 저장소 rladies/spain_nightingale에서 엑셀 형태로 된 데이터를 가져와서 전처리 작업을 진행한다.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\ndeath_raw &lt;- read_excel(\"data/datos_florence.xlsx\", sheet = \"Sheet1\", skip = 1)\n\ndeath_tbl &lt;- death_raw |&gt; \n  janitor::clean_names() |&gt; \n  set_names(c(\"Month\", \"Army\", \"Disease\", \"Wounds\", \"Other\", \"Disease.rate\", \"Wounds.rate\", \"Other.rate\")) |&gt; \n  mutate(Date = lubridate::my(Month)) |&gt; \n  separate(Month, into = c(\"Month\", \"Year\"), sep = \" |_\") |&gt; \n  select(Date, Month, Year, everything()) \n\ndeath_tbl\n#&gt; # A tibble: 24 × 10\n#&gt;   Date       Month Year   Army Disease Wounds Other Disease.rate Wounds.rate\n#&gt;   &lt;date&gt;     &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 1854-04-01 Apr   1854   8571       1      0     5          1.4         0  \n#&gt; 2 1854-05-01 May   1854  23333      12      0     9          6.2         0  \n#&gt; 3 1854-06-01 Jun   1854  28333      11      0     6          4.7         0  \n#&gt; 4 1854-07-01 Jul   1854  28722     359      0    23        150           0  \n#&gt; 5 1854-08-01 Aug   1854  30246     828      1    30        328.          0.4\n#&gt; 6 1854-09-01 Sep   1854  30290     788     81    70        312.         32.1\n#&gt; # ℹ 18 more rows\n#&gt; # ℹ 1 more variable: Other.rate &lt;dbl&gt;\n\nHistDate 패키지에 동일한 데이터셋이 잘 정제되어 있어 이를 바로 활용해도 좋다.\n\nlibrary(HistData)\n\nHistData::Nightingale |&gt; \n  as_tibble()\n#&gt; # A tibble: 24 × 10\n#&gt;   Date       Month  Year  Army Disease Wounds Other Disease.rate Wounds.rate\n#&gt;   &lt;date&gt;     &lt;ord&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;  &lt;int&gt; &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 1854-04-01 Apr    1854  8571       1      0     5          1.4         0  \n#&gt; 2 1854-05-01 May    1854 23333      12      0     9          6.2         0  \n#&gt; 3 1854-06-01 Jun    1854 28333      11      0     6          4.7         0  \n#&gt; 4 1854-07-01 Jul    1854 28722     359      0    23        150           0  \n#&gt; 5 1854-08-01 Aug    1854 30246     828      1    30        328.          0.4\n#&gt; 6 1854-09-01 Sep    1854 30290     788     81    70        312.         32.1\n#&gt; # ℹ 18 more rows\n#&gt; # ℹ 1 more variable: Other.rate &lt;dbl&gt;\n\n\n21.2.2 데이터와 사투\n앞서 준비한 death_tbl 데이터프레임에서 사망 관련 데이터를 처리하고 시각화하기 위한 전처리를 수행하여 시각화를 위한 준비작업을 수행한다. 먼저 Date, Disease.rate, Wounds.rate, Other.rate 칼럼을 선택하고, pivot_longer 함수를 사용해 시각화에 적합한 데이터로 재구조화한다. str_replace_all 함수를 사용하여 칼럼 이름에서 “.rate”를 제거하고, ifelse 함수를 이용해 날짜를 기준으로 나이팅게일 팀이 준비한 방식을 적용하기 전과 후의 “이전”과 “이후” 체제로 구분한다. factor 함수를 사용하여 범주 순서를 정의하고, 마지막으로 month 함수를 이용해 날짜에서 해당 월을 추출하고 death_viz에 저장한다.\n\ndeath_viz &lt;- death_tbl %&gt;% \n  select(Date, Disease.rate, Wounds.rate, Other.rate) %&gt;% \n  pivot_longer(-Date, names_to = \"사망원인\", values_to = \"사망자수\") |&gt; \n  mutate(사망원인 = str_replace_all(사망원인, \"\\\\.rate\", \"\"), \n         체제 = ifelse(Date &lt;= as.Date(\"1855-03-01\"), \"조치이전\", \"조치이후\")) %&gt;% \n  mutate(체제 = factor(체제, levels = c(\"조치이전\", \"조치이후\"))) %&gt;%  \n  mutate(해당월 = month(Date, label = TRUE, abbr = TRUE)) |&gt; \n  mutate(사망원인 = case_when(사망원인 == \"Disease\" ~ \"질병\",\n                              사망원인 == \"Wounds\" ~ \"부상\",\n                              사망원인 == \"Other\" ~ \"기타\")) |&gt; \n  mutate(사망원인 = factor(사망원인, levels = c(\"질병\", \"부상\", \"기타\")))\n\ndeath_viz\n#&gt; # A tibble: 72 × 5\n#&gt;   Date       사망원인 사망자수 체제     해당월\n#&gt;   &lt;date&gt;     &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;    &lt;ord&gt; \n#&gt; 1 1854-04-01 질병          1.4 조치이전 \" 4\"  \n#&gt; 2 1854-04-01 부상          0   조치이전 \" 4\"  \n#&gt; 3 1854-04-01 기타          7   조치이전 \" 4\"  \n#&gt; 4 1854-05-01 질병          6.2 조치이전 \" 5\"  \n#&gt; 5 1854-05-01 부상          0   조치이전 \" 5\"  \n#&gt; 6 1854-05-01 기타          4.6 조치이전 \" 5\"  \n#&gt; # ℹ 66 more rows",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>시각화</span>"
    ]
  },
  {
    "objectID": "tool-viz.html#시각화",
    "href": "tool-viz.html#시각화",
    "title": "21  시각화",
    "section": "\n21.3 시각화",
    "text": "21.3 시각화\n \n‘ggplot2’ 패키지를 이용하여 크림전쟁 나이팅게일 활약상을 담은 데이터를 시각화한다. 나이팅게일 활약 전과 후로 데이터(death_viz)를 나눠 “크림전쟁 병사 사망원인”에 대한 극좌표계 시각화를 통해 이해하기쉬운 설득력 있는 시각화 결과물을 제시하고 있다. 추가적으로, ‘showtext’ 패키지로 구글 “Noto Serif KR” 글꼴을 선택 적용하고, ‘hrbrthemes’ 라이브러리를 이용하여 뒷배경 검정색을 사용하여 붉은색 질병으로 인한 사망자 수의 확연한 감소를 시각적으로 강조한다.\n\nlibrary(hrbrthemes) \nlibrary(showtext)\nshowtext.auto()\nfont_add_google(name = \"Noto Serif KR\", family = \"noto_serif\")\nnoto_font &lt;- \"noto_serif\"\n\ndeath_gg &lt;- death_viz %&gt;% \n  ggplot(aes(x = 해당월, y = 사망자수, fill = 사망원인)) +\n  geom_col(color = \"grey20\") + \n  theme_modern_rc(base_family = noto_font, subtitle_family = noto_font) + \n  scale_fill_manual(values = c(\"firebrick\", \"orange\", \"#365181\"), name = \"\") +\n  scale_y_sqrt() +\n  facet_wrap(~ 체제) + \n  coord_equal(ratio = 1) +  \n  coord_polar() +\n  labs(title = \"크림전쟁 병사 사망원인\", \n       subtitle = \"데이터 시각화와 커뮤니케이션\", \n       caption = \"데이터 출처: 크림전쟁 사망자\") + \n  theme(legend.position = \"top\", \n        text = element_text(family = noto_font, size = 18),\n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text.x   = element_text(family = \"MaruBuri\", color = \"white\", size = 50),\n        axis.text.y   = element_text(family = \"MaruBuri\", color = \"white\", size = 50),\n        plot.margin = unit(rep(0.7, 4), \"cm\"),\n        plot.title = element_text(color = \"white\", family = noto_font, size = 100),\n        plot.subtitle = element_text(color = \"grey70\", size = 70),\n        plot.caption = element_text(color = \"grey70\", family = noto_font, size = 54),\n        legend.title = element_text(color = \"white\", size = 70),\n        legend.text = element_text(color = \"white\", size = 55),\n        strip.text = element_text(color = \"white\", size = 80, face = \"bold\", family = noto_font, hjust = 0.5))\n\ndeath_gg\n\nragg::agg_jpeg(\"images/viz_death_gg.jpeg\", width = 10, height = 7, units = \"in\", res = 600)\ndeath_gg\ndev.off()\n\n\n\n\n\n\n그림 21.5: 크림전쟁 병사 사망원인 조치전후 시각화\n\n\n\n21.3.1 선그래프\n \n나이팅게일은 간호 분야의 선구자로 잘 알려져 있지만, 통계학자로서 “콕스콤(CoxComb)” 또는 “장미 다이어그램”(Rose Diagram)으로 알려진 원그래프를 제시하였지만 현재는 시간의 흐름에 따라 병사 사망자 수 변화를 조치 전후로 명확히 하는 방법으로 선그래프가 기본 기법으로 자리 잡고 있다.\n\ndeath_new_gg &lt;- death_viz |&gt; \n  ggplot(aes(x = Date, y = 사망자수, color = 사망원인)) +\n    geom_line() +\n    geom_point() +\n    geom_vline(xintercept = as.Date(\"1855-03-15\"), linetype= 2) +\n    theme_ipsum_pub(base_family = noto_font, subtitle_family = noto_font) +\n    labs(title = \"크림전쟁 병사 사망원인\", \n         subtitle = \"데이터 시각화와 커뮤니케이션\", \n         caption = \"데이터 출처: 크림전쟁 사망자\",\n         x = \"월일\") + \n    scale_y_continuous(labels = scales::comma, limits = c(0, 1150)) +\n    theme(legend.position = \"top\", \n          text = element_text(family = noto_font, size = 35),\n          axis.ticks = element_blank(),\n          plot.margin = unit(rep(0.7, 4), \"cm\"),\n          axis.title.x   = element_text(family = \"MaruBuri\", size = 45),\n          axis.title.y   = element_text(family = \"MaruBuri\", size = 45),\n          axis.text.x    = element_text(family = \"MaruBuri\", size = 35),\n          axis.text.y    = element_text(family = \"MaruBuri\", size = 35),\n          plot.title = element_text(color = \"black\", family = noto_font, size = 100),\n          plot.subtitle = element_text(color = \"black\", family = noto_font, size = 80),\n          plot.caption = element_text(color = \"grey10\", family = noto_font, size = 37),\n          legend.text = element_text(color = \"black\", size = 45)) +\n    geom_segment(x = as.Date(\"1854-03-01\"), y = 1100,\n                 xend = as.Date(\"1855-03-01\"), yend = 1100,\n                 color = \"gray70\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    geom_segment(x = as.Date(\"1855-04-01\"), y = 1100,\n                 xend = as.Date(\"1856-03-01\"), yend = 1100,\n                 color = \"gray15\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"text\", x = as.Date(\"1854-09-01\"), y = 1140, label = \"조치이전\",\n             size = 20.5, color = \"gray30\", family = noto_font) +\n    annotate(\"text\", x = as.Date(\"1855-09-01\"), y = 1140, label = \"조치이후\",\n             size = 20.5, color = \"gray15\", family = noto_font)          \n\ndeath_new_gg\n\nragg::agg_jpeg(\"images/death_new_gg.jpeg\", width = 10, height = 7, units = \"in\", res = 600)\ndeath_new_gg\ndev.off()\n\n\n\n\n\n\n그림 21.6: 크림전쟁 병사 사망원인 조치전후 선 그래프\n\n\n\n21.3.2 막대그래프\n동일한 정보를 막대그래프를 통해 시각화를 할 수도 있다. 원그래프와 비교하여 보면 명확하게 사망자 수를 직관적으로 비교할 수 있다는 점에서 큰 장점이 있다.\n\ndeath_bar_gg &lt;- death_viz |&gt; \n  ggplot() +\n    geom_col(aes(x = Date, y = 사망자수, fill = 사망원인), colour=\"white\") +\n    geom_vline(xintercept = as.Date(\"1855-03-15\"), linetype= 2) +\n    scale_fill_manual(values = c(\"firebrick\", \"orange\", \"#365181\")) + \n    # theme_ipsum_pub(base_family = noto_font, subtitle_family = noto_font) +\n    labs(title = \"크림전쟁 병사 사망원인\", \n         subtitle = \"데이터 시각화와 커뮤니케이션\", \n         caption = \"데이터 출처: 크림전쟁 사망자\",\n         x = \"월일\") + \n    scale_y_continuous(labels = scales::comma, limits = c(0, 1150)) +\n    theme(legend.position = \"top\", \n          text = element_text(family = noto_font, size = 35),\n          axis.ticks = element_blank(),\n          plot.margin = unit(rep(0.7, 4), \"cm\"),\n          axis.title.x   = element_text(family = \"MaruBuri\", size = 45),\n          axis.title.y   = element_text(family = \"MaruBuri\", size = 45),\n          axis.text.x    = element_text(family = \"MaruBuri\", size = 35),\n          axis.text.y    = element_text(family = \"MaruBuri\", size = 35),\n          plot.title = element_text(color = \"black\", family = noto_font, size = 100),\n          plot.subtitle = element_text(color = \"black\", family = noto_font, size = 80),\n          plot.caption = element_text(color = \"grey10\", family = noto_font, size = 37),\n          legend.title = element_text(color = \"black\", size = 70),\n          legend.text = element_text(color = \"black\", size = 45)) +\n    geom_segment(x = as.Date(\"1854-03-01\"), y = 1100,\n                 xend = as.Date(\"1855-03-01\"), yend = 1100,\n                 color = \"gray70\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    geom_segment(x = as.Date(\"1855-04-01\"), y = 1100,\n                 xend = as.Date(\"1856-03-01\"), yend = 1100,\n                 color = \"gray15\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"text\", x = as.Date(\"1854-09-01\"), y = 1140, label = \"조치이전\",\n             size = 8.5, color = \"gray30\", family = noto_font) +\n    annotate(\"text\", x = as.Date(\"1855-09-01\"), y = 1140, label = \"조치이후\",\n             size = 8.5, color = \"gray15\", family = noto_font) \n\ndeath_bar_gg\n\nragg::agg_jpeg(\"images/death_bar_gg.jpeg\", width = 10, height = 7, units = \"in\", res = 600)\ndeath_bar_gg\ndev.off()\n\n\n\n\n\n\n그림 21.7: 크림전쟁 병사 사망원인 조치전후 막대 그래프",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>시각화</span>"
    ]
  },
  {
    "objectID": "tool-viz.html#표-문법",
    "href": "tool-viz.html#표-문법",
    "title": "21  시각화",
    "section": "\n21.4 표 문법",
    "text": "21.4 표 문법\n \n데이터 문법, 그래프 문법에 이어 최근 “표 문법”이 새롭게 자리를 잡아가고 있다. 표 문법에 맞춰 나이팅게일 크림전쟁 사망자 수를 조치 이전과 조치 이후로 나눠 요약하면 확연한 차이를 파악할 수 있다.\ngt와 gtExtras 패키지를 활용하여 death_viz 데이터프레임을 사망 원인별 사망자 수를 “조치 이전”과 “조치 이후”로 구분하여 표를 두 개 생성한다. 각 표는 날짜, 질병, 부상, 기타 범주로 사망자 수와 그 합계를 표시하며, 총 사망자 수가 250명을 초과하는 행에 대한 강조 색상을 입히고 나서 두 표를 나란히 배치하여 조치 전후 효과를 시각적으로 비교한다.\n\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(patchwork)\n\nbefore_tbl &lt;- death_viz |&gt; \n  filter(체제 == \"조치이전\")\n\nafter_tbl &lt;- death_viz |&gt; \n  filter(체제 == \"조치이후\")\n\nbefore_gt &lt;- before_tbl |&gt; \n  pivot_wider(names_from = 사망원인, values_from = 사망자수) |&gt; \n  select(날짜 = Date, 질병, 부상, 기타) |&gt; \n  mutate(합계 = 질병 + 부상 + 기타) |&gt; \n  gt() |&gt; \n    gt_theme_538() |&gt; \n    cols_align(\"center\") |&gt; \n    fmt_integer( columns = 질병:합계) |&gt; \n    tab_spanner(label = \"조치 이전\", columns = c(질병, 부상, 기타)) |&gt; \n    data_color(\n      columns = c(질병, 부상, 기타, 합계),\n      rows = 합계 &gt; 250,      \n      method = \"numeric\",\n      palette = \"ggsci::red_material\")\n\nafter_gt &lt;- after_tbl |&gt; \n  pivot_wider(names_from = 사망원인, values_from = 사망자수) |&gt; \n  select(날짜 = Date, 질병, 부상, 기타) |&gt; \n  mutate(합계 = 질병 + 부상 + 기타) |&gt; \n  gt() |&gt; \n    gt_theme_538() |&gt; \n    cols_align(\"center\") |&gt; \n    fmt_integer( columns = 질병:합계) |&gt; \n  tab_spanner(label = \"조치 이후\", columns = c(질병, 부상, 기타)) |&gt; \n  data_color(\n    columns = c(질병, 부상, 기타, 합계),\n    rows = 합계 &gt; 250,      \n    method = \"numeric\",\n    palette = \"ggsci::red_material\")\n\n# before_gt |&gt; \n#    gtsave(\"before_gt.png\", path = \"images\")\n# after_gt  |&gt; \n#    gtsave(\"after_gt.png\", path = \"images\")\n\nlibrary(cowplot)\np111 &lt;- ggdraw() + draw_image(\"images/before_gt.png\", scale = 1.0)\np112 &lt;- ggdraw() + draw_image(\"images/after_gt.png\", scale = 0.9)\nplot_grid(p111, p112)\n\n\n\n\n\n\n그림 21.8: 조치 전후 변화를 표 시각화",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>시각화</span>"
    ]
  },
  {
    "objectID": "tool-viz.html#커뮤니케이션",
    "href": "tool-viz.html#커뮤니케이션",
    "title": "21  시각화",
    "section": "\n21.5 커뮤니케이션",
    "text": "21.5 커뮤니케이션\n \n데이터를 기반으로 뭔가 유용한 것을 창출한 후에 이를 알리기 위해 커뮤니케이션 단계를 거치게 된다. 가장 흔히 사용하는 방식은 엑셀, 워드, 파워포인트와 같은 MS 오피스 제품을 활용하는 방식이다. 과거 SAS, SPSS, 미니탭 등 외산 통계 패키지로 데이터를 분석하고 유용한 모형 등을 찾아낸 후에 이를 커뮤니케이션하기 위해 MS 오피스 제품을 통해 커뮤니케이션을 하기도 했다. 하지만, 각각은 별개의 시스템으로 분리되어 있어 일일이 사람 손이 가는 번거로움이 많았다. 이를 해결하기 위한 방법은 하나의 도구 혹은 언어로 모든 작업을 처리하는 것이다. 2\n우선 엑셀은 tidyverse로 대체가 되고, 워드는 R 마크다운을 거쳐 쿼토(Quarto), 파워포인트도 R 마크다운(xaringan 등)에서 진화한 reveal.js 기반 쿼토 슬라이드가 빠르게 자리를 잡아가고 있다.\n\n\n\n\n\n그림 21.9: 오피스 기반 커뮤니케이션 현재 상태점검\n\n\n데이터 과학을 커뮤니케이션하는 방식은 다양한 방식이 존재하지만 직장상사뿐만 아니라 집단지성을 넘어 AI를 적극 도입하여 데이터 분석 역량을 고도화하는 데 동료 개발자 및 협업하시는 분들과 커뮤니케이션뿐만 아니라 불특정 다수를 대상으로 한 인터넷 공개와 공유를 통해 새로운 관계를 맺어가는 것도 그 중요성을 더해가고 있다.\n\n동료 개발자나 협업하시는 분: .qmd 파일\n직장상사\n\nPDF 파일: \\(\\LaTeX\\), pandoc, quarto\n\n파워포인트 슬라이드덱: reveal.js 기반 quarto\n\n대시보드: flexdashboard를 지나 quarto\n\n\n\n일반 공개\n\n웹사이트: distill을 지나 quarto\n\n블로그: blogdown을 지나 quarto\n\n책: bookdown을 지나 quarto\n\n\n\n프로그래밍\n\n과학기술: 줄리아(julia)\n웹 데이터 시각화: observable JS, plotly, leaflet 등\n데이터 과학: R\n기계학습과 딥러닝: 파이썬\n데이터베이스: SQL\n자동화: 유닉스 쉘\n버전제어와 협업: git, github, gitlab, bitbucket 등\n\n\n\n데이터 과학 커뮤니케이션에서 동료 개발자나 협업하는 분들과 의사소통할 때는 .qmd 파일을 사용한다. 직장상사와 커뮤니케이션할 때는 PDF 파일, 파워포인트 슬라이드덱, 대시보드 등 다양한 방식을 활용하는데, PDF 파일은 \\(\\LaTeX\\), pandoc, quarto를 사용하고, 파워포인트 슬라이드덱은 reveal.js 기반 quarto를, 대시보드는 flexdashboard를 지나 quarto 대시보드를 사용한다. 일반 공개를 위해서는 웹사이트, 블로그, 책 등의 형태로 공유하는데, 웹사이트는 distill을 지나 quarto 웹사이트를, 블로그는 blogdown을 지나 quarto 블로그를, 책은 bookdown을 지나 quarto 책(book)을 활용한다.\n\n\n\n\n\n그림 21.10: 이해당사자 쿼토 커뮤니케이션",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>시각화</span>"
    ]
  },
  {
    "objectID": "tool-viz.html#footnotes",
    "href": "tool-viz.html#footnotes",
    "title": "21  시각화",
    "section": "",
    "text": "출처: How Florence Nightingale Changed Data Visualization Forever - The celebrated nurse improved public health through her groundbreaking use of graphic storytelling↩︎\nMeghan Hall (June 15, 2021), “Extending R Markdown”, RStudio: R in Sports Analytics,↩︎",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>시각화</span>"
    ]
  },
  {
    "objectID": "tool-gpt.html",
    "href": "tool-gpt.html",
    "title": "\n22  챗GPT 코딩\n",
    "section": "",
    "text": "22.1 코딩 패러다임\n1950년대부터 본격적으로 컴퓨터가 도입되면서 CLI를 필두로 다양한 사용자 인터페이스(User Interface)가 적용되었다. 스티브 잡스의 애플사는 매킨토시 GUI에 이어 아이폰 모바일 인터페이스를 일반화시켰다면, 최근 챗GPT는 언어 사용자 인터페이스(LUI)를 통해 각 분야에 혁신을 예고하고 있다. 챗GPT[1] 데이터 과학도 사용자 관점에서 보자. 기존 R, 파이썬, SQL, 엑셀 등 데이터 과학 구문을 머리속에 암기하고 있거나 구글이나 네이버를 통해 중요 키워드를 통해 문제를 해결해야 했었다. 하지만, 이제 챗GPT가 자연어를 이해하기 때문에 데이터 전처리, 통계 작업, 데이터 분석, 시각화, 모형개발 등 데이터 과학 전반에 변화는 필연적이다. [2], [3], wickham2019welcome?\n챗GPT[4]는 인터넷에서 방대한 양의 데이터를 학습하여 이를 정말 잘 압축한 하나의 저장소로 이해할 수 있다. 따라서, 압축을 풀게 되면 정확히 원본을 복원할 수 있는 부분도 있지만, 그렇지 못한 부분도 당연히 있게 된다. 챗GPT를 “웹의 흐릿한 JPEG”[5]으로 비유하고 있다. JPEG 기술 자체는 손실 압축기술로 무손실 압축기술로 대표적인 PNG와 대비된다. 흐릿한 이미지가 선명하지 않거나 정확하지 않은 것처럼 챗GPT도 항상 완벽한 답변을 제공하거나 모든 질문을 제대로 이해하는 것은 아니다. 하지만 사용자와의 대화를 기반으로 끊임없이 학습하고 개선하고 있다. 더 많은 사람들이 챗GPT를 사용할수록 사람의 언어를 더 잘 이해하고 반응할 수 있게 개발된 기술이다.\n코딩 세계는 끊임없이 진화한다. 전통적인 코딩 방식에서 시작하여 기계학습 코딩, 최근 챗GPT와 같은 대규모 언어모델(LLM)을 활용한 프롬프트 기반 코딩에 이르기까지, 새로운 패러다임이 계속해서 등장하고 있다.\n전통적인 코딩은 프로그래머가 직접 모든 코드를 작성하고 논리를 구축하는 방식으로 개발자 전문성과 경험에 크게 의존하며, 코드 품질과 효율성은 프로그래머의 역량에 달려 있다. 하지만 이러한 접근법은 시간이 많이 소요되고 반복적인 작업이 많아 생산성이 낮아 통합개발환경(IDE), 소프트웨어 공학, 소프트웨어 아키텍처, 디자인 패턴 등의 방법론과 도구가 발전하며 개선되어 왔고 현재 주류를 형성하고 있다.\n기계학습 코딩은 데이터와 알고리즘을 활용하여 컴퓨터 스스로 코드를 생성하고 최적화할 수 있게 하는 방식으로 전통적인 코딩 방식으로 풀 수 없는 복잡한 문제를 해결할 수 있다. 기계학습 코딩 방식은 대규모 데이터셋과 복잡한 알고리즘을 필요로 하지만, 일단 학습이 완료되면 개발자가 작성한 코드보다 월등한 성능을 보인다. 다만 기계학습 모형의 성능은 데이터의 질과 양, 알고리즘의 복잡성, 컴퓨팅 자원에 따라 크게 좌우된다.\n최근 챗GPT와 같은 대규모 언어모델을 활용한 프롬프트 기반 코딩이 주목받고 있다. 자연어 프롬프트를 입력하면 언어모델이 이해하고 관련 코드를 생성한다는 점에서 일종의 생성형 모형으로 볼 수 있다. 프롬프트 기반 코딩은 전통적인 코딩 방식과 기계학습 코딩의 장점을 결합했다. 프로그래머는 자연어로 의도를 표현할 수 있고, LLM은 의도를 파악하여 코드로 변환해주어 생산성과 효율성을 높일 수 있어 매우 유용하다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>챗GPT 코딩</span>"
    ]
  },
  {
    "objectID": "tool-gpt.html#코딩-패러다임",
    "href": "tool-gpt.html#코딩-패러다임",
    "title": "\n22  챗GPT 코딩\n",
    "section": "",
    "text": "그림 22.2: 코딩 패러다임 변화",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>챗GPT 코딩</span>"
    ]
  },
  {
    "objectID": "tool-gpt.html#프롬프트-공학",
    "href": "tool-gpt.html#프롬프트-공학",
    "title": "\n22  챗GPT 코딩\n",
    "section": "\n22.2 프롬프트 공학",
    "text": "22.2 프롬프트 공학\n \n프롬프트 공학(Prompt Engineering)은 챗GPT와 같은 AI 언어 모형으로부터 구체적이고 정확하며 관련성 있는 응답을 도출하기 위해 프롬프트(Prompt, 지시명령어)를 설계하고 개선하는 과정이다. 프롬프트의 품질이 GPT 모형 출력결과에 큰 영향을 미칠 수 있기 때문에 이 작업은 매우 중요하다. 프롬프트 엔지니어링의 목표는 사용자와 AI 모델 사이의 커뮤니케이션을 최적화하여 AI 시스템의 유용성과 효율성을 향상시키는 것이다.\n프롬프트 엔지니어링은 반복적인 작업과정으로 AI의 응답에 따라 프롬프트를 조정하고 개선해야 할 수도 있다는 점을 항상 염두에 두고, 다음 프롬프트 구성요소를 프롬프트에 녹여 제작할 경우 AI 언어 모델이 목표에 부합하는 정확하고 관련성 있는 구체적인 답변을 효과적으로 생성할 수 있다.\n프롬프트 공학을 코딩에 적용할 때의 장점은 자연어로 의도를 표현할 수 있어 코딩 입문자도 쉽게 접근할 수 있고, 수많은 프로그래밍 언어에 대한 장벽이 크게 낮아진 것을 들 수 있다. 또한 기존 코드를 수정하거나 새로운 코드를 작성할 때 생산성을 높일 수 있고, 언어모델이 제공하는 광범위한 지식을 활용할 수 있어 코드의 품질이 향상되었다. 하지만, 프롬프트를 잘 설계하기 위해서는 프롬프트 공학에 대한 지식과 경험이 필요하기 때문에 프롬프트 공학에 대한 깊은 이해가 필요하고, 언어모델 출력 결과가 완벽하지 않기 때문에 필연적으로 전문 개발자의 검토와 수정이 필요하며 저작권을 비롯한 보안 및 윤리 문제를 풀어야 하는 숙제가 남아있다.\n프롬프트 공학을 코딩에 적용하는 절차는 전통적인 코딩 절차와 별반 다르지 않다. 첫째로 목표를 설정하고, 작성하려는 코드의 정확한 기능과 요구사항을 명시하여 코드가 어떤 입력을 받고, 어떤 출력을 내야 하는지 파악한다. 둘째로 목표를 자연어 프롬프트에 담아낸다. 가능한 상세하고 구체적인 프롬프트를 작성하고 사례, 제약조건 등도 포함한다. 셋째로 프롬프트를 대규모 언어모델에 입력한다. 언어모형이 프롬프트를 이해하고 관련 코드를 생성하는 본 작업을 진행하고 필요한 경우 추가 프롬프트를 언어모형에 피드백을 제공한다. 넷째로 생성된 코드를 주의 깊게 검토하고 오류, 비효율성, 스타일 이슈 등을 반영하여 코드품질을 향상시킨다. 다섯째로 다양한 입력 값으로 코드를 실행하고 출력을 검증하며 발견된 오류는 디버깅하여 수정한다. 마지막으로 테스트 결과를 바탕으로 프롬프트를 개선할 점을 파악하고, 프롬프트를 수정하여 새로운 코드를 생성한 후 결과를 비교하는 과정을 반복한다.\n\n\n\n\n\n그림 22.3: 챗GPT 코딩 작업흐름\n\n\n챗GPT 코딩 구성요소는 프롬프트와 언어모형으로 구성되어 있다고 보면 된다. 자연어 프롬프트를 통해 생성하고자 하는 코드의 목적과 요구사항을 명시하여 언어모형에 전달하면 언어모형은 프롬프트를 이해하고 관련 코드를 생성한다. 몇년전 영어가 가장 뜨거운 프로그래밍 언어라는 주장이 허언은 아닌 것이다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>챗GPT 코딩</span>"
    ]
  },
  {
    "objectID": "tool-gpt.html#챗gui-인터페이스",
    "href": "tool-gpt.html#챗gui-인터페이스",
    "title": "\n22  챗GPT 코딩\n",
    "section": "\n22.3 챗GUI 인터페이스",
    "text": "22.3 챗GUI 인터페이스\n \nOpenAI 챗GPT, 구글 Gemini, 앤트로픽 클로드 등을 사용하여 코드를 생성시킬 수 있다. 챗GPT를 비롯한 대규모 언어모형은 API도 제공하고 있어서 챗 인터페이스가 불편한 경우 API를 통해 개발하고 있는 프로그램에 직접 적용할 수도 있다. 먼저 최근 인기를 얻고 있는 챗GPT 대항마로 떠오르고 있는 앤트로픽 클로드(claude)를 통해 코드를 생성하는 방법을 알아보자.\n\n\n프롬프트\n출력결과\n\n\n\n초등학교 5학년을 대상으로 프로그래밍을 처음 시작하려 합니다. 정말 읽기 쉽고 코딩에 흥미를 가질 수 있는 R 코드를 작성해줘\n\n\n\n\n\n그림 22.4: 앤트로픽 클로드 챗 인터페이스\n\n\n\n\n초등학생을 대상으로 R 프로그래밍을 가르치려면 단계적이고 재미있는 접근이 필요합니다. 여기 간단한 예제 코드를 제공하겠습니다:\n# 안녕하세요! R은 재미있는 프로그래밍 언어예요.\n# 이번 시간에는 숫자와 문자를 다뤄볼 거예요.\n\n# 숫자 계산하기\n숫자1 &lt;- 5\n숫자2 &lt;- 3\n합계 &lt;- 숫자1 + 숫자2\nprint(합계) # 8이 출력됩니다.\n\n# 문자 다루기\n이름 &lt;- \"철수\"\nprint(paste(\"안녕하세요,\", 이름, \"님!\")) # \"안녕하세요, 철수 님!\"이 출력됩니다.\n\n# 조건문 사용하기\n나이 &lt;- 10\nif (나이 &gt;= 20) {\n  print(\"어른이군요!\")\n} else {\n  print(\"아직 어려운 친구예요.\")\n} # \"아직 어려운 친구예요.\"가 출력됩니다.\n\n# 반복문 사용하기\nfor (숫자 in 1:5) { # 1부터 5까지 반복합니다.\n  print(paste(숫자, \"번째 반복입니다.\"))\n}\n\n# 벡터(여러 개의 값을 한번에 다루기)\n과일 &lt;- c(\"사과\", \"바나나\", \"딸기\")\nprint(과일) # \"사과\" \"바나나\" \"딸기\"가 출력됩니다.\n이 코드에서는 다음과 같은 개념들을 다룹니다:\n\n변수 할당과 연산\n문자열 다루기\n조건문 (if-else)\n반복문 (for 루프)\n벡터 (여러 값을 한번에 저장)\n\n각 개념마다 친숙한 예시를 사용하여 학생들이 이해하기 쉽도록 하였습니다. 또한 print() 함수를 활용하여 결과를 바로 확인할 수 있게 하였습니다.\n이 코드를 바탕으로 학생들과 함께 살펴보며 R 프로그래밍의 기초를 익힐 수 있을 것입니다. 궁금한 점이 더 있다면 언제든 물어보세요!",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>챗GPT 코딩</span>"
    ]
  },
  {
    "objectID": "tool-gpt.html#프로그래밍",
    "href": "tool-gpt.html#프로그래밍",
    "title": "\n22  챗GPT 코딩\n",
    "section": "\n22.4 프로그래밍",
    "text": "22.4 프로그래밍\n \nGUI 챗팅 인터페이스를 사용하면 직관적으로 R 코드를 생성할 수 있지만, 자동화를 할 수 없다는 문제와 함께 재사용도 매번 복사하여 붙여넣기를 해야한다는 문제가 있다. 대신 프로그래밍을 통해 R 혹은 파이썬 코드 생성작업을 자동화하고 재사용할 수 있도록 한걸음 더 들어가 보자.\n자연어로 시각화하는 프로그램 작성을 본격적으로 들어가기 전에 기본적인 설정을 다음과 같이 한다. 다양한 언어로 OpenAI API를 활용하는 것이 가능하지만 지면관계상 파이썬으로 OpenAI GPT 모델을 사용해 사용자 질문에 자동으로 답하는 스크립트를 작성한다. 헬로월드(“Hello World!”)를 통해 기본 설정이 정상동작하는지 확인한다. \n\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI(\n    api_key=os.getenv('OPENAI_API_KEY'),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"R 언어가 뭔지 간략하게 설명해줘\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(chat_completion.choices[0].message.content)\n\n\n\n모듈 가져오기:\n\n\nos는 운영 체제와 상호작용하고, 환경 변수에 접근하는 데 사용된다.\n\nopenai는 OpenAI의 파이썬 클라이언트 라이브러리로, GPT 모델을 사용하는 데 필요하다.\n\ndotenv는 .env 파일에서 환경 변수를 로드하는 데 사용된다.\n\n\n\n환경 변수 로드:\n\n\nload_dotenv()는 프로젝트 루트의 .env 파일로부터 환경 변수를 로드한다. .env 파일에 저장된 OPENAI_API_KEY 환경변수를 가져온다.\n\n\n\nOpenAI 클라이언트 초기화:\n\n\nOpenAI를 사용해 API 클라이언트를 생성한다.\n\napi_key=os.getenv('OPENAI_API_KEY')는 환경 변수에서 OPENAI_API_KEY를 가져와 클라이언트를 인증한다.\n\n\n\n채팅 완성 생성:\n\n\nclient.chat.completions.create는 OpenAI의 채팅 완성 API를 사용해 채팅 대화를 생성한다.\n\nmessages는 사용자의 입력 메시지를 담고 있다. 이 경우 “R 언어가 뭔지 간략하게 설명해줘”라는 질문이 포함되어 있다.\n\nmodel=\"gpt-3.5-turbo\"는 사용할 GPT 모델을 지정한다.\n\n\n\n결과 출력:\n\n\nprint(chat_completion.choices[0].message.content)는 생성된 채팅 대화에서 첫 번째 선택 항목의 메시지 내용을 출력한다. 이는 GPT 모델이 생성한 답변을 보여준다.\n\n\n\nR 언어는 통계 분석 및 데이터 시각화를 위한 프로그래밍 언어로, 특히 데이터 분석 및 머신러닝 분야에서 널리 사용됩니다. R은 무료로 사용할 수 있고 다양한 통계 및 그래픽 라이브러리를 제공하여 데이터 분석가들이 데이터를 쉽게 다룰 수 있도록 도와줍니다. R 언어는 벡터화된 연산을 통해 효율적으로 대용량 데이터를 다룰 수 있는 장점을 가지고 있습니다.\nOpenAI GPT-3.5 모델이 정상적으로 응답하는 것을 확인한 후, 다음 단계로 진행한다. 챗GPT 인터페이스로 작성된 프롬프트를 바탕으로, \"\"\" 안에 빈도수를 계산할 romeo.txt 파일 본문과 자연어로 텍스트에서 빈도수가 높은 단어를 화면에 출력하는 R 코드를 만들도록 하면, API 프로그래밍으로 만들어진 파이썬 스크립트에 적용된 인증 과정을 거친 후 R 코드를 생성하게 된다.\n\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI(\n    api_key=os.getenv('OPENAI_API_KEY'),\n)\n\ncode_message = \"\"\"\nYou are an expert in R language. \nThe following text have been provided to you. \nPlease convert my query into an appropriate r codes.\n\nBut soft what light through yonder window breaks\nIt is the east and Juliet is the sun\nArise fair sun and kill the envious moon\nWho is already sick and pale with grief\n\n\nLet's write an r program to count the most frequent words on the screen.\n\"\"\"\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": code_message,\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(chat_completion.choices[0].message.content)\n\n출력결과가 상당히 만족스럽다. 영어로 되어있지만 주석과 함께 R 코드가 잘 생성된 것을 확인할 수 있다. 생성된 결과물 중 R 코드에 대한 부분만 추출하여 실제 실행시켜도 오류 없이 정상 실행됨이 확인된다.\n\n# Create a vector of the provided text\ntext &lt;- c(\"But soft what light through yonder window breaks\",\n          \"It is the east and Juliet is the sun\",\n          \"Arise fair sun and kill the envious moon\",\n          \"Who is already sick and pale with grief\")\n\n# Convert the text to lowercase\ntext &lt;- tolower(text)\n\n# Flatten the text into a single string\ntext &lt;- paste(text, collapse = \" \")\n\n# Split the text into individual words\nwords &lt;- strsplit(text, \"\\\\W+\")\n\n# Convert the list of words into a single vector\nwords &lt;- unlist(words)\n\n# Count the frequency of each word\nword_counts &lt;- table(words)\n\n# Sort the words by frequency\nword_counts &lt;- sort(word_counts, decreasing = TRUE)\n\n# Print the 5 most frequent words\nhead(word_counts, 5)\n#&gt; words\n#&gt;     and      is     the     sun already \n#&gt;       3       3       3       2       1\n\nOpenAI GPT-3.5 LLM 모형은 범용 언어모형이지만, 제시된 텍스트에서 가장 빈도수가 많은 단어를 추출하는 작업을 수행하는 R 코드를 생성했다. 생성된 R 코드는 주어진 텍스트를 소문자로 변환하고, 단어로 분리한 후, 각 단어 빈도수를 계산하여 가장 빈도수가 높은 단어 5개를 출력하는 코드를 제시했다.\nOpenAI GPT-3.5 모델이 GPT-4 모형과 비교하여 성능이 떨어지는 것으로 알려져 있지만, 단순한 텍스트 처리와 빈도수 계산에 대한 R 코드를 생성하는 데는 충분히 사용할 수 있을 것으로 보인다.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>챗GPT 코딩</span>"
    ]
  },
  {
    "objectID": "tool-gpt.html#심슨의-역설",
    "href": "tool-gpt.html#심슨의-역설",
    "title": "\n22  챗GPT 코딩\n",
    "section": "\n22.5 심슨의 역설",
    "text": "22.5 심슨의 역설\n\n심슨의 역설(Simpson’s Paradox)은 데이터를 취합할 때 의미 있는 변수를 생략하면 변수 간에 관찰되는 추세가 역전되는 데이터 현상이다. 부리의 길이와 깊이는 전체적으로 음의 상관관계를 보이지만, 종을 포함하면 이러한 추세가 반전되어 종 내에서는 부리 길이와 부리 깊이 사이에 양의 상관관계가 뚜렷하게 나타난다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n다음 코드에서 group 인수를 species로 설정하게 되면, 기존 부리 길이와 깊이 관계가 음의 상관에서 양의 상관관계로 변화함을 확인하게 되어 패러독스가 발생함을 확인할 수 있다.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>챗GPT 코딩</span>"
    ]
  },
  {
    "objectID": "tool-gpt.html#프로젝트",
    "href": "tool-gpt.html#프로젝트",
    "title": "\n22  챗GPT 코딩\n",
    "section": "프로젝트",
    "text": "프로젝트\n\n21 장 나이팅게일 데이터를 이용하여 크림전쟁 사망자수를 위생조치를 취함으로써 줄인 시각화를 생성한다. 21 장 R 코드를 OpenAI 챗GPT, 구글 제미나이, 앤스로픽 클로드를 이용하여 파이썬 코드를 생성하고 파이썬 코드를 실행하여 정상 동작여부, 코드 품질, 시각화 결과물을 21 장 R 코드를 이용하여 생성한 결과물과 비교한다.\n\n\n\n\n[1] \nA. Vaswani 기타, “Attention is all you need”, Advances in neural information processing systems, vol 30, 2017.\n\n\n[2] \nH. Wickham, M. Çetinkaya-Rundel, 와/과 G. Grolemund, R for data science. \" O’Reilly Media, Inc.\", 2023.\n\n\n[3] \nR. Gozalo-Brizuela 와/과 E. C. Garrido-Merchan, “ChatGPT is not all you need. A State of the Art Review of large Generative AI models”, arXiv preprint arXiv:2301.04655, 2023.\n\n\n[4] \nT. Wu 기타, “A brief overview of ChatGPT: The history, status quo and potential future development”, IEEE/CAA Journal of Automatica Sinica, vol 10, 호 5, pp 1122–1136, 2023.\n\n\n[5] \nT. Chiang, “ChatGPT is a blurry JPEG of the web”, The New Yorker, vol 9, p 2023, 2023.",
    "crumbs": [
      "**3부** 분야별 코딩",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>챗GPT 코딩</span>"
    ]
  },
  {
    "objectID": "ide.html",
    "href": "ide.html",
    "title": "23  IDE 선택과 발전",
    "section": "",
    "text": "23.1 IDE 탄생과 발전\n소프트웨어 개발 세계는 끊임없이 진화하는 도구 생태계다. 중심에는 개발자 생산성을 극대화하는 통합 개발 환경(IDE, Integrated Development Environment)이 있다. IDE는 단순한 코드 편집기를 넘어, 컴파일, 디버깅, 버전 관리 등 개발 전 과정을 하나의 창에서 처리하는 강력한 작업 공간이다.\n모든 개발자에게 완벽한 단 하나의 IDE는 없다. 프로젝트 종류, 주 사용 언어, 개발 스타일에 따라 최적 도구는 달라진다. 본문에서는 IDE 역사부터 최신 AI 트렌드까지 살펴보고, 자신에게 맞는 IDE 선택과 구성에 필요한 지식을 다룬다.\n그림 23.1 는 오픈소스 소프트웨어(OSS) VS Code 기반 현대 데이터 과학 IDE(Positron, Cursor 등) 아키텍처다. 클라이언트 레이어(UI), Extension Host(확장 프로그램 실행), 커널 레이어(코드 실행), 외부 서비스 연동이 명확히 분리되어 있다.\nIDE 역사는 ’어떻게 하면 개발을 더 편하고 효율적으로 할 수 있을까?’라는 고민의 역사와 같다. 60년이 넘는 시간 동안 IDE는 기술 패러다임의 변화와 함께 진화해왔다.\n그림 23.2 은 1964년부터 2025년까지 IDE 진화를 5개 기술 시대로 구분해 보여준다. 각 시대는 당시 컴퓨팅 환경 특성을 반영한다. 메인프레임 시대에는 시분할 시스템으로 원격 터미널을 통해 대화형 프로그래밍이 가능해졌다. 1964년 다트머스 베이직(Dartmouth BASIC)은 학생들이 터미널에서 직접 코드를 입력하고 결과를 즉시 확인할 수 있는 최초의 대화형 환경을 제공했다.\nPC 시대가 열리면서 IDE는 개인 컴퓨터 위에서 작동하는 독립적인 소프트웨어가 되었다. 1983년 터보 파스칼(Turbo Pascal)은 앤더스 헤일스버그(Anders Hejlsberg)가 개발한 혁명적 제품으로, 초고속 컴파일과 통합 에디터를 $49.99라는 파격적 가격에 제공하며 상업용 IDE를 대중화했고 볼란드(Borland) 전성기였다. 1991년 Visual Basic은 드래그 & 드롭 GUI로 비주얼 프로그래밍 패러다임을 열었고, RAD(Rapid Application Development) 혁명을 일으켰다. 개발자가 폼 디자이너에서 버튼을 배치하고 속성을 설정하면 코드가 자동 생성되는 방식은 당시로서는 놀라운 생산성 향상이었다.\n인터넷 시대에는 오픈소스 IDE가 부상했다. 2001년 이클립스(Eclipse)가 플러그인 아키텍처로 자바(Java) 표준 IDE가 되었고, IBM의 대규모 지원으로 확장 생태계를 구축했다. 같은 해 등장한 인텔리제이(IntelliJ) IDEA는 심층 코드 분석과 리팩토링 혁신으로 “스마트 IDE” 기준을 세웠다. 젯브레인즈(JetBrains)가 내건 “즐거운 개발(Develop with Pleasure)” 슬로건은 단순히 개발자 경험을 개선하겠다는 선언이었고, 이후 Kotlin 언어까지 탄생시키는 혁신의 기반이 되었다.\n클라우드 시대는 개발 환경에 대한 물리적 제약을 허물었다. 2015년 등장한 VS Code는 일렉트론(Electron) 기반 크로스 플랫폼 편집기로 시작해, 모나코 편집기(Monaco Editor) 웹 기술과 30,000개 이상의 확장 프로그램으로 시장을 지배하게 되었다. 2020년 GitHub Codespaces는 브라우저 IDE로 설치 없이 즉시 개발할 수 있는 환경을 제공했으며, 컨테이너 기반으로 개발 환경을 정의하면 클라우드에서 즉시 실행되는 클라우드 네이티브 개발 방식이 확산되었다.\nAI IDE 시대는 2021년 GitHub Copilot 등장으로 본격화되었다. 생성형 코드로 개발자 생산성을 혁신했고, OpenAI Codex 기반 AI 페어 프로그래머는 주석이나 함수명만으로도 전체 함수를 자동 생성했다. 2024-25년에는 포지트론(Positron)이 R/Python 데이터 과학 특화 IDE로, Claude Code가 CLI 기반 자율 에이전트로 등장하며 AI 코딩의 새 지평을 열고 있다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>IDE 선택과 발전</span>"
    ]
  },
  {
    "objectID": "ide.html#sec-ide-history",
    "href": "ide.html#sec-ide-history",
    "title": "23  IDE 선택과 발전",
    "section": "",
    "text": "그림 23.2: IDE 발전사: 60년의 진화",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>IDE 선택과 발전</span>"
    ]
  },
  {
    "objectID": "ide.html#sec-ide-kernel",
    "href": "ide.html#sec-ide-kernel",
    "title": "23  IDE 선택과 발전",
    "section": "23.2 IDE 작동 원리: 커널 아키텍처",
    "text": "23.2 IDE 작동 원리: 커널 아키텍처\n60년 역사를 거치며 진화한 IDE는 어떤 구조로 파이썬, R, Julia, SQL 등 수십 가지 언어를 동시에 지원하는가? 비결은 커널(Kernel) 아키텍처에 있다.\n주피터(Jupyter) 프로젝트에서 시작해 이제는 많은 IDE 표준이 된 커널 아키텍처는 IDE(프론트엔드)와 언어 실행 엔진(백엔드)을 명확히 분리한다. 프론트엔드는 개발자가 코드를 입력하는 UI 부분이다. 주피터 노트북의 코드 셀이나 VS Code의 인터랙티브 창이 여기 해당한다. 프론트엔드 자체는 코드 실행 능력이 없고, 사용자가 입력한 코드를 커널에 전달하는 메신저 역할만 한다.\n실제 코드 실행은 커널이 담당한다. 커널은 별도 독립 프로세스로 백그라운드에서 동작한다. 파이썬 코드를 실행하면 IPython 커널이, R 코드를 실행하면 IRkernel이 작동하며, 프론트엔드로부터 받은 코드를 해당 언어 인터프리터로 실행한다. 프론트엔드와 커널은 ZeroMQ(ZMQ) 고성능 메시징 라이브러리로 통신한다. 메시지 종류와 형식은 주피터 메시징 프로토콜로 표준화되어 있다.\n주요 메시지 4가지가 있다. execute_request로 프론트엔드가 코드 실행을 요청하면, 커널은 실행 과정에서 stream으로 print() 같은 텍스트 출력을 실시간 전송하고, display_data로 그래프, 이미지, 표를 특정 포맷(image/png, text/html)으로 포장해 보낸다. 모든 실행이 끝나면 execute_reply로 완료결과를 전송한다.\n커널 아키텍처의 가장 큰 장점은 확장성이다. 새로운 언어를 지원하려면 주피터 메시징 프로토콜을 따르는 커널만 만들면 된다. IDE 프론트엔드는 수정할 필요가 없다. 주피터 생태계가 수백 개 언어 커널을 가질 수 있고, VS Code가 파이썬 확장 프로그램 하나로 복잡한 데이터 과학 워크플로우를 지원하는 비결이 여기 있다. 언어 서버 프로토콜(LSP, Language Server Protocol)이 언어 ‘분석’ 기능을 분리하고 표준화한 것처럼, 커널 아키텍처는 언어 ‘실행’ 기능을 분리하고 표준화해 놀라운 유연성과 확장성을 제공한다.\n\n\n\n\n\n\n그림 23.3: 커널 아키텍처 - 프론트엔드와 실행 엔진의 분리\n\n\n\n그림 23.3 는 IDE 프론트엔드(사용자 인터페이스)와 커널(언어 실행 엔진) 분리 구조와 ZeroMQ, 주피터 메시징 프로토콜을 통한 통신을 보여준다. 이러한 분리 덕분에 하나의 IDE가 여러 프로그래밍 언어를 지원한다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>IDE 선택과 발전</span>"
    ]
  },
  {
    "objectID": "ide.html#sec-ide-features",
    "href": "ide.html#sec-ide-features",
    "title": "23  IDE 선택과 발전",
    "section": "23.3 IDE 핵심 기능",
    "text": "23.3 IDE 핵심 기능\nIDE 구조를 이해했으니 이제 실전에서 사용하는 핵심 기능을 살펴보자. 현대 IDE는 단순한 텍스트 편집기를 넘어 개발 전 과정을 통합한 작업 환경으로, 마우스 클릭보다는 키보드 단축키로 모든 기능을 빠르게 호출하는 것이 효율적인 워크플로우의 핵심이다.\n\n\n\n\n\n\n그림 23.4: IDE 핵심 기능: 키보드 중심 워크플로우\n\n\n\n그림 23.4 는 현대 오픈소스 IDE의 화면 구성과 키보드 중심 워크플로우를 보여준다. 화면은 크게 4개 영역으로 나뉜다. 왼쪽 탐색기는 프로젝트 파일 트리를 표시하며, 중앙 편집기는 코드를 작성하는 메인 작업 공간이다. 하단 터미널은 명령어를 실행하고, 최하단 상태 바는 Git 브랜치, 오류 개수, 언어 버전을 한눈에 보여준다.\n편집기 영역을 자세히 보면 AI가 개발에 깊이 통합된 모습이 드러난다. 9번 라인에서 df.dropna() 다음에 .reset_index(drop=True) 메서드가 회색으로 표시되는데, 이것은 GitHub Copilot의 AI 자동완성 제안이다. 개발자가 코드 문맥을 읽고 다음에 필요할 로직을 미리 제안하는 것이다. Tab 키를 누르면 제안을 수락하고, 무시하려면 계속 타이핑하면 된다. 더 나아가 Cmd+I 단축키로 AI 편집 모드를 열어 “파일 없음 에러 처리 추가”처럼 자연어로 의도를 설명하면, AI가 직접 코드를 생성하거나 수정한다. 이것은 코딩 패러다임의 근본적 변화다.\nCmd+Shift+P 명령 팔레트는 IDE 숨겨진 보물이다. 마우스로 메뉴를 탐색하지 않고도 2,000개 이상의 명령에 즉시 접근한다. “Python: Select Interpreter”를 입력하면 파이썬 버전을 바꾸고, “Terminal: Create New Terminal”로 터미널을 추가하며, “Git: Commit”으로 커밋한다. 모든 작업이 키보드에서 손을 떼지 않고 진행된다.\n기존 개발 워크플로우는 에러가 발생하면 브라우저로 전환해 StackOverflow를 검색하고, 코드를 복사해 붙여넣은 뒤 디버깅하는 방식이었다. 현대 AI 네이티브 워크플로우는 다르다. Cmd+I로 “에러 설명해줘”라고 물으면 AI가 즉시 답하고, Tab으로 상용구 코드(boilerplate)를 자동 완성하며, IDE를 벗어나지 않고 플로우를 유지한다. 컨텍스트 전환이 사라지면서 생산성이 2-3배 향상되는 이유다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>IDE 선택과 발전</span>"
    ]
  },
  {
    "objectID": "ide.html#sec-ide-evolution",
    "href": "ide.html#sec-ide-evolution",
    "title": "23  IDE 선택과 발전",
    "section": "23.4 IDE 진화",
    "text": "23.4 IDE 진화\n최근 IDE는 두 가지 방향으로 진화하고 있다. 하나는 특정 개발 영역에 깊이 파고들어 전문화되는 것이고, 다른 하나는 AI로 개발 방식 자체를 근본적으로 바꾸는 것이다. 데이터 과학 분야는 두 트렌드가 모두 적용되는 대표적인 영역으로, 전문화된 기능과 AI 통합이 동시에 진행되고 있다.\n\n23.4.1 데이터 과학 IDE\n데이터 과학 IDE는 일반 프로그래밍 IDE와 다른 특화된 기능을 갖춘다. 코드 실행 결과로 생성된 플롯이나 그래프를 IDE 내에서 직접 확인하고 상호작용하는 데이터 시각화 도구가 핵심이다. 변수 탐색기는 현재 실행 환경의 데이터프레임, 변수, 객체 등을 실시간으로 보여주며, 데이터 구조와 값을 쉽게 파악하게 한다. 주피터(Jupyter) 노트북 통합은 코드, 텍스트, 시각화를 하나의 문서로 엮어 재현가능한 분석 보고서를 만든다. 패키지 및 환경 관리 기능은 uv나 conda로 프로젝트별로 격리된 환경을 구성하고, pandas나 scikit-learn 같은 데이터 과학 라이브러리를 손쉽게 설치하고 업데이트한다.\n\n\n23.4.2 AI 에이전트 개발 환경\n최근 LLM 활용 AI 에이전트 개발이 급부상하며 특화된 개발 환경이 등장했다. 에이전트 동작 흐름을 시각적으로 설계하고, 여러 에이전트 간 상호작용을 테스트하며, 복잡한 프롬프트 체인을 관리하는 도구들이다.\nLangSmith는 LangChain 기반 에이전트 동작을 추적하고 디버깅하는 플랫폼이다. 에이전트가 어떤 도구를 호출했고, 어떤 프롬프트를 사용했으며, 왜 특정 결정을 내렸는지 시각화한다. AutoGen Studio는 여러 에이전트 팀을 손쉽게 만들고 테스트하는 시각 인터페이스를 제공한다. 코드 작성 에이전트, 리뷰 에이전트, 테스트 에이전트가 협업하는 과정을 그래프로 보여준다. Flowise와 Langflow는 코드 없이 드래그 & 드롭으로 LLM 애플리케이션을 만드는 시각 IDE 역할을 한다. 프롬프트 노드, LLM 노드, 데이터 처리 노드를 연결해 복잡한 AI 워크플로우를 구축한다.\n\n\n23.4.3 AI 통합\nAI 코딩 도구는 지난 3년간 급격히 진화했다. 처음에는 개발자가 별도 웹사이트를 방문해야 했던 챗 인터페이스에서 시작해, IDE 내부로 통합된 확장 프로그램으로 발전했고, 이제는 프로젝트 전체를 자율적으로 편집하는 에이전트 단계에 이르렀다. 진화 과정의 핵심은 AI가 개발자 작업 흐름 속으로 점점 더 깊숙이 들어와 컨텍스트 전환을 최소화하는 것이다.\n\n\n\n\n\n\n그림 23.5: AI 코딩 진화 - 챗 인터페이스에서 IDE 통합까지\n\n\n\n그림 23.5 는 AI 코딩 도구의 4단계 진화 과정을 보여준다. 1단계(2022-2023)는 챗GPT(ChatGPT) 웹사이트를 별도로 방문하는 방식이었다. IDE에서 코드를 작성하다가 질문이 생기면 브라우저로 전환하고, 챗GPT에 질문하고, 답변을 복사해서 IDE로 돌아와 붙여넣었다. 컨텍스트 전환이 빈번하고 코드 컨텍스트를 수동으로 복사해야 했으며, 플로우 상태가 깨지면서 생산성이 저하되었다.\n2단계(2023)는 GitHub Copilot 같은 IDE 확장이 등장하면서 시작되었다. AI가 IDE 내부로 들어와 자동완성을 제공했다. 코드 문맥을 자동으로 인식하고, Tab으로 제안을 수락하며, IDE를 벗어나지 않게 되었다. 하지만 자동완성만 지원할 뿐 대화형 설명이나 복잡한 요청은 어려웠다.\n3단계(2024)는 커서(Cursor)와 윈드서프(Windsurf) 같은 AI 네이티브 IDE가 깊은 통합을 실현했다. Cmd+K로 인라인 편집 모드를 열고, 자연어로 의도를 설명하면 AI가 코드를 직접 수정한다. 대화하며 결과를 개선할 수 있다. 하지만 여전히 IDE 내부로 제한되고, 파일 단위 작업이며, 자율성은 제한적이었다.\n4단계(2024-2025)는 클로드 코드(Claude Code)와 구글 앤티그래비티(Google Antigravity) 같은 자율 에이전트다. 2025년 11월 구글이 제미나이(Gemini) 3와 함께 발표한 앤티그래비티는 “에이전트 우선” 개발 플랫폼이다. Editor View(AI 기반 IDE)와 에이전트 관리자(Agent Manager)라는 두 가지 모드를 제공한다. Agent Manager에서는 에이전트를 생성하고, 오케스트레이션하며, 비동기로 작업하는 과정을 관찰한다. 에이전트는 작업 목록, 구현 계획, 스크린샷, 브라우저 녹화 같은 아티팩트(Artifacts)를 생성해 로직을 검증 가능하게 한다. 제미나이 3 Pro뿐 아니라 클로드 소넷(Claude Sonnet) 4.5, OpenAI 모델도 지원하며 무료 공개 프리뷰로 제공된다.\nCLI 명령으로 작업을 요청하면 프로젝트 전체를 이해하고, 다중 파일을 자율적으로 편집하며, Git 커밋까지 자동화한다. 프로젝트 전체 컨텍스트를 유지하고, 스스로 의사결정하며, 개발자의 플로우를 완전히 유지한다. 생산성이 10-100배 향상되는 혁명적 단계다.\n진화의 핵심은 컨텍스트 전환 최소화다. AI가 개발자의 작업 공간 밖에 있을 때는 지속적으로 전환해야 했지만, IDE 안으로 들어오고, 더 깊이 통합되고, 마침내 자율 에이전트가 되면서 개발자는 플로우 상태를 유지한 채로 AI의 도움을 받게 되었다.\n💡 생각해볼 점\n60년 IDE 역사가 보여준 패턴은 명확하다. 컴퓨팅 패러다임이 바뀔 때마다 새로운 IDE가 등장했고, 이전 도구는 레거시가 되었다. 메인프레임 시대 시분할 시스템, PC 시대 터보 파스칼, 인터넷 시대 이클립스, 클라우드 시대의 VS Code, 지금 AI 시대 커서와 구글 앤티그래비티. 중요한 것은 “현재 패러다임에 최적화된 도구를 빠르게 습득”하는 능력이다. 2025년 12월 현재 AI 통합 수준이 IDE 선택의 가장 중요한 기준이 되었다.\n키보드 중심 워크플로우는 생산성의 핵심이다. Cmd+P, Cmd+Shift+P, Cmd+I, F5, F9, Ctrl+\\ - 6개 단축키만 암기해도 마우스 의존도가 80% 줄어든다. 명령 팔레트(Cmd+Shift+P)로 2,000개 이상의 기능에 즉시 접근하고, AI 편집(Cmd+I)으로 자연어를 코드로 바꾸며, 디버거(F5, F9)로 버그를 추적한다. 근육 기억이 형성되면 플로우 상태가 유지되고, 생산성이 2-3배 향상된다.\nAI 도구 선택은 통합 수준으로 판단한다. 아직 별도 챗GPT 웹사이트에서 복사-붙여넣기 사용한다면 1단계다. GitHub Copilot으로 자동완성을 받는다면 2단계, 커서로 대화하며 코드를 수정한다면 3단계다. 구글 앤티그래비티나 클로드 코드처럼 에이전트 관리자에서 에이전트가 프로젝트 전체를 자율적으로 편집하고 아티팩트(Artifacts)를 생성하며 검증하는 4단계가 현재 최전선이다. 단계가 높을수록 컨텍스트 전환은 줄고 작업플로우는 유지된다.\n다음 장에서는 데이터 과학에 특화된 포지트론(Positron) IDE를 설치하고, R과 파이썬 환경을 구성하며, 키보드 단축키를 실전에서 활용하는 방법을 다룬다. 이론을 넘어 실제로 “작업플로우에서 AI와 함께 코딩하는” 환경을 직접 구축한다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>IDE 선택과 발전</span>"
    ]
  },
  {
    "objectID": "ide_positron.html",
    "href": "ide_positron.html",
    "title": "24  포지트론",
    "section": "",
    "text": "24.1 포지트론 철학\n데이터 과학 세계는 빠르게 변화하고 있으며, 이제 R과 파이썬(Python) 강력한 언어를 함께 사용하는 것이 표준이 되었다. R은 통계 분석과 시각화에 독보적 강점을 가지며, 파이썬은 머신러닝, 범용 프로그래밍, 시스템 통합에 널리 쓰인다. 하지만 오랫동안 데이터 과학자들은 두 언어를 동시에 편안하게 사용할 완벽한 통합 개발 환경(IDE)을 찾기 어려웠다. R 사용자에게는 RStudio가, 파이썬 사용자에게는 다양한 선택지가 있었지만, 두 세계를 자연스럽게 넘나들기에는 항상 아쉬움이 남았다.\n이런 문제의식에서 출발한 것이 포짓(Posit, 과거 RStudio)사가 개발한 차세대 데이터 과학 IDE 포지트론(포지트론)이다. 포지트론은 “하나의 팀, 두 개의 언어” 현실을 받아들이고, R과 파이썬을 모두 일급 시민1으로 대우하는 현대 ‘다언어(polyglot)’ 개발 환경을 지향한다.\n포지트론 핵심 철학은 RStudio 데이터 과학 전문성과 Visual Studio Code(VS Code) 현대적 개발 경험 결합이다. 포지트론은 VS Code 오픈소스 버전인 ‘Code OSS’ 기반으로 구축되었다. VS Code의 빠르고 유연한 인터페이스, 방대한 확장 기능 생태계, 강력한 코드 편집 기능을 가져오면서, RStudio가 수십 년간 쌓은 데이터 과학 워크플로우 이해를 녹여냈다.\n그림 24.1 는 포지트론 전체 화면 구성을 보여준다. 왼쪽 탐색기는 프로젝트 파일 트리를, 중앙 편집기는 R 코드를, 오른쪽 패널은 변수 탐색기와 플롯 창을, 하단은 R 콘솔을 표시한다. 콘솔 상단 드롭다운으로 R과 파이썬을 즉시 전환할 수 있다. 편집기에서는 AI가 회색으로 다음 코드를 제안하며, 변수 탐색기는 메모리에는 데이터프레임을 포함한 모든 객체를 실시간으로 보여준다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>포지트론</span>"
    ]
  },
  {
    "objectID": "ide_positron.html#sec-positron-philosophy",
    "href": "ide_positron.html#sec-positron-philosophy",
    "title": "24  포지트론",
    "section": "",
    "text": "그림 24.1: 포지트론 화면 구성 - R/Python 동시 지원 UI\n\n\n\n\n\n\n\n\n\n노트왜 RStudio를 두고 포지트론을 만들었나?\n\n\n\n포짓 답변은 명확하다: “RStudio는 계속된다.”\n포지트론 개발은 RStudio 대체가 아니다. RStudio와 포지트론은 서로 다른 목표와 사용자를 가진다. RStudio는 R 언어에 깊이 집중하는 데이터 분석가와 통계학자를 위한 최고의 R 개발 환경으로 계속 발전하고 유지된다. 반면 포지트론은 R과 파이썬을 함께 사용하는 다언어 데이터 과학 팀과 개발자를 위한 새로운 선택지다.\n포지트론은 ‘R 전용’ RStudio의 성공적 틀을 넘어, ’R과 파이썬 모두’를 필요로 하는 현대 데이터 과학의 새로운 요구에 부응하기 위한 포짓의 전략적 확장이자 AI 시대 경쟁에서 밀릴 수 없다는 전략적 노림수다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>포지트론</span>"
    ]
  },
  {
    "objectID": "ide_positron.html#sec-positron-ai",
    "href": "ide_positron.html#sec-positron-ai",
    "title": "24  포지트론",
    "section": "\n24.2 AI 시대 포지트론",
    "text": "24.2 AI 시대 포지트론\n포지트론 가장 큰 혁신은 단순 다언어 지원을 넘어, AI 기능을 데이터 과학 워크플로우에 깊이 통합한 점이다. 포지트론 AI 어시스턴트는 일반 코딩 도우미와 근본적으로 다르다. 현재 실행 중인 R/파이썬 세션 내부 상태(메모리 데이터, 변수, 플롯 등)를 직접 파악하고 상호작용하기 때문이다.\n탐색적 데이터 분석(EDA) 단계를 예로 들어보자. “penguins 데이터셋에서 종(species)별로 몸무게(body_mass_g) 분포를 박스플롯으로 그려줘”라고 자연어로 요청하면, AI는 현재 메모리에 있는 penguins 데이터프레임 구조를 이해하고 즉시 ggplot2나 matplotlib 코드를 생성해 실행 결과를 플롯 창에 보여준다. 탐색적 데이터 탐색 단계가 몇 분에서 몇 초로 단축된다.\n데이터 전처리(data wrangling)도 마찬가지다. “결측치가 있는 행을 제거하고, ‘bill_length_mm’와 ’bill_depth_mm’ 열만 선택해줘”라는 요청을 dplyr이나 pandas 코드로 즉시 변환한다. 개발자는 파이프 연산자 문법이나 메서드 체이닝(method chaining)을 기억하는 데 에너지를 쏟지 않고, 데이터 분석에 대한 큰 그림과 로직에 집중할 수 있다.\n더 나아가 AI는 코드뿐 아니라 통계 모델 결과까지 해석한다. “방금 실행한 선형 회귀 모델의 \\(R^2\\) 값은 무엇을 의미하지?” 또는 “복잡한 purrr 코드를 단계별로 설명해줘” 같은 질문에, AI는 통계학적 배경 지식과 함께 깊이 있는 답변을 제공한다. 데이터 과학자가 더 나은 통찰(insight)을 얻도록 돕는 지능형 파트너 역할도 한다.\n\n\n\n\n\n\n힌트IDE 선택 가이드\n\n\n\n표 24.1 비교 정보를 바탕으로 선택은 명확하다. R만 사용하는 통계학자라면 R 패키지 개발, Shiny 앱 배포, R Markdown 프로파일링 같은 고급 기능이 완벽히 통합된 RStudio가 가장 안정적이고 편리하다. 웹 개발, 시스템 프로그래밍 등 범용 목적이라면 수많은 확장 기능을 갖춘 VS Code가 최고 유연성을 제공한다. 하지만 R과 파이썬을 함께 사용하며 최신 AI 기능을 적극 활용하고 싶다면, 포지트론은 두 언어를 매끄럽게 오가며 AI 지원을 받을 수 있는 현재 가장 진보적 환경이다.\n\n\n\n\n\n\n\n\n특성\nRStudio\nVS Code\n포지트론\n\n\n\n주력 언어\nR\n범용 (모든 언어)\nR & Python\n\n\n주요 사용자\nR 데이터 분석가, 통계학자\n모든 종류의 개발자\n다언어 데이터 과학자\n\n\n설정\n거의 불필요 (R 최적화)\n높은 유연성 (직접 구성)\n낮은 설정 (R/Python 최적화)\n\n\n장점\nR 생태계 완벽 통합\n최고의 유연성과 확장성\nR/Python 동시 작업 및 AI 통합\n\n\n단점\nPython 지원 제한적\n데이터 과학 초기 설정 복잡\n일부 고급 기능 아직 개발 중\n\n\n\n\n\n\n\n표 24.1: IDE 비교: RStudio vs VS Code vs 포지트론",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>포지트론</span>"
    ]
  },
  {
    "objectID": "ide_positron.html#sec-positron-install",
    "href": "ide_positron.html#sec-positron-install",
    "title": "24  포지트론",
    "section": "\n24.3 포지트론 설치",
    "text": "24.3 포지트론 설치\n포지트론을 설치하기 전에 시스템이 최소 요구사항을 충족하는지 확인한다. 윈도우(Windows) 10 이상, 맥OS(macOS) 11 이상, 리눅스(Linux)(우분투(Ubuntu) 20.04+)를 지원한다. 메모리는 최소 4GB지만 8GB 이상을 권장한다. 디스크 여유 공간은 500MB 이상 필요하다. R 버전은 4.0 이상, 파이썬(Python) 버전은 3.8 이상이 필요하며, 각각 4.3+와 3.11+를 권장한다.\n포지트론은 포짓 공식 웹사이트에서 무료로 다운로드할 수 있다. https://positron.posit.co에 접속해 운영체제에 맞는 설치 파일을 받는다. 맥OS는 .dmg 파일을 열고 Applications 폴더로 드래그한다. 윈도우는 .exe 설치 파일 실행 후 기본 설정으로 진행한다. 리눅스는 .deb 또는 .rpm 패키지를 설치한다.\n첫 실행 시 초기 설정 마법사가 나타난다. R과 파이썬 인터프리터를 자동으로 감지하며, 없다면 설치를 안내한다. 인터프리터 경로가 자동으로 감지되지 않으면 수동으로 지정할 수 있다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>포지트론</span>"
    ]
  },
  {
    "objectID": "ide_positron.html#sec-positron-features",
    "href": "ide_positron.html#sec-positron-features",
    "title": "24  포지트론",
    "section": "\n24.4 주요 기능 활용 사례",
    "text": "24.4 주요 기능 활용 사례\n포지트론 핵심 기능을 실제 데이터 과학 워크플로우에서 어떻게 활용하는지 살펴보자. 다언어 콘솔 전환부터 변수 탐색기, AI 어시스턴트까지 세 가지 핵심 기능을 통해 포지트론이 제공하는 생산성 향상을 직접 경험할 수 있다.\n\n24.4.1 다언어 콘솔 전환\n포지트론 핵심 강점은 R과 파이썬을 즉시 전환하며 작업하는 것이다. 화면 하단 콘솔 영역 오른쪽 상단에 언어 선택 드롭다운이 있다. 여기서 “R” 또는 “Python”을 선택하면 즉시 해당 언어의 REPL(Read-Eval-Print Loop) 환경으로 전환된다.\n데이터 과학에서 가장 흔한 사례를 살펴보자. R 콘솔에서 ggplot2로 시각화를 그린다. 파이썬 콘솔로 전환해 scikit-learn으로 머신러닝 모델을 훈련한다. 다시 R로 전환해 통계 모델을 검증한다. 세션 전환 없이 모든 작업이 동일 IDE 내에서 이뤄진다.\n\n24.4.2 변수 탐색기\n화면 오른쪽 사이드바에 변수 탐색기가 있다. 현재 실행 중인 R/파이썬 세션 모든 변수, 데이터프레임, 리스트를 실시간으로 보여준다. 데이터프레임 이름을 클릭하면 테이블 뷰어가 열린다. 데이터 정렬, 필터링, 열 타입 확인이 가능하다. 대용량 데이터(수백만 행)도 가상화 기술로 빠르게 탐색할 수 있다.\n\n24.4.3 AI 어시스턴트 사용법\n포지트론 AI 어시스턴트는 단순한 코드 자동완성을 넘어 세션 상태를 이해하는 지능형 도우미다. 현재 메모리에 로드된 데이터, 설치된 패키지, 변수 구조를 직접 파악하기 때문에 자연어 요청만으로도 즉시 실행 가능한 코드를 생성한다.\n\n\n\n\n\n그림 24.2: 포지트론 AI 워크플로우 - 세션 상태 인식 기반 코드 생성\n\n\n그림 24.2 는 포지트론 AI 어시스턴트 작동 방식을 보여준다. 사용자가 자연어로 요청하면, AI는 현재 R/파이썬 세션의 메모리 데이터, 설치된 패키지, 변수 구조를 직접 파악할 수 있다. 파악된 정보를 바탕으로 즉시 실행 가능한 코드를 생성하고, 실행 결과를 플롯 창에 시각화한다. 일반 AI 챗봇과 달리 세션 상태를 알기 때문에 “penguins가 무엇인지” 따로 설명할 필요가 없다.\n자연어 요청 예시를 살펴보자. “mtcars 데이터셋에서 mpg와 wt의 상관관계를 산점도로 그려줘”라고 입력하면, AI는 다음 R 코드를 자동생성한다.\nlibrary(ggplot2)\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles per Gallon\")\nAI는 현재 메모리에 mtcars가 로드되어 있는지 확인하고, ggplot2 패키지가 설치되었는지 검증한 후 코드를 생성한다. 바로 실행하면 플롯 창에 결과가 나타난다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>포지트론</span>"
    ]
  },
  {
    "objectID": "ide_positron.html#sec-positron-migration",
    "href": "ide_positron.html#sec-positron-migration",
    "title": "24  포지트론",
    "section": "\n24.5 RStudio에서 포지트론 전환",
    "text": "24.5 RStudio에서 포지트론 전환\nRStudio를 오랫동안 사용해온 데이터 과학자라면 포지트론 전환이 생각보다 자연스럽다. 두 IDE 모두 데이터 과학자를 염두에 두고 포짓에서 개발했기 때문에 화면 레이아웃 구성이 유사하고, 익숙한 단축키도 대부분 그대로 작동한다. RStudio에서 쌓아온 작업 습관을 버리지 않고도 포지트론 다언어 지원과 AI 통합 기능을 즉시 무리없이 활용할 수 있다.\n\n\n\n\n\n그림 24.3: 포지트론 vs RStudio 레이아웃 비교\n\n\n그림 24.3 는 RStudio와 포지트론의 화면 레이아웃 차이를 보여준다. RStudio는 4개 패널(Source, Console, Environment, Files)로 R 중심 워크플로우에 최적화되어 있다. 포지트론은 VS Code 기반으로 왼쪽 탐색기, 중앙 편집기, 오른쪽 변수/플롯 패널, 하단 콘솔 구조를 가지며, 콘솔에서 R과 파이썬을 드롭다운으로 즉시 전환할 수 있다.\n포지트론 설치 후 기존 R 프로젝트 폴더를 연다. .Rproj 파일이 있다면 자동으로 프로젝트로 인식한다. R 콘솔에서 renv::restore()로 패키지 복원한다. 단축키 설정을 “RStudio” 프리셋으로 변경할 수 있다(설정 → Keybindings → “RStudio”).\n현재(2025년 베타) 포지트론은 일부 기능이 아직 구현되지 않았다. R Markdown 프로파일링(메모리, 성능 분석), Shiny 앱 배포 버튼(shinyapps.io, Posit Connect), R 패키지 개발 전용 도구(devtools 통합), .Rproj 설정 일부 옵션이 개발중에 있다.\n포지트론은 아직 베타 버전이며 RStudio 일부 기능(예: R Markdown 프로파일링, 간편한 앱 배포)이 아직 완전히 구현되지 않았다는 한계가 있다. 하지만 R과 파이썬이 공존하는 현대 데이터 과학 흐름을 가장 잘 반영하고, AI를 개발 워크플로우 핵심으로 가져왔다는 점에서 미래가 기대된다.\n포지트론은 단순히 새로운 도구가 아니다. 포짓 팀이 생각하는 미래 데이터 과학 작업 환경의 구체적 실험이다. AI와 함께 더 빠르고 깊이 있게 데이터 문제를 해결하고 싶은 데이터 과학자라면, 포지트론은 여정을 함께할 흥미로운 파트너가 될 수 있다.\n💡 생각해볼 점\n포지트론은 AI가 촉발시킨 데이터 과학 워크플로우의 근본적 변화를 반영한다. 과거에는 “R 또는 파이썬” 중 하나를 선택해야 했지만, 이제는 “R과 파이썬 모두”를 사용하는 것이 표준이 되었다. 포지트론은 이러한 변화를 받아들이고, 두 언어를 동등하게 지원하며, AI로 생산성을 극대화하는 첫 번째 IDE 중 하나다.\n전환은 점진적으로 시작한다. 작은 EDA 프로젝트에서 포지트론을 시도해보자. R로 통계 분석을 하다가 파이썬 scikit-learn이 필요하면 콘솔을 전환한다. AI 어시스턴트에게 자연어로 요청하고, 생성된 코드를 즉시 실행한다. 이러한 과정을 몇 번 반복하면 포지트론 없이는 일하기 힘들어진다.\n다음 장에서는 포지트론의 진정한 힘을 발휘하는 확장 프로그램(extension) 설치와 설정을 다룬다. 포지트론 기본 설치만으로는 부족하며, R과 파이썬 언어 서버, 쿼토, 린터 등 필수 확장 프로그램을 설치해야 완전한 개발 환경이 갖춰진다. 이론을 넘어 실제로 작동하는 포지트론 환경을 구축한다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>포지트론</span>"
    ]
  },
  {
    "objectID": "ide_positron.html#footnotes",
    "href": "ide_positron.html#footnotes",
    "title": "24  포지트론",
    "section": "",
    "text": "일급 시민(first-class citizen)은 시스템에서 완전한 지원을 받는 대상을 의미한다. 포지트론에서 R과 파이썬 모두 동등하게 완전한 기능을 제공받으며, 어느 한쪽이 부차적으로 취급되지 않는다.↩︎",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>포지트론</span>"
    ]
  },
  {
    "objectID": "ide_extension.html",
    "href": "ide_extension.html",
    "title": "25  IDE 확장 프로그램",
    "section": "",
    "text": "25.1 IDE 확장 프로그램 아키텍처\n현대 통합 개발 환경(IDE)의 가장 큰 힘은 ’확장성’에 있다. 어떤 IDE도 세상의 모든 프로그래밍 언어, 프레임워크, 도구를 기본 지원할 수는 없다. 그렇게 시도한다면 IDE는 극도로 무거워지고 복잡해져 사용할 수 없게 된다.\n문제 해결의 핵심 아이디어가 바로 확장 프로그램(Extensions)이다. IDE는 핵심 기능(텍스트 편집, UI)만 제공하고, 추가 기능들은 사용자가 필요에 따라 ’레고 블록’처럼 조립해 사용한다. 이번 장에서 IDE 확장 프로그램이 왜 필요하며, 어떤 아키텍처로 안정적으로 구현되는지 살펴본다.\n확장 프로그램이 IDE를 무너뜨리지 않으면서도 강력한 기능을 제공하려면 어떻게 해야 할까? 현대 IDE는 이러한 문제를 해결하기 위해 공통된 아키텍처 패턴을 따른다. 핵심은 ‘격리’다. 확장 프로그램을 메인 프로세스에서 분리하고, 언어 기능을 표준 프로토콜로 분리하며, 기여 지점을 명확히 정의한다. 비주얼 스튜디오 Code(VS Code)는 세 가지 원칙을 가장 성공적으로 구현한 대표 사례로, 30,000개 이상의 확장 프로그램을 지원하면서도 가볍고 안정적인 성능을 유지한다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>IDE 확장 프로그램</span>"
    ]
  },
  {
    "objectID": "ide_extension.html#sec-extension-architecture",
    "href": "ide_extension.html#sec-extension-architecture",
    "title": "25  IDE 확장 프로그램",
    "section": "",
    "text": "25.1.1 확장 프로그램 격리\nVS Code는 확장 프로그램을 IDE 메인 프로세스가 아닌, ‘확장 호스트(Extension Host)’ 라고 불리는 별도 독립 프로세스에서 실행한다. 이러한 격기 구조의 가장 큰 목적은 안정성이다. 특정 확장 프로그램이 과도한 메모리를 사용하거나 오류를 일으켜 멈추더라도, IDE 메인 프로세스(UI, 텍스트 편집 등)는 전혀 영향을 받지 않는다. 사용자는 문제가 된 확장 프로그램을 비활성화하거나 재시작할 수 있으며, 작업 내용은 안전하게 보존된다.\n\n\n\n\n\n\n그림 25.1: VS Code 안정성 비결 - 확장 프로그램 격리 아키텍처\n\n\n\n그림 25.1 는 VS Code가 메인 프로세스와 확장 호스트를 분리해 안정성을 확보하는 방식을 보여준다. 확장 프로그램이 오류를 일으켜도 메인 프로세스는 안전하게 작동한다.\n\n\n25.1.2 LSP: 언어 기능 분리\n과거에는 C++ 언어 ‘코드 자동 완성’ 기능을 만들려면, VS Code용, Sublime Text용, 아톰용 코드를 각각 따로 만들어야 했다. 언어 개발자(m)와 IDE 개발자(n) 모두에게 m x n의 비효율적 개발 부담을 주었다.\n마이크로소프트가 개발해 표준으로 제안한 언어 서버 프로토콜(Language Server Protocol, LSP)은 이 문제를 해결했다. LSP는 언어 관련 기능(코드 분석, 자동 완성, 오류 검출 등)을 ‘언어 서버’ 라는 독립 프로세스로 분리한다. IDE(클라이언트)는 표준화된 JSON-RPC 메시지로 언어 서버와 통신하며 정보를 주고받는다.\n효과는 획기적이었다. C++ 언어 개발자는 ‘C++ 언어 서버’ 하나만 만들면 된다. LSP를 지원하는 모든 IDE(VS Code, 이클립스, 주피터 등)는 별도 노력 없이 C++ 언어의 모든 지능형 기능을 사용할 수 있다. 개발 부담을 m + n으로 획기적으로 줄였고, 새로운 언어가 빠르게 다양한 IDE에 채택될 수 있는 길을 열었다.\n\n\n\n\n\n\n그림 25.2: 언어 서버 프로토콜 (LSP): m × n 문제의 해결\n\n\n\n그림 25.2 는 LSP가 m × n 문제를 m + n으로 해결한 과정을 보여준다. 과거에는 각 언어와 IDE마다 별도 통합 작업이 필요했지만, LSP로 언어 서버 하나만 만들면 모든 LSP 지원 IDE에서 사용할 수 있게 되었다.\n\n\n25.1.3 기능 기여 모델\n확장 프로그램은 package.json이라는 Manifest(설명서) 파일로 IDE 메뉴, 아이콘, 명령어 목록에 자신의 기능을 추가한다. 매니페스트 파일에는 확장 프로그램 이름, 버전, 설명 등 기본 정보와 함께, ‘어떤 조건에서 활성화될지’(Activation Events), ‘IDE 어느 부분에 어떤 기능을 추가할지’(Contribution Points)가 명시되어 있다.\nVS Code는 확장 프로그램이 기여할 수 있는 ’슬롯’을 미리 정의해 두었다. 예를 들어, contributes.commands는 새로운 명령어를, contributes.menus는 메뉴 항목을, contributes.views는 사이드바에 새로운 UI 창을 추가한다. IDE는 시작될 때 package.json 파일들을 읽어들여 전체 UI와 기능을 구성한다.\n\n\n\n\n\n\n경고아톰 편집기 교훈: 자유도 vs 안정성\n\n\n\n아톰(Atom) 편집기는 VS Code의 성공적 아키텍처를 이해하는 데 좋은 대조 사례다. 아톰 역시 일렉트론(Electron) 기반으로 만들어졌고 ‘핵킹 가능한(hackable)’ 편집기를 표방하며 엄청난 유연성을 제공했지만, VS Code와 결정적 아키텍처 차이가 있었다.\n아톰은 확장 프로그램을 격리된 ’확장 호스트’에서 실행하지 않았다. 모든 확장 프로그램은 편집기 UI와 동일한 렌더러 프로세스에서 실행되었다. 이러한 구조는 편집기 거의 모든 부분을 수정할 수 있는 극강의 자유도를 제공했지만, 치명적 단점을 낳았다. 확장 하나가 오작동하거나 느려지면 편집기 전체가 버벅거리거나 멈추는 현상이 잦았다. 결국 아톰은 성능 저하 문제로 사용자를 잃었다.\n아톰 사례는 확장 프로그램 아키텍처에서 ‘격리’ 가 왜 중요한지 명확히 보여준다. VS Code가 확장 호스트로 안정성과 성능을 모두 잡을 수 있었던 것은 아톰 편집기 단점을 반면교사로 삼았기 때문이다. 소프트웨어 설계에서 한 가지를 얻으면 다른 것을 포기해야 하는 절충은 피할 수 없다. 중요한 것은 사용자가 진정으로 원하는 것이 무엇인지 정확히 파악하는 것이다. 개발자들은 “자유롭지만 느린” 편집기보다 “약간 제한적이지만 빠르고 안정적인” 편집기를 선택했다.\n(참고: 아톰 프로젝트는 2022년 12월 공식 개발 중단)\n\n\n\n\n25.1.4 VS Code 아키텍처 영향\n최근 포지트론(Positron), 커서(Cursor) 등 많은 IDE가 VS Code 오픈소스 코어인 ‘Code - OSS’ 기반으로 만들어지고 있다. VS Code 확장 프로그램 아키텍처가 그만큼 뛰어나고, 현대 IDE 개발의 ’성공 공식’이 되었다는 의미다.\n새로운 IDE가 VS Code 기반으로 만들어진다는 것은, 수만 개의 기존 VS Code 확장 프로그램을 거의 그대로 사용할 수 있다는 뜻이다. 새로운 IDE는 처음부터 모든 언어 지원, 테마, 도구를 만들 필요 없이, 이미 검증된 거대한 생태계를 즉시 활용해 개발을 시작한다. VS Code ‘확장 호스트’ 같은 멀티 프로세스 아키텍처는 안정성과 성능이 이미 검증되었다. 새로운 IDE는 복잡한 기반을 직접 설계하는 대신, 자신만의 핵심적 특화 기능 개발에만 집중한다.\n전 세계 수많은 개발자가 이미 VS 코드 UI와 사용 방식에 익숙하다는 점도 중요하다. VS 코드 기반으로 만들어진 IDE는 사용자가 별도 학습 없이도 쉽고 빠르게 적응할 수 있다. VS 코드 확장 프로그램 아키텍처는 단순 기술적 성공을 넘어, 다른 IDE가 활용할 수 있는 강력한 플랫폼이자 생태계를 창조했다. 많은 현대 IDE가 ’바퀴를 재발명’하는 대신 VS Code라는 거인의 어깨 위에 올라타는 이유다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>IDE 확장 프로그램</span>"
    ]
  },
  {
    "objectID": "ide_extension.html#sec-positron-extensions",
    "href": "ide_extension.html#sec-positron-extensions",
    "title": "25  IDE 확장 프로그램",
    "section": "25.2 포지트론 필수 확장 프로그램",
    "text": "25.2 포지트론 필수 확장 프로그램\n포지트론이 데이터 과학 IDE로서 진정한 힘을 발휘하려면 핵심 확장 프로그램들을 설치해야 한다. 포지트론은 VS Code 기반이므로 VS Code 확장 생태계를 그대로 활용하면서, 데이터 과학에 특화된 확장들을 추가로 제공한다.\n\n\n\n\n\n\n노트Positron vs VS Code: 내장 기능 차이\n\n\n\nPositron IDE는 R (Ark Kernel), Python, Jupyter 기능이 이미 내장되어 있다. VS Code 사용자는 이 확장들을 별도로 설치해야 하지만, Positron 사용자는 설치 없이도 즉시 사용할 수 있다. 아래 목록에서 “* Positron 내장” 표시가 있는 확장은 Positron에서 이미 제공되므로, VS Code에서만 추가 설치가 필요하다.\n\n\n\n\n\n\n\n\n그림 25.3: 포지트론 데이터 과학 확장 프로그램 생태계\n\n\n\n그림 25.3 는 포지트론에서 실제로 사용하는 15종 핵심 확장 프로그램을 3개 그룹으로 보여준다. 1) 엔진 & 앱 개발: R, Python, Shiny Publisher, Quarto, Thunder Client로 Shiny/Streamlit 앱 개발과 배포를 지원한다. 2) 인터랙티브 데이터 분석: SandDance, Data Wrangler, SQLTools, Geo Data Viewer, Rainbow CSV로 다양한 데이터 탐색과 변환을 수행한다. 3) AI, 협업 & 운영: Jupyter, GitHub Copilot, Live Share, Remote-SSH, Docker로 노트북, AI 코딩, 원격 협업, 컨테이너화를 통합한다.\n\n25.2.1 언어 지원: R & Python\nR (Positron 내장, VS Code: REditorSupport.r)는 R 언어 지원의 핵심이다. Positron은 자체 개발한 Ark Kernel로 R을 실행하며, VS Code보다 훨씬 빠르고 안정적이다. R 언어 서버(LSP)를 통해 코드 자동완성, 함수 시그니처 도움말, 정의로 이동 기능을 제공한다. R 콘솔에서 install.packages(c(\"lintr\", \"styler\"))로 린터와 포맷터를 설치하면, 코드 스타일 문제를 실시간으로 감지하고 tidyverse 스타일로 자동 정리한다.\nPython (Positron 내장, VS Code: ms-python.python)은 파이썬 개발 환경을 완성한다. Pylance 언어 서버로 빠른 타입 체크와 자동완성을 지원하고, venv, conda 환경을 자동 감지한다. 파이썬 디버거는 중단점, 변수 검사, 단계별 실행을 제공한다.\n\n\n25.2.2 앱 배포: Shiny Publisher\nShiny & Publisher (posit.publisher)는 Shiny 앱과 Streamlit 앱을 Posit Connect 또는 shinyapps.io에 원클릭으로 배포한다. 로컬에서 개발한 인터랙티브 대시보드를 프로덕션 환경에 즉시 올리고, 팀원들과 공유하며, 버전 관리와 롤백을 지원한다.\n설치는 “Posit Publisher”를 검색해 설치한다. Shiny 앱 디렉토리에서 Cmd+Shift+P → “Publish Content”를 실행하면 배포 대상(Connect, shinyapps.io)을 선택하고, 계정 인증 후 자동으로 배포된다. 배포 히스토리를 추적하고 이전 버전으로 롤백할 수 있다.\n\n\n25.2.3 API 테스트: Thunder Client\nThunder Client (rangav.vscode-thunder-client)는 Postman처럼 API 엔드포인트를 테스트하는 도구다. R Plumber나 Python FastAPI로 만든 모델 API를 IDE 내에서 바로 테스트한다. GET/POST 요청을 보내고, JSON 응답을 확인하며, 환경 변수를 관리한다.\n설치는 “Thunder Client”를 검색해 설치한다. 좌측 사이드바에 번개 아이콘이 생기며, 클릭하면 REST 클라이언트가 열린다. “New Request” → URL 입력 → Send로 API를 테스트한다. 요청을 컬렉션으로 저장해 재사용한다.\n\n\n25.2.4 문서 작성: Quarto\nQuarto (quarto.quarto)는 재현가능한 데이터 분석 보고서 작성의 핵심이다. .qmd 파일에서 R과 파이썬 코드 청크를 실행하고, 결과를 즉시 미리보기로 확인한다. Render 버튼 클릭으로 HTML, PDF, Word, 슬라이드로 변환한다. 실시간 미리보기는 수정 사항을 즉시 반영하며, 코드, 텍스트, 시각화, 표를 하나의 문서로 엮어 논문, 기술 보고서, 블로그 포스트를 작성한다.\n설치는 확장 마켓플레이스에서 “Quarto”를 검색해 설치한다. .qmd 파일을 열면 자동으로 활성화되며, Cmd+Shift+K (macOS) 또는 Ctrl+Shift+K (Windows/Linux)로 렌더링한다. YAML 헤더에서 출력 형식(format: html, format: pdf)을 지정하고, R/파이썬 코드 청크는 ```{r} 또는 ```{python}로 시작한다.\n\n\n25.2.5 시각적 탐색: SandDance\nSandDance (msrvida.vscode-sanddance)는 마이크로소프트 리서치가 개발한 혁신적인 3D/2D 데이터 시각화 도구다. 코드 없이 데이터를 드래그 앤 드롭으로 탐색하며, 산점도, 바차트, 밀도 플롯을 실시간으로 전환해 데이터 패턴을 발견한다. 수십만 행 데이터도 WebGL로 부드럽게 렌더링한다.\n설치는 “SandDance”를 검색해 설치한다. CSV나 JSON 파일을 우클릭 → “View in SandDance”를 선택하면 인터랙티브 시각화 창이 열린다. 축을 변경하고, 색상을 매핑하며, 필터를 적용해 탐색적 데이터 분석(EDA)을 몇 초 만에 수행한다.\n\n\n25.2.6 데이터베이스: SQLTools\nSQLTools (mtxr.sqltools)는 PostgreSQL, MySQL, SQLite 등 다양한 데이터베이스에 연결해 쿼리를 실행하고 결과를 시각화한다. IDE 내에서 데이터베이스 탐색, 테이블 스키마 확인, SQL 자동완성, 쿼리 히스토리 관리를 수행한다.\n설치는 “SQLTools”를 검색해 설치한다. 좌측 사이드바에 데이터베이스 아이콘이 생기며, “Add New Connection”으로 DB 연결 정보를 입력한다. SQL 파일을 열고 Cmd+E Cmd+E로 쿼리를 실행하면 결과가 테이블로 표시된다.\n\n\n25.2.7 지도 시각화: Geo Data Viewer\nGeo Data Viewer (randomfractals.geo-data-viewer)는 GeoJSON, Shapefile, KML 같은 지리 데이터를 즉시 지도로 시각화한다. 공간 데이터 분석 프로젝트에서 지도를 코드 없이 확인하고, 레이어를 토글하며, 속성을 검사한다.\n설치는 “Geo Data Viewer”를 검색해 설치한다. .geojson 파일을 열면 자동으로 지도가 표시되며, 줌/팬으로 탐색한다. 속성 테이블을 클릭해 각 지형지물의 메타데이터를 확인한다.\n\n\n25.2.8 데이터 탐색: Rainbow CSV\nRainbow CSV (mechatroner.rainbow-csv)는 CSV 파일을 다루는 필수 도구다. CSV 파일을 열면 각 열을 다른 색으로 표시해 가독성을 높인다. SQL 쿼리로 CSV를 탐색하고(SELECT * FROM this WHERE age &gt; 30), 열 정렬, 필터링, 통계 요약을 제공한다. 큰 CSV 파일(수백만 행)도 가상화 기술로 빠르게 열린다.\n설치는 “Rainbow CSV”를 검색해 설치한다. CSV 파일을 열면 자동으로 각 열에 색상이 적용된다. 열 구분자(쉼표, 탭, 파이프 등)를 자동 감지하며, Cmd+Shift+P → “Rainbow CSV: Query”로 SQL 쿼리 모드를 연다. SELECT name, age FROM this WHERE age &gt; 25 ORDER BY age DESC 같은 쿼리로 데이터를 탐색하고, 결과를 새 창에 표시한다.\n\n\n25.2.9 AI 코딩: GitHub Copilot\nGitHub Copilot (GitHub.copilot)은 포지트론에서도 그대로 작동한다. 주석이나 함수 이름을 입력하면 전체 함수 구현을 제안하고, Tab 키로 수락한다. 데이터 과학 코드에 특화되어 ggplot2, dplyr, pandas, scikit-learn 패턴을 잘 이해한다. “결측치 제거하고 표준화”처럼 자연어 주석을 입력하면 즉시 코드를 생성한다.\n설치는 GitHub 계정으로 로그인하고 Copilot 구독이 필요하다. 확장 마켓플레이스에서 “GitHub Copilot”을 설치하고, GitHub 계정으로 인증한다. 회색 텍스트로 표시되는 제안을 Tab으로 수락하고, Esc로 거부한다. Alt+]로 다음 제안을, Alt+[로 이전 제안을 확인한다.\n\n\n25.2.10 데이터 변환: Data Wrangler\nData Wrangler (ms-toolsai.datawrangler)는 데이터 탐색과 변환을 시각적으로 수행하는 마이크로소프트 확장이다. 변수 탐색기에서 데이터프레임을 클릭하고 “Open in Data Wrangler”를 선택하면 그래픽 인터페이스가 열린다. 필터링, 정렬, 그룹화, 피벗 작업을 드래그 앤 드롭으로 수행하고, 모든 작업은 자동으로 R 또는 파이썬 코드로 변환된다.\n설치는 “Data Wrangler”를 검색해 설치한다. 복잡한 전처리 파이프라인을 GUI로 만들고, “Export Code” 버튼으로 dplyr이나 pandas 코드를 생성한다. 코드를 복사해 스크립트에 붙여넣으면 재현 가능한 전처리 워크플로우가 완성된다.\n\n\n25.2.11 노트북: Jupyter\nJupyter (Positron 내장, VS Code: ms-toolsai.jupyter)는 .ipynb 노트북 파일을 네이티브로 실행한다. 코드 셀, 마크다운 셀, 출력 결과를 하나의 문서로 통합하며, 변수 탐색기와 연동해 노트북 실행 중 생성된 변수를 실시간으로 확인한다. Jupyter 커널(IPython, IRkernel)을 자동 감지하고 전환한다.\nPositron은 Jupyter를 내장하므로 별도 설치가 필요 없다. VS Code 사용자는 “Jupyter”를 검색해 설치한다. .ipynb 파일을 열면 자동으로 활성화되며, Shift+Enter로 셀을 실행한다. 상단 커널 선택 드롭다운에서 Python/R 커널을 전환하고, 노트북 결과를 HTML/PDF로 내보낼 수 있다.\n\n\n25.2.12 실시간 협업: Live Share\nLive Share (ms-vsliveshare.vsliveshare)는 실시간 동시 편집을 가능하게 한다. Google Docs처럼 여러 개발자가 동일 코드를 동시에 편집하고, 디버깅 세션을 공유하며, 터미널까지 함께 사용한다. 원격 페어 프로그래밍과 코드 리뷰에 혁명을 일으킨 도구다.\n설치는 “Live Share”를 검색해 설치한다. Cmd+Shift+P → “Live Share: Start Collaboration Session”으로 세션을 시작하고, 공유 링크를 팀원에게 전송한다. 팀원이 링크로 접속하면 실시간으로 코드를 함께 편집하고, 커서 위치와 선택 영역이 실시간으로 동기화된다.\n\n\n25.2.13 원격 개발: Remote-SSH\nRemote - SSH (ms-vscode-remote.remote-ssh)는 고성능 GPU 서버에 SSH로 연결해 원격 개발 환경을 로컬처럼 사용한다. 딥러닝 모델 훈련처럼 고사양 컴퓨팅이 필요한 작업을 원격 서버에서 수행하면서도, IDE는 로컬 컴퓨터에서 부드럽게 작동한다.\n설치는 “Remote - SSH”를 검색해 설치한다. Cmd+Shift+P → “Remote-SSH: Connect to Host”로 서버 정보를 입력한다. 연결되면 좌측 하단에 “SSH: 서버명”이 표시되며, 모든 파일 탐색, 편집, 터미널 명령이 원격 서버에서 실행된다.\n\n\n25.2.14 컨테이너화: Docker\nDocker (ms-azuretools.vscode-docker)는 분석 환경을 컨테이너로 패키징해 재현성을 보장한다. Dockerfile로 R/Python 패키지, 시스템 라이브러리를 정의하면, 어디서나 동일한 환경을 재현할 수 있다. 컨테이너 빌드, 실행, 디버깅을 IDE 내에서 수행한다.\n설치는 “Docker”를 검색해 설치한다. Dockerfile을 우클릭 → “Build Image”로 이미지를 빌드하고, 좌측 사이드바 Docker 아이콘에서 컨테이너를 관리한다. 컨테이너 내부 터미널을 열어 환경을 테스트하고, Docker Compose로 다중 컨테이너를 오케스트레이션한다.\n\n\n25.2.15 확장 설치 및 관리\n포지트론에서 확장을 설치하는 방법은 간단하다. 좌측 사이드바에서 확장 아이콘(네모 4개)을 클릭하고, 검색창에 확장 이름을 입력한다. “Install” 버튼을 누르면 자동으로 다운로드되고 활성화된다.\n데이터 과학자를 위한 필수 확장 15종 (그룹별 설치 권장):\n1) 엔진 & 앱 개발: 1. R (REditorSupport.r) - R 언어 지원, Shiny 실행 2. Python (ms-python.python) - Python 언어, Streamlit 지원 3. Shiny Publisher (posit.publisher) - 앱 배포 (Connect/ShinyApps) 4. Quarto (quarto.quarto) - 재현가능한 문서, 대시보드 5. Thunder Client (rangav.vscode-thunder-client) - API 테스트\n2) 인터랙티브 데이터 분석: 6. SandDance (msrvida.vscode-sanddance) - 3D/2D 시각화 7. Data Wrangler (ms-toolsai.datawrangler) - GUI 데이터 변환 8. SQLTools (mtxr.sqltools) - DB 연결 & 쿼리 9. Geo Data Viewer (randomfractals.geo-data-viewer) - 지도 시각화 10. Rainbow CSV (mechatroner.rainbow-csv) - CSV 탐색\n3) AI, 협업 & 운영: 11. Jupyter (ms-toolsai.jupyter) - 노트북 실행 12. GitHub Copilot (GitHub.copilot) - AI 코딩 (유료) 13. Live Share (ms-vsliveshare.vsliveshare) - 실시간 협업 14. Remote-SSH (ms-vscode-remote.remote-ssh) - 원격 GPU 서버 15. Docker (ms-azuretools.vscode-docker) - 컨테이너화\n확장 충돌이나 성능 문제가 발생하면 Cmd+Shift+P → “Extensions: Disable All Installed Extensions”으로 모든 확장을 비활성화한 후, 하나씩 다시 활성화하며 문제를 찾는다. 확장 개수가 너무 많으면 IDE 시작 속도가 느려질 수 있으므로, 실제 사용하는 확장만 활성화하고 나머지는 비활성화한다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>IDE 확장 프로그램</span>"
    ]
  },
  {
    "objectID": "ide_extension.html#sec-extension-conclusion",
    "href": "ide_extension.html#sec-extension-conclusion",
    "title": "25  IDE 확장 프로그램",
    "section": "25.3 결론",
    "text": "25.3 결론\n현대 IDE 확장 프로그램 아키텍처는 ‘분리와 표준화’ 두 가지 핵심 원칙에 기반한다. Extension Host로 각 확장 프로그램을 분리해 안정성을 확보하고, LSP라는 표준화된 프로토콜로 언어 기능을 재사용 가능하게 만든다.\n영리한 아키텍처 덕분에 VS Code 같은 현대 IDE는 수많은 언어와 도구를 지원하는 거대한 생태계를 구축하면서도, 가볍고 안정적인 성능을 유지한다. IDE가 더 이상 하나의 회사가 만드는 단일 제품이 아니라, 전 세계 개발자 커뮤니티가 함께 만들어가는 ’플랫폼’이 되었다.\n💡 생각해볼 점\n확장 프로그램 생태계가 IDE의 핵심 경쟁력이 된 시대다. Positron을 선택했다면 R과 Python이 이미 내장되어 있으므로, Shiny Publisher, Quarto, SandDance부터 설치해 즉시 인터랙티브 앱과 문서를 만들어보자. VS Code를 사용한다면 R, Python, Jupyter를 먼저 설치하고 언어 기반을 구축한 뒤 나머지 확장을 추가한다.\n15개 확장을 한 번에 설치하기보다는, 프로젝트 성격에 따라 선택적으로 설치한다. Shiny 대시보드 개발 프로젝트라면 Shiny Publisher와 Thunder Client를, 공간 데이터 분석이라면 Geo Data Viewer와 SQLTools를, 원격 GPU 서버에서 딥러닝을 훈련한다면 Remote-SSH와 Docker를 먼저 설치한다. 필요할 때마다 하나씩 추가하며 자신만의 최적 환경을 구축하는 것이 가장 효율적이다.\n협업 프로젝트에서는 Live Share로 실시간 페어 프로그래밍을 시도해보자. 주니어 개발자가 막힌 부분을 시니어가 원격으로 함께 디버깅하거나, 코드 리뷰를 화면 공유 없이 IDE 내에서 직접 수행하는 경험은 협업 방식을 근본적으로 바꾼다.\n확장 생태계는 계속 진화한다. 2025년 현재 15개 필수 확장이 데이터 과학 워크플로우를 완성하지만, 1년 후에는 새로운 AI 도구, 시각화 확장, 협업 플랫폼이 등장할 것이다. 중요한 것은 “어떤 확장이 있는지” 지속적으로 탐색하고, “내 워크플로우에 필요한지” 빠르게 판단하는 능력이다. 확장 마켓플레이스에서 “data science”, “visualization”, “collaboration” 키워드로 정기적으로 검색하며 새로운 도구를 발견하자.\n다음 장에서는 이론을 넘어 실제로 포지트론 개발 환경을 구축한다. 운영체제별 설치, R/Python 인터프리터 설정, 필수 확장 15종 설치, 키보드 단축키 암기까지 완전한 작업 환경을 처음부터 끝까지 구성한다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>IDE 확장 프로그램</span>"
    ]
  },
  {
    "objectID": "ide_setup.html",
    "href": "ide_setup.html",
    "title": "26  개발 환경 구축",
    "section": "",
    "text": "26.1 Git 설치\nAI로 데이터 과학 문제를 해결하기 위한 첫걸음은 강력한 프로그래밍 언어와 안정적인 개발 환경 구축이다. 본 장에서는 필수적인 버전 관리 도구 Git과 재현 가능한 환경을 위한 도커(Docker), 워크플로우 자동화를 위한 Make를 시작으로, 데이터 과학의 양대 산맥인 R과 파이썬(Python)을 설치하고, 마지막으로 이들을 통합해 사용할 포지트론 IDE 설정 전 과정을 안내한다.\n그림 26.1 는 데이터 과학 개발 환경 구축의 전체 단계를 보여준다. Git, 도커, Make 같은 필수 도구를 먼저 설치하고, R과 파이썬을 설치한 후, 마지막으로 포지트론 IDE를 설치한다.\n본격적인 개발 환경 구축에 앞서, 가장 중요한 버전 관리 시스템 Git을 먼저 설치한다. Git은 코드 변경 이력을 추적하고, 여러 개발자가 협업하게 하며, AI 모델 개발 시 다양한 실험을 관리하는 필수 도구다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>개발 환경 구축</span>"
    ]
  },
  {
    "objectID": "ide_setup.html#sec-setup-git",
    "href": "ide_setup.html#sec-setup-git",
    "title": "26  개발 환경 구축",
    "section": "",
    "text": "26.1.1 Git 설치\n맥OS(macOS)에서는 터미널을 열고 xcode-select --install 명령어를 실행하면 뜨는 팝업창에서 ’설치’를 클릭한다. Xcode Command Line Tools에 Git이 포함되어 있다. 이미 설치되어 있다면 git --version 명령어로 확인한다.\n윈도우(Windows)에서는 git-scm.com 공식 다운로드 페이지에 접속해 최신 버전 설치 파일을 다운로드한다. 설치 프로그램을 실행하고 대부분 옵션을 기본값으로 두고 설치를 진행한다. ’Git Bash’가 함께 설치되어 강력한 명령어 환경을 제공한다.\n리눅스(Linux, 우분투/데비안)에서는 터미널을 열고 다음 명령어를 실행한다:\nsudo apt-get update\nsudo apt-get install git\n\n26.1.2 설치 확인\n터미널에서 다음 명령어를 실행해 Git이 정상 설치되었는지 확인한다:\n$ git --version\ngit version 2.39.2 (Apple Git-143)\n\n26.1.3 최초 설정\nGit 설치 후, 터미널에서 다음 두 명령어를 실행해 사용자 이름과 이메일 주소를 반드시 설정한다. 정보는 코드를 변경하고 저장(커밋)할 때마다 기록된다.\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"youremail@example.com\"\n\"Your Name\"과 \"youremail@example.com\" 부분을 본인의 정보로 바꿔서 입력하세요.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>개발 환경 구축</span>"
    ]
  },
  {
    "objectID": "ide_setup.html#컨테이너-환경-도커-설치",
    "href": "ide_setup.html#컨테이너-환경-도커-설치",
    "title": "26  개발 환경 구축",
    "section": "\n26.2 컨테이너 환경: 도커 설치",
    "text": "26.2 컨테이너 환경: 도커 설치\nAI/머신러닝 프로젝트는 복잡한 라이브러리, 시스템 의존성, 드라이버 버전 등으로 내 컴퓨터에서는 잘 동작하던 코드가 다른 사람의 컴퓨터나 서버에서는 동작하지 않는 ’환경 문제’를 자주 겪는다. 도커는 프로젝트에 필요한 모든 것을 ’컨테이너’라는 격리된 공간에 담아 어디서든 동일한 환경을 완벽하게 복제한다. 재현 가능한 연구와 안정적 배포를 위한 현대 AI 개발 필수 도구다.\n\n\n도커 Desktop 설치:\n\n\n도커 공식 웹사이트에 접속해 자신의 운영체제(맥OS, 윈도우, 리눅스)에 맞는 도커 Desktop을 다운로드하고 설치한다.\n\n\n\n설치 확인: 설치 후 도커 Desktop을 실행한다. 터미널에서 다음 명령어를 실행해 정상 설치를 확인한다:\n$ docker --version\nDocker version 24.0.6, build ed223bc\n\n$ docker run hello-world\nHello from Docker!\nThis message shows that your installation appears to be working correctly.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>개발 환경 구축</span>"
    ]
  },
  {
    "objectID": "ide_setup.html#sec-setup-make",
    "href": "ide_setup.html#sec-setup-make",
    "title": "26  개발 환경 구축",
    "section": "\n26.3 워크플로우 자동화",
    "text": "26.3 워크플로우 자동화\n데이터 과학 프로젝트는 ‘데이터 가져오기 → 전처리 → 모델 학습 → 결과 분석 → 보고서 생성’ 같은 여러 단계 작업 흐름을 가진다. 각 단계를 수동으로 반복 실행하는 것은 비효율적일 뿐 아니라 실수를 유발한다. 1970년대 유닉스 시스템에서 탄생한 make는 이런 반복 작업을 자동화하는 검증된 도구다.\nmake 핵심은 의존성 기반 실행이다. Makefile에 “분석 보고서는 전처리된 데이터에 의존하고, 전처리된 데이터는 원본 데이터에 의존한다”는 관계를 정의하면, make는 변경된 파일만 감지해 필요한 작업만 지능적으로 재실행한다. 예를 들어 원본 데이터가 바뀌면 전처리부터 보고서까지 전부 재생성하지만, 분석 코드만 수정했다면 전처리는 건너뛰고 분석과 보고서만 다시 생성한다. 시간과 컴퓨팅 자원을 크게 절약하는 방식이다.\n\n26.3.1 Make 설치\nmake는 대부분의 운영체제에 이미 설치되어 있거나 개발 도구와 함께 제공된다. 맥OS에서는 Git 설치 시 사용한 Xcode Command Line Tools에 make가 포함되어 있고, 리눅스(우분투/데비안)에서도 기본으로 설치된 경우가 많다. 터미널에서 make --version을 실행해 설치 여부를 확인할 수 있다. 리눅스에서 make가 없다면 sudo apt-get install build-essential 명령어로 컴파일 도구 일체를 설치한다.\n윈도우는 상황이 다르다. Git for Windows를 설치했다면 Git Bash 환경에서 make를 사용할 수 있지만, 네이티브 윈도우 환경에서는 별도 설치가 필요하다. Chocolatey 패키지 매니저가 설치되어 있다면 choco install make 명령어로 간단히 설치할 수 있다.\n\n26.3.2 워크플로우 도구 진화\nmake는 1976년 Stuart Feldman이 Bell Labs에서 C 프로그램 컴파일 자동화를 위해 개발했다. 이후 50년 가까이 소프트웨어 빌드의 표준 도구로 자리잡았지만, 데이터 과학의 부상과 함께 새로운 요구가 생겼다. make의 shell 스크립트 기반 문법은 데이터 분석 파이프라인을 표현하기에 복잡하고, 크로스 플랫폼 지원도 제한적이다. 특히 R이나 Python 생태계와의 통합이 자연스럽지 않다. 이런 한계를 극복하기 위해 언어별, 용도별로 특화된 도구들이 등장했다.\n\n\n\n\n\n그림 26.2: 워크플로우 자동화 도구의 진화\n\n\n지난 50년간 워크플로우 자동화 도구는 세 번의 큰 물결을 거쳤다. 그림 26.2 은 이 진화 과정을 시간순으로 보여준다. 첫 번째 물결은 1976년 make로 시작되었다. C 프로그램 컴파일을 위해 탄생했지만, 곧 소프트웨어 빌드 전반의 표준이 되었다. 두 번째 물결은 2012년 스네이크메이크의 등장이다. 독일 뒤셀도르프 대학의 Johannes Köster가 생물정보학 연구를 위해 개발했는데, 수백 개의 샘플을 처리하는 유전체 분석 파이프라인에서 make의 한계가 명확했기 때문이다. Python 문법을 직접 사용할 수 있고, 클러스터 환경에서 자동 병렬화를 지원하는 스네이크메이크는 곧 생명과학을 넘어 데이터 과학 전반으로 확산되었다. 세 번째 물결은 2017-2020년 사이 동시다발적으로 일어났다. 2017년 Go로 작성된 Task가 YAML 기반의 간결한 문법으로 등장했고, 2018년에는 Rust 기반 Just가 명령어 실행에 특화된 미니멀한 접근으로 개발자들을 끌어모았다. 2020년 R 커뮤니티에서는 Will Landau가 targets를 발표하며 데이터 과학 워크플로우 자동화의 새 장을 열었다. drake 패키지의 후속작인 targets는 R 객체 수준에서 의존성을 추적하고, Quarto와의 완벽한 통합으로 재현가능한 연구의 표준 도구가 되었다.\n\n26.3.3 현대 데이터 과학 도구\nmake가 50년 역사를 자랑하지만, 현대 데이터 과학 프로젝트는 make가 설계되지 않았던 요구사항들을 갖는다. R 데이터 분석 프로젝트는 함수와 데이터 객체 간의 복잡한 의존성을 추적해야 하고, Python 생물정보학 파이프라인은 수천 개의 파일을 클러스터에서 병렬 처리해야 한다. 웹 기반 Quarto 프로젝트는 윈도우와 맥OS, 리눅스에서 동일한 명령어로 작동해야 한다. 이런 특수한 요구를 충족하기 위해 언어별, 용도별로 최적화된 도구들이 등장했다.\n\n\n\n\n\n그림 26.3: 데이터 과학 워크플로우 자동화 도구\n\n\n그림 26.3 는 현대 데이터 과학에서 사용되는 세 가지 워크플로우 자동화 도구를 비교한다. R 커뮤니티는 targets를 통해 데이터 분석의 재현가능성을 한 단계 높였다. 함수 하나를 수정하면 그 함수에 의존하는 모든 타겟이 자동으로 재계산되고, Quarto 보고서까지 연쇄적으로 업데이트된다. Python 생태계는 스네이크메이크로 대규모 데이터 파이프라인을 관리한다. make와 비슷한 규칙 기반 문법에 Python의 강력함을 더해, 생물정보학부터 기계학습 실험 추적까지 폭넓게 활용된다. 범용 빌드 도구로는 Task가 부상했다. YAML 파일 하나로 프로젝트의 모든 반복 작업을 정의하고, 크로스 플랫폼 환경에서 동일하게 실행할 수 있다.\nR 데이터 과학: targets\nR 프로젝트에서는 targets 패키지가 워크플로우 자동화의 표준이다. targets는 함수와 데이터 객체 간의 의존성을 자동으로 추적하고, 변경이 발생한 부분만 재실행한다. 특히 Quarto 문서와의 통합이 뛰어나 데이터 분석부터 보고서 생성까지 하나의 워크플로우로 관리할 수 있다.\nR 콘솔이나 포지트론 터미널에서 다음 명령어로 설치한다:\ninstall.packages(\"targets\")\ninstall.packages(\"tarchetypes\")  # Quarto 통합용\n\n# 설치 확인\nlibrary(targets)\npackageVersion(\"targets\")\ntarchetypes 패키지는 Quarto 문서를 targets 파이프라인에 통합하는 tar_quarto() 함수를 제공한다. 데이터 분석 결과가 변경되면 Quarto 문서도 자동으로 재렌더링된다.\nPython 데이터 과학: Snakemake\n스네이크메이크는 Python 기반 워크플로우 관리 시스템으로, 생물정보학 분야에서 시작해 데이터 과학 전반으로 확산되었다. make와 유사한 규칙 기반 문법에 Python 코드를 결합해 복잡한 파이프라인을 표현할 수 있다. 클러스터나 클라우드 환경에서 대규모 병렬 처리를 지원하며, 재현가능한 연구를 위한 표준 도구로 자리잡았다.\n스네이크메이크는 conda나 pip로 설치할 수 있다. conda를 사용하면 의존성 관리가 자동화되고, 필요한 생물정보학 도구들과 함께 환경을 구성할 수 있다:\n# conda로 설치 (권장 - 범용)\nconda install -c conda-forge snakemake\n\n# 생물정보학 프로젝트라면 bioconda 채널 사용\nconda install -c bioconda snakemake\n\n# pip로도 설치 가능\npip install snakemake\n\n# 설치 확인\nsnakemake --version\nconda-forge는 범용 데이터 과학 프로젝트에 적합하고, bioconda는 유전체 분석 같은 생물정보학 도구들을 함께 사용할 때 유용하다. conda 환경이 없다면 pip로도 충분히 설치할 수 있다.\n범용 빌드: Task\nTask는 make의 현대적 대안으로, YAML 파일에 작업을 정의하고 실행한다. make의 복잡한 문법 대신 읽기 쉬운 YAML을 사용하며, 윈도우와 맥OS, 리눅스 모두에서 동일하게 작동한다. Quarto 프로젝트의 렌더링, 테스트, 배포 같은 반복 작업을 자동화할 때 유용하다.\n운영체제별 설치 방법은 다음과 같다:\n# macOS\nbrew install go-task/tap/go-task\n\n# Windows (Chocolatey)\nchoco install go-task\n\n# Windows (Scoop)\nscoop install task\n\n# Linux\nsh -c \"$(curl --location https://taskfile.dev/install.sh)\" -- -d -b /usr/local/bin\n\n# 설치 확인\ntask --version\n프로젝트 루트에 Taskfile.yml을 생성하고 작업을 정의하면, task 작업명 명령어로 실행할 수 있다. targets나 스네이크메이크처럼 복잡한 의존성 추적은 제공하지 않지만, 간단한 빌드 스크립트를 작성할 때 make보다 훨씬 직관적이다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>개발 환경 구축</span>"
    ]
  },
  {
    "objectID": "ide_setup.html#sec-setup-isolation-comparison",
    "href": "ide_setup.html#sec-setup-isolation-comparison",
    "title": "26  개발 환경 구축",
    "section": "\n26.4 uv vs 도커",
    "text": "26.4 uv vs 도커\n파이썬 개발 환경 격리 도구인 uv와 도커는 모두 ‘재현 가능한 환경’ 구축을 목표로 하지만, 접근 방식과 범위가 다르다.\n\n\n\n\n\n그림 26.4: 환경 격리 수준 비교: uv vs 도커\n\n\n그림 26.4 는 uv와 도커의 가상화 계층 구조를 보여준다. uv는 파이썬 패키지 수준만 격리하는 반면, 도커는 OS부터 모든 시스템 의존성까지 완전히 격리한다.\n\n26.4.1 유사점\n두 도구 모두 프로젝트마다 독립적 환경을 제공해 의존성 충돌을 방지하는 환경 격리를 목표로 한다. 또한 동일한 환경을 다른 시스템에서 정확히 재현할 수 있는 재현가능성을 보장하며, 프로젝트 의존성을 파일로 명확히 정의한다. uv는 requirements.txt로, 도커는 Dockerfile로 의존성을 명시한다.\n\n26.4.2 차이점\n\n\n\n\n\n\n\n\n특성\nuv\n도커\n\n\n\n격리 수준\n파이썬 패키지 수준 (가상 환경)\nOS 수준 (완전한 컨테이너)\n\n\n범위\n파이썬 패키지만 관리\n파이썬, R, 시스템 라이브러리, OS 설정 등 전체\n\n\n무게\n매우 가볍고 빠름 (러스트 기반)\n상대적으로 무거움 (이미지 크기 수백 MB~GB)\n\n\n시작 속도\n즉시 (초 단위)\n컨테이너 시작 필요 (초~분)\n\n\n학습 곡선\n낮음 (pip/venv 익숙하면 쉬움)\n높음 (Dockerfile, 이미지, 컨테이너 개념)\n\n\n사용 시나리오\n로컬 파이썬 개발\n복잡한 멀티 언어 프로젝트, 배포, CI/CD\n\n\n시스템 의존성\n시스템 라이브러리에 의존\n시스템과 완전 독립\n\n\n\n\n\n\n\n표 26.1: uv와 도커 환경 격리 도구 비교\n\n\n\n\n26.4.3 언제 무엇을 사용하나?\nuv로 충분한 경우:\n순수 파이썬 패키지만 사용하는 프로젝트는 uv로 완벽히 재현 가능하다.\n# 예: 웹 개발, 데이터 분석 기본\npandas, requests, fastapi, pydantic, numpy, scikit-learn\n이런 패키지는 시스템 라이브러리 의존성이 없거나 최소화되어, uv가 파이썬 버전과 패키지만 관리해도 어떤 시스템에서든 동일하게 작동한다.\n도커가 필요한 경우:\n시스템 라이브러리 의존성이 있는 패키지는 도커 권장이다.\n# 예: 컴퓨터 비전, 데이터베이스, 지리정보\nopencv-python     # C++ 라이브러리 (OpenCV)\npsycopg2          # PostgreSQL 라이브러리\nGDAL              # 지리정보 시스템 라이브러리\n이런 패키지는 OS의 시스템 라이브러리가 필요하므로, 도커로 OS부터 완전히 격리하는 것이 안전하다.\n둘 다 사용: 도커 컨테이너 안에서 uv를 사용해 파이썬 패키지를 관리하는 것도 가능하다. 도커로 시스템 라이브러리 환경을 구축하고, uv로 파이썬 패키지를 빠르게 관리하는 조합이다.\n결론: 순수 파이썬 프로젝트는 uv로 시작하고, 시스템 의존성이 생기면 도커를 고려한다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>개발 환경 구축</span>"
    ]
  },
  {
    "objectID": "ide_setup.html#sec-setup-languages",
    "href": "ide_setup.html#sec-setup-languages",
    "title": "26  개발 환경 구축",
    "section": "\n26.5 코딩 언어 설치",
    "text": "26.5 코딩 언어 설치\nR은 통계 분석, 데이터 시각화, 학술 연구 분야에서 전통적 강점을 가진다. ggplot2 같은 강력한 시각화 라이브러리와 수많은 통계 패키지는 R의 큰 자산이다.\n반면 파이썬은 머신러닝, 딥러닝, 웹 개발, 시스템 자동화 등 범용성과 확장성에서 뛰어나다. TensorFlow, PyTorch 같은 딥러닝 프레임워크와 방대한 커뮤니티를 자랑한다.\n두 언어는 경쟁 관계이기도 하지만, 서로의 단점을 보완하는 강력한 상보 관계이기도 하다. R로 데이터를 깊이 있게 탐색하고 시각화한 후, 파이썬으로 복잡한 머신러닝 모델을 구축하거나 서비스로 배포하는 워크플로우는 매우 효과적이다. 현대 데이터 과학자에게 두 언어 모두를 능숙하게 다루는 능력은 큰 경쟁력이 된다.\n\n26.5.1 R 설치 및 환경 관리\nR 설치\nR은 CRAN(The Comprehensive R Archive Network) 공식 네트워크로 배포된다.\nCRAN 공식 웹사이트에 접속해 자신의 운영체제(리눅스, 맥OS, 윈도우)에 맞는 R 설치 파일을 다운로드하고 실행한다. 설치 과정에서는 대부분 기본 설정을 유지하는 것이 좋다.\n\n참고: RStudio나 Positron은 R을 실행하기 위한 IDE일 뿐, R 자체는 아니다. IDE 사용 전에 반드시 시스템에 R 언어가 먼저 설치되어 있어야 한다.\n\nR 환경 관리: renv\n프로젝트마다 사용하는 R 패키지 버전이 다르면 충돌이 발생할 수 있다. renv는 프로젝트별로 독립된 패키지 라이브러리를 만들어 의존성 문제를 해결하는 도구다.\n중요한 제한사항: renv는 R 패키지만 격리한다. R 인터프리터(언어 실행기) 자체는 시스템에 설치된 것을 공유해 사용한다. renv.lock 파일에 “R 버전 4.3.0”이라고 기록은 하지만, 해당 R 버전이 시스템에 없으면 설치해주지 않고 경고만 표시한다. R 버전까지 완전히 격리하려면 rig (R Installation Manager) 같은 별도 도구와 함께 사용해야 한다.\n이는 파이썬 uv와의 큰 차이다. uv는 uv python install 3.11 명령으로 파이썬 인터프리터 자체를 다운로드하고 관리하지만, renv는 그런 기능이 없다.\n설치 및 사용:\nR 콘솔에서 install.packages(\"renv\")를 실행해 설치한다. RStudio나 Positron에서 새로운 프로젝트를 시작할 때 renv 사용 옵션을 체크하면 프로젝트 폴더에 renv 관련 파일이 생성된다. renv::snapshot()으로 패키지 목록과 버전을 기록하고, renv::restore()로 다른 환경에서 복원한다.\nrig + renv 조합으로 완전한 격리:\nR 버전까지 완전히 격리하려면 rig (R Installation Manager)와 renv를 함께 사용한다. 먼저 rig GitHub에서 운영체제에 맞는 설치 프로그램을 다운로드한다. rig add 4.3.0 명령으로 원하는 R 버전을 설치하고, rig default 4.3.0 또는 .Rprofile 파일로 프로젝트별 R 버전을 고정한다. 이후 renv로 패키지를 관리하면 파이썬 uv처럼 런타임(R 인터프리터)과 패키지 모두 프로젝트별로 격리할 수 있다.\n\n\n\n\n\n그림 26.5: uv vs rig + renv: 런타임 격리 방식 비교\n\n\n\n26.5.2 파이썬 설치 및 환경 관리\n파이썬 설치\n파이썬 설치 방법은 크게 두 가지다.\n공식 파이썬 설치 프로그램 사용 (권장): 파이썬 공식 웹사이트에 접속해 최신 안정화 버전을 다운로드한다. 윈도우 설치 시 첫 화면에서 “Add Python.exe to PATH” 옵션을 반드시 체크해야 터미널에서 python 명령어를 바로 사용할 수 있다. 이 방법은 가장 깔끔하고 표준적인 파이썬 환경을 제공한다.\n아나콘다(Anaconda) 배포판 사용: 아나콘다 배포판은 파이썬 자체뿐 아니라 numpy, pandas, scikit-learn 등 수백 개의 데이터 과학 패키지를 함께 묶어 제공한다. 초보자에게는 편리할 수 있지만, 시스템 환경을 복잡하게 만들 수 있고 용량이 크다는 단점이 있다. 이 책에서는 공식 파이썬 설치를 기준으로 설명한다.\n파이썬 환경 관리: uv\n과거에는 pip로 패키지를 설치하고 venv로 가상 환경을 만드는 등 여러 도구를 조합해야 했지만, 최근에는 uv라는 차세대 통합 도구가 등장해 파이썬 개발 환경 관리가 훨씬 빠르고 간편해졌다. uv는 러스트(Rust)로 작성되어 기존 도구보다 수십 배에서 수백 배 빠른 속도를 자랑한다.\nuv 설치: 터미널에서 운영체제에 맞는 명령어를 실행해 uv를 설치한다. 맥OS/리눅스에서는 curl -LsSf https://astral.sh/uv/install.sh | sh를, 윈도우 파워셸(PowerShell)에서는 powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"를 실행한다.\n가상 환경 생성: 프로젝트 폴더로 이동한 후 터미널에서 uv venv 명령어를 실행하면 .venv라는 폴더에 가상 환경이 생성된다.\n가상 환경 활성화: 윈도우에서는 .\\.venv\\Scripts\\activate를, 맥OS/리눅스에서는 source .venv/bin/activate를 실행한다. 활성화되면 터미널 프롬프트 앞에 (.venv)와 같은 표시가 나타난다.\n패키지 설치: 가상 환경이 활성화된 상태에서 uv pip install 명령어로 패키지를 매우 빠르게 설치할 수 있다. 예를 들어 uv pip install pandas scikit-learn 명령으로 pandas와 scikit-learn을 설치하거나, uv pip install -r requirements.txt 명령으로 requirements.txt 파일로부터 패키지를 설치한다.\n작업이 끝나면 터미널에서 deactivate 명령어를 실행해 가상 환경을 비활성화한다.\n설치 확인: 다음 명령어로 uv가 정상 설치되었는지 확인한다:\n$ uv --version\nuv 0.1.18\n\n$ uv pip list\nPackage    Version\n---------- -------\npip        24.0\nsetuptools 69.0.3",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>개발 환경 구축</span>"
    ]
  },
  {
    "objectID": "ide_setup.html#sec-setup-ide",
    "href": "ide_setup.html#sec-setup-ide",
    "title": "26  개발 환경 구축",
    "section": "\n26.6 통합 개발 환경 설치",
    "text": "26.6 통합 개발 환경 설치\n\n26.6.1 Positron IDE 설치\nPositron은 R과 파이썬을 모두 지원하는 차세대 데이터 과학 IDE다. VS Code 기반의 현대적인 인터페이스와 RStudio의 강력한 데이터 과학 기능을 결합하여 두 언어를 함께 사용하는 데이터 과학자에게 최적화된 환경을 제공한다.\nPositron 다운로드에서 운영체제(윈도우, 맥OS, 리눅스)에 맞는 설치 프로그램을 다운로드한다. Positron은 현재 베타 버전이므로 최신 정보를 확인하는 것이 중요하다. 다운로드한 파일을 실행하고 안내에 따라 설치를 진행한다. 대부분 경우 기본 설정을 따르는 것이 좋다.\nPositron은 R과 파이썬 인터프리터를 함께 사용한다. 설치 후 Positron을 실행하여 Tools → Global Options 또는 Preferences에서 R 및 파이썬 인터프리터 경로가 올바르게 설정되었는지 확인한다. 이를 통해 Positron이 시스템에 설치된 R 및 파이썬 환경을 정확히 인식하고 활용할 수 있다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>개발 환경 구축</span>"
    ]
  },
  {
    "objectID": "ide_setup.html#sec-setup-quarto",
    "href": "ide_setup.html#sec-setup-quarto",
    "title": "26  개발 환경 구축",
    "section": "\n26.7 문학적 프로그래밍",
    "text": "26.7 문학적 프로그래밍\nPositron IDE는 쿼토를 잘 지원하지만, 쿼토의 모든 기능을 활용해 다양한 포맷(특히 PDF)으로 문서를 렌더링하려면 몇 가지 추가 도구가 필요할 수 있다.\n판독(Pandoc): 쿼토는 문서 변환의 핵심 엔진으로 판독을 사용한다. 대부분 경우 쿼토 설치 시 판독이 함께 번들되어 제공되므로 별도로 설치할 필요는 없다. 터미널에서 pandoc --version 명령어를 실행해 설치 여부와 버전을 확인할 수 있다.\n\\(\\LaTeX\\) 배포판 (PDF 출력을 위해 필수): 쿼토로 PDF 문서를 생성하려면 \\(\\LaTeX\\) 배포판이 시스템에 설치되어 있어야 한다. 쿼토는 자동으로 TinyTEX을 설치할 수 있도록 지원하며, 이는 가장 권장되는 방법이다. R 콘솔 또는 Positron의 터미널에서 다음 명령어를 실행해 TinyTEX을 설치한다:\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\nTinyTEX 대신 MiKTEX(윈도우)나 TEX Live(리눅스/맥OS) 같은 다른 \\(\\LaTeX\\) 배포판을 설치할 수도 있다. 하지만 TinyTEX이 가장 가볍고 쿼토와 통합이 용이하다.\n쿼토 CLI 설치 확인: Positron 자체에 쿼토 기능이 통합되어 있더라도 터미널에서 쿼토 명령어를 직접 사용하려면 쿼토 CLI가 설치되어 있어야 한다. 쿼토 공식 웹사이트에서 설치하거나 quarto install 명령어를 통해 설치할 수 있다. Positron이 쿼토를 번들하는 경우도 많으므로 먼저 quarto --version으로 확인하는 것이 좋다.\n설정 확인: 쿼토 관련 설정이 모두 완료되면 터미널에서 quarto check 명령어를 실행해 필요한 도구들이 올바르게 설정되었는지 진단할 수 있다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>개발 환경 구축</span>"
    ]
  },
  {
    "objectID": "ide_setup.html#sec-setup-fonts",
    "href": "ide_setup.html#sec-setup-fonts",
    "title": "26  개발 환경 구축",
    "section": "\n26.8 개발 글꼴",
    "text": "26.8 개발 글꼴\n개발자가 마주하는 폰트 환경은 세 가지로 나뉜다. 코드 편집기와 터미널에서는 고정폭(monospace) 폰트가 필수다. 웹이나 앱의 UI를 디자인할 때는 시스템과 조화를 이루는 가변폭 산세리프 폰트를 사용한다. 기술 문서나 블로그처럼 장문을 다룰 때는 장시간 읽기에 적합한 폰트가 필요하다.\n\n\n\n\n\n그림 26.6: 개발자 폰트 생태계\n\n\n그림 26.6 은 개발자 폰트 생태계를 용도별로 정리한다. 코딩 환경에서는 JetBrains Mono, Fira Code, D2코딩이 대표적이다. 특히 D2코딩은 한글 가독성이 뛰어나 한국 개발자에게 필수로 꼽힌다. 웹과 앱 UI에서는 프리텐다드가 한국 웹 표준으로 자리잡았고, 글로벌 환경에서는 Inter가 널리 쓰인다. 구글의 Noto Sans CJK는 “No more Tofu”라는 슬로건 아래 800개 이상의 언어를 지원하며, 폰트 깨짐 없는 다국어 환경을 제공한다. 문서 작성에서는 Noto 패밀리가 Sans, Serif, Mono 전방위로 활약하며, IBM Plex Sans나 마루부리 같은 폰트도 기술 블로그와 출판물에 자주 등장한다.\nNoto 프로젝트는 구글이 시작한 범세계적 폰트 이니셔티브다. “Tofu”는 폰트가 없어서 나타나는 □ 문자를 의미하는데, Noto는 이 문제를 해결하고자 탄생했다. 한글, 중국어, 일본어를 포함한 CJK(Chinese, Japanese, Korean) 언어부터 아랍어, 히브리어, 태국어까지 모든 문자를 하나의 통일된 디자인으로 제공한다. 개발자가 다국어 환경을 구축할 때 Noto 패밀리를 사용하면 언어별로 폰트를 따로 관리할 필요 없이 일관된 타이포그래피를 유지할 수 있다.\n\n26.8.1 D2코딩 폰트 설치\n개발 환경에서 폰트를 사용하려면 두 단계를 거친다. 먼저 폰트를 운영체제에 설치하고, 그 다음 IDE 설정에서 해당 폰트를 지정한다. D2코딩은 한글과 영문이 조화롭게 어우러지도록 설계된 모노스페이스 폰트로, 한국 개발자에게 가장 많이 쓰인다.\nD2코딩 GitHub 릴리즈 페이지에서 최신 zip 파일을 다운로드한 뒤 압축을 풀면 .ttf 파일이 나타난다. 이 파일을 더블 클릭하면 윈도우에서는 ‘설치’ 버튼이, 맥OS에서는 ‘서체 설치’ 버튼이 나타난다. 버튼을 클릭하면 시스템 전체에서 D2코딩 폰트를 사용할 수 있게 된다.\n시스템 설치가 끝나면 IDE에서 이 폰트를 지정한다. Positron이나 VS Code에서 Cmd/Ctrl + ,로 설정을 열고 font family를 검색한다. ‘Editor: Font Family’ 항목의 맨 앞에 'D2Coding',을 추가하면 코드 편집기가 D2코딩을 최우선으로 사용한다. 예를 들어 다음과 같은 형태가 된다:\n'D2Coding', \"Apple SD Gothic Neo\", \"Malgun Gothic\", monospace\n폰트 목록의 맨 앞에 배치한 이유는 간단하다. IDE는 목록 순서대로 폰트를 찾아 사용하므로, D2코딩이 설치되어 있으면 이 폰트로 표시하고, 없으면 다음 폰트로 넘어간다. 설정을 저장하면 편집기 화면이 즉시 D2코딩으로 바뀐다.\n\n26.8.2 프리텐다드 가변 글꼴\n프리텐다드는 D2코딩과 달리 웹 환경을 위해 설계된 가변 글꼴이다. 가변 폰트(Variable Font)는 하나의 파일에 여러 weight(두께)를 담는 기술로, 프리텐다드는 Thin(100)부터 Black(900)까지 9단계를 지원한다. 한글, 영문, 일본어를 모두 포괄하며, 윈도우와 맥OS, 리눅스에서 일관된 렌더링을 보장한다. 특히 쿼토로 생성한 HTML 문서나 GitHub Pages, 기술 블로그처럼 웹에 게시되는 콘텐츠에서 시스템 폰트의 한계를 넘어서는 가독성을 제공한다.\n프리텐다드는 사용 목적에 따라 두 가지 방식으로 설치한다. 프리텐다드 GitHub 릴리즈에서 zip 파일을 다운로드하면 여러 형식의 폰트 파일이 들어 있다. IDE나 데스크톱 애플리케이션에서 사용하려면 public/static 폴더의 OTF나 TTF 파일을 시스템에 설치한다. D2코딩과 같은 방식이다.\n웹 페이지에 직접 삽입하려면 web/variable 폴더의 WOFF2 파일을 사용한다. 쿼토 프로젝트의 _quarto.yml이나 CSS 파일에서 @font-face로 선언하면 방문자의 시스템에 폰트가 없어도 웹 페이지에서 프리텐다드를 표시할 수 있다:\n@font-face {\n  font-family: 'Pretendard';\n  src: url('fonts/PretendardVariable.woff2') format('woff2-variations');\n  font-weight: 100 900;\n}\n\nbody {\n  font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, sans-serif;\n}\n웹 폰트로 사용하면 모든 플랫폼의 방문자에게 일관된 타이포그래피를 제공할 수 있다. 시스템 폰트는 사용자의 OS에 따라 달라지지만, 웹 폰트는 제작자가 의도한 대로 정확히 표시된다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>개발 환경 구축</span>"
    ]
  },
  {
    "objectID": "ide_setup.html#sec-setup-verify",
    "href": "ide_setup.html#sec-setup-verify",
    "title": "26  개발 환경 구축",
    "section": "\n26.9 개발 환경 검증",
    "text": "26.9 개발 환경 검증\n지금까지 포지트론 IDE부터 시작해 R과 파이썬, Git과 도커, 워크플로우 자동화 도구(make, targets, 스네이크메이크, Task), 쿼토 출판 시스템, 개발 글꼴(D2코딩, 프리텐다드)까지 현대 데이터 과학 환경의 핵심 요소들을 설치했다. 각 도구는 독립적으로 작동하지만, 함께 사용할 때 시너지를 발휘한다. 포지트론에서 R 코드로 데이터를 분석하고, targets로 워크플로우를 자동화하며, 쿼토로 결과를 문서화하고, Git으로 버전을 관리하는 통합 환경이 완성된 것이다. 설치가 제대로 되었는지 확인하는 것이 다음 단계다.\n핵심 도구들의 설치 여부를 터미널에서 한번에 확인할 수 있다:\n$ git --version && docker --version && make --version | head -1 && R --version | head -1 && python --version && uv --version\ngit version 2.39.2 (Apple Git-143)\nDocker version 24.0.6, build ed223bc\nGNU Make 3.81\nR version 4.3.0 (2023-04-21) -- \"Already Tomorrow\"\nPython 3.11.5\nuv 0.1.18\n모든 명령어가 버전 정보를 출력하면 기본 환경 구축이 완료된 것이다. 워크플로우 자동화 도구는 필요에 따라 선택적으로 확인한다. R 프로젝트라면 R 콘솔에서 library(targets)를 실행해 targets 패키지가 로드되는지 확인하고, Python 생물정보학 프로젝트라면 snakemake --version으로 스네이크메이크 설치를 점검한다. 범용 빌드 도구인 Task를 설치했다면 task --version으로 확인한다.\n개발 글꼴도 IDE 설정에서 확인한다. 포지트론이나 VS Code의 설정(Cmd/Ctrl + ,)에서 ’Font Family’를 검색하면 D2코딩이나 프리텐다드가 목록 맨 앞에 있는지 확인할 수 있다. 코드 편집기에서 한글과 영문이 조화롭게 표시되면 글꼴 설정이 올바른 것이다.\n모든 검증이 완료되면 포지트론을 실행하고 R과 파이썬 인터프리터 경로를 설정해 첫 데이터 과학 프로젝트를 시작할 준비가 된다.\n💡 생각해볼 점\n개발 환경 구축은 한 번에 완성되지 않는다. Git, Docker, R, Python, Positron, Quarto, 워크플로우 자동화, 개발 글꼴까지 나열된 도구들을 보면 압도될 수 있지만, 모든 것을 한 번에 설치할 필요는 없다. 프로젝트 성격에 따라 점진적으로 추가하는 것이 효율적이다.\n첫 데이터 분석 프로젝트라면 Positron과 R 또는 Python 하나만 설치하고 시작하자. CSV 파일을 열고, 간단한 전처리를 하며, ggplot2나 matplotlib로 그래프를 그려보자. 프로젝트가 커지면서 버전 관리가 필요해지면 Git을 추가한다. 팀원과 협업하거나 재현 가능성이 중요해지면 Docker를 도입한다. 분석 보고서를 작성할 때가 되면 Quarto와 TinyTeX를 설치한다.\n환경 격리 도구 선택은 프로젝트 복잡도로 판단한다. 순수 Python 패키지만 사용하는 데이터 분석이라면 uv로 충분하다. pandas, scikit-learn, matplotlib만으로 대부분의 분석이 가능하다. 하지만 OpenCV(컴퓨터 비전), PostgreSQL(데이터베이스), GDAL(공간 데이터) 같은 시스템 라이브러리가 필요하거나, R과 Python을 함께 사용하는 멀티 언어 프로젝트라면 Docker가 더 안전하다.\nR 사용자는 특별히 주의해야 한다. renv는 패키지만 격리하고 R 버전은 격리하지 않는다. Python의 uv가 uv python install 3.11로 런타임까지 관리하는 것과 대조적이다. R 버전까지 완전히 격리하려면 rig + renv 조합을 사용하거나, 아예 Docker로 R 버전부터 시스템까지 모두 패키징하는 것이 확실하다.\n개발 글꼴은 생산성에 직접적 영향을 준다. D2코딩 하나만 설치해도 한글과 영문이 조화로운 코딩 환경을 얻는다. 1(숫자)과 l(소문자 L), 0(숫자)과 O(대문자 o)를 명확히 구분할 수 있는 글꼴은 버그를 줄이고 눈의 피로를 덜어준다.\n지금 구축한 환경은 “완벽”이 아니라 “시작점”이다. 다음 장부터는 실전 프로젝트를 진행하면서 부족한 부분을 발견하고, 필요한 도구를 추가하며, 자신만의 최적 워크플로우를 만들어간다. Git으로 실험을 추적하고, Quarto로 분석을 문서화하며, Docker로 환경을 공유하는 과정에서 각 도구의 가치를 체감하게 된다.",
    "crumbs": [
      "**4부** 통합 개발 환경",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>개발 환경 구축</span>"
    ]
  },
  {
    "objectID": "docker_concept.html",
    "href": "docker_concept.html",
    "title": "27  도커: 재현 환경",
    "section": "",
    "text": "27.1 가상화 기술의 진화\n2023년 어느 연구팀의 이야기다. 논문 심사 과정에서 심사위원이 “분석 결과를 재현할 수 없습니다”라고 지적했다. 연구자는 당황했다. 6개월 전 자신의 컴퓨터에서는 완벽히 작동했기 때문이다. 코드와 데이터를 다시 실행했지만, 결과 그래프가 달랐다. 그 사이 R이 4.2에서 4.3으로 업그레이드되었고, ggplot2 패키지가 3.4에서 3.5로 바뀌면서 테마 기본값이 변경되었다. 맥OS도 Monterey에서 Sonoma로 업데이트되었다. 원본 환경을 복구하려 했지만, 구버전 R과 패키지를 다시 설치하는 과정에서 새로운 오류들이 발생했다. 결국 논문 게재는 6개월 지연되었다.\n이것이 재현가능성 위기다. “내 컴퓨터에서는 되는데요”는 개발자가 가장 두려워하는 말이다. 동일한 코드, 동일한 데이터인데도 환경 차이로 결과가 달라진다. 의존성(dependency) 문제는 과학 연구의 신뢰성을 위협한다.\n도커(Docker)는 이 문제를 근본적으로 해결한다. 분석 환경 전체를 “스냅샷”으로 저장한다. Ubuntu 20.04, R 4.3.0, ggplot2 3.4.2, 시스템 라이브러리, 데이터까지 모든 것을 하나의 컨테이너로 패키징한다. 이 컨테이너는 어디서나 동일하게 실행된다. 5년이 지나도, 다른 컴퓨터에서도, 클라우드 서버에서도 정확히 같은 결과를 재현한다.\n도커를 이해하려면 가상화 기술의 역사를 알아야 한다. 도커는 갑자기 등장한 기술이 아니라, 50년 가상화 역사의 최신 진화다.\n가상화(Virtualization)는 “하나의 물리적 컴퓨터에서 여러 개의 독립적인 컴퓨터를 실행”하는 기술이다. 1960년대 IBM 메인프레임에서 시작해, 2000년대 VMware가 대중화했고, 2013년 도커가 혁명을 일으켰다.\n1) 가상 머신 (VM) 시대 (1999-2010)\nVMware, VirtualBox 같은 도구는 하드웨어 가상화를 제공한다. 윈도우 위에서 리눅스를 실행하거나, 맥OS 위에서 윈도우를 실행한다. 각 VM은 완전한 OS를 포함하므로 무겁고 느리다. Windows 10 VM 하나가 20GB 디스크를 차지하고, 4GB 메모리를 사용하며, 부팅에 30초가 걸린다. 노트북에서 3개의 VM을 동시에 실행하면 시스템이 느려진다.\n2) 컨테이너 등장 (2008-2012)\n리눅스 컨테이너(LXC)는 OS 수준 가상화를 제공한다. 하나의 리눅스 커널을 여러 컨테이너가 공유하므로 VM보다 훨씬 가볍다. 컨테이너 하나가 몇 MB에서 수백 MB이고, 부팅은 1초 이내다. 하지만 사용법이 복잡하고, 리눅스에서만 작동했다.\n3) 도커 혁명 (2013-현재)\n도커는 컨테이너를 누구나 쉽게 사용할 수 있게 만들었다. 간단한 명령어, 이미지 공유 플랫폼(Docker Hub), 크로스 플랫폼 지원(윈도우/맥OS/리눅스)으로 컨테이너 기술을 대중화했다. 2013년 Solomon Hykes가 PyCon에서 5분 데모를 선보인 후, 몇 년 만에 업계 표준이 되었다.\n그림 27.1 은 가상화 기술의 50년 진화를 보여준다. 가상 머신(VM)은 완전한 OS를 포함해 무겁지만(20GB+, 4GB RAM) 완벽한 격리를 제공한다. 컨테이너는 OS 커널을 공유해 가볍지만(수십 MB, 실사용 메모리만) 리눅스에서만 작동했다. 도커는 컨테이너의 가벼움을 유지하면서도, Windows/macOS에서도 사용할 수 있게 만들었다. Docker Hub로 이미지를 공유하고, 간단한 명령어로 누구나 컨테이너를 사용하게 되었다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>도커: 재현 환경</span>"
    ]
  },
  {
    "objectID": "docker_concept.html#sec-docker-history",
    "href": "docker_concept.html#sec-docker-history",
    "title": "27  도커: 재현 환경",
    "section": "",
    "text": "그림 27.1: 가상화 기술 진화 - VM → 컨테이너 → 도커\n\n\n\n\n\n\n\n\n\n노트도커 vs 쿠버네티스: 데이터 과학자 선택\n\n\n\n그림 27.1 에서 보았듯이 도커(2013) 이후 쿠버네티스(2015)가 등장했다. 쿠버네티스가 더 최신 기술인데, 왜 데이터 과학은 도커에 초점을 맞추는가?\n해결하는 문제가 다르다. 도커는 “환경을 패키징”하는 도구다. 쿠버네티스는 “수백 개 컨테이너를 자동으로 운영”하는 도구다. 넷플릭스가 수천 대 서버에서 마이크로서비스를 운영하거나, 구글이 트래픽 급증 시 자동으로 서버를 늘리는 상황에 필요하다.\n\n\n\n\n\n\n\n\n구분\n도커\n쿠버네티스\n\n\n\n주 사용자\n개발자, 데이터 과학자\nDevOps, 인프라 엔지니어\n\n\n학습 곡선\n몇 시간~며칠\n몇 주~몇 달\n\n\n시작 명령\ndocker run\n클러스터 설정부터\n\n\n운영 규모\n1~10개 컨테이너\n수십~수천 개 컨테이너\n\n\n\n\n\n\n\n표 27.1: 도커와 쿠버네티스 비교\n\n\n\n데이터 과학자 대부분은 개인 또는 소규모 팀으로 작업한다. 탐색적 분석을 빠르게 실행하고 중지하며, 자동 스케일링보다는 “동일 환경 재현”이 핵심 목표다. 노트북에서 docker run 한 줄이면 작업을 시작할 수 있다. 반면 쿠버네티스는 Shiny 앱 동시 접속자가 수천 명에 달하거나, ML API가 초당 수만 건 요청을 처리해야 하거나, 24시간 무중단 서비스가 필요한 상황에 적합하다. 대부분의 데이터 과학자에게 쿠버네티스는 “알면 좋지만 당장 필요하지 않은” 기술이다. 도커만으로 재현가능성, 이식성, 공유성 문제를 충분히 해결할 수 있다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>도커: 재현 환경</span>"
    ]
  },
  {
    "objectID": "docker_concept.html#sec-docker-concepts",
    "href": "docker_concept.html#sec-docker-concepts",
    "title": "27  도커: 재현 환경",
    "section": "\n27.2 이미지와 컨테이너",
    "text": "27.2 이미지와 컨테이너\n도커를 처음 접하면 “이미지”와 “컨테이너”라는 용어가 혼란스럽다. 둘 다 “가상 환경”처럼 들리지만, 역할이 완전히 다르다. 이미지(Image)는 컴퓨팅 환경 “설계도”다. 프로그래밍 클래스(Class)처럼 “무엇을 포함할지” 정의한 템플릿이다. 컨테이너(Container)는 설계도로 만든 “실제 작동하는 컴퓨팅 환경”이다. 클래스에서 new로 인스턴스를 생성하듯, 이미지에서 docker run으로 컨테이너를 생성한다. 하나의 이미지에서 여러 컨테이너를 만들 수 있고, 각 컨테이너는 독립적으로 실행/중지/삭제할 수 있다.\n\n\n\n\n\n그림 27.2: 도커 이미지와 컨테이너 관계\n\n\n그림 27.2 는 이미지와 컨테이너의 관계를 보여준다. 왼쪽의 rocker/rstudio:4.3.0 이미지 하나에서 오른쪽의 여러 컨테이너가 생성된다. 각 컨테이너는 독립적으로 실행되며, 서로 다른 포트(8787, 8788, 8789)에서 접속할 수 있다. 프로그래밍의 Class와 Instance 관계와 동일하다.\n도커 이미지는 가상 컴퓨터의 “설계도”다. Ubuntu 20.04, R 4.3.0, tidyverse 패키지, 데이터 파일이 포함된 완전한 환경 스냅샷이다. 이미지는 읽기 전용(read-only)이며, 파일로 저장되고 공유된다. Docker Hub에서 전 세계 개발자가 만든 이미지를 다운로드하거나, 자신의 이미지를 업로드해 공유한다.\n도커 컨테이너는 이미지를 “실행한 것”이다. 이미지가 “프로그램 파일”이라면, 컨테이너는 “실행 중인 프로세스”다. 하나의 이미지에서 여러 컨테이너를 동시에 실행할 수 있다. 예를 들어, rocker/rstudio 이미지 하나로 3개의 RStudio 컨테이너를 각각 다른 포트에서 실행할 수 있다.\n\n\n\n\n\n\n힌트데이터 과학 도커 핵심 가치\n\n\n\n1. 재현성: 논문, 보고서, 분석 결과를 도커 이미지와 함께 제공하면, 누구나 정확히 동일한 환경에서 결과를 재현할 수 있다. 5년 후에도, 다른 컴퓨터에서도 같은 결과가 나온다.\n2. 이식성: 로컬 컴퓨터에서 개발한 환경을 GPU 서버로, AWS/GCP 클라우드로, 동료 컴퓨터로 그대로 이동한다. “로컬에서는 되는데 서버에서는 안 돼요” 문제가 사라진다.\n3. 공유성: 팀원에게 코드와 README만 주면 환경 설정에 반나절이 걸린다. 도커 이미지를 주면 docker run 명령 하나로 5분 안에 작업을 시작한다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>도커: 재현 환경</span>"
    ]
  },
  {
    "objectID": "docker_concept.html#sec-docker-how",
    "href": "docker_concept.html#sec-docker-how",
    "title": "27  도커: 재현 환경",
    "section": "\n27.3 도커의 작동 원리",
    "text": "27.3 도커의 작동 원리\n도커가 “컴퓨터 안의 컴퓨터”를 만드는 방식은 가상 머신(VM)과 근본적으로 다르다. VM은 하드웨어를 가상화해 완전한 운영체제를 실행하지만, 도커 컨테이너는 호스트의 리눅스 커널을 공유하면서 네임스페이스로 프로세스를 격리하고 cgroups로 자원을 제한한다. 커널을 공유하기 때문에 VM처럼 수십 GB의 디스크와 수 GB의 메모리가 필요하지 않다. 컨테이너는 수십 MB에서 수백 MB로 가볍고, 부팅 시간도 1초 이내다.\n\n\n\n\n\n그림 27.3: 도커 내부 구조\n\n\n그림 27.3 는 컨테이너가 호스트 커널 위에서 작동하는 방식을 보여준다. 컨테이너 A와 B는 각각 독립된 네임스페이스를 갖지만, 맨 아래 리눅스 커널은 공유한다. 이미지 레이어도 공유된다. 두 컨테이너가 같은 Ubuntu 베이스 이미지를 사용하면, 해당 레이어는 디스크에 한 번만 저장된다. cgroups는 각 컨테이너가 사용할 수 있는 CPU와 메모리를 제한해 한 컨테이너가 시스템 자원을 독점하지 못하게 한다.\n도커 이미지는 레이어(layer) 구조로 이루어진다. 마치 투명 필름을 겹치듯 Ubuntu 레이어 위에 R 설치 레이어가 올라가고, 그 위에 tidyverse 레이어가 쌓인다. 각 레이어는 읽기 전용이며, 컨테이너가 실행될 때 맨 위에 쓰기 가능한 얇은 레이어가 추가된다. 컨테이너 안에서 파일을 수정하면 쓰기 레이어에만 기록되고, 원본 이미지는 변경되지 않는다. 레이어 구조의 장점은 캐싱에 있다. Dockerfile에서 코드 파일만 수정했다면 OS와 패키지 레이어는 그대로 재사용되어 빌드 시간이 몇 분에서 몇 초로 단축된다. 여러 이미지가 같은 베이스 레이어를 공유하면 디스크 공간도 절약된다.\n네임스페이스(namespace)는 컨테이너가 마치 독립된 시스템처럼 보이게 하는 리눅스 커널 기능이다. PID 네임스페이스 덕분에 컨테이너 내부에서는 프로세스 ID가 1번부터 시작하고, 다른 컨테이너의 프로세스는 보이지 않는다. NET 네임스페이스는 각 컨테이너에 독립된 네트워크 인터페이스를 제공해 IP 주소와 포트가 충돌하지 않게 한다. MNT 네임스페이스는 파일시스템을 격리해 컨테이너가 호스트의 다른 파일에 접근하지 못하게 한다. cgroups(control groups)는 자원 할당을 제어한다. docker run --cpus=2 --memory=4g 명령으로 컨테이너가 최대 2개 CPU 코어와 4GB 메모리만 사용하도록 제한할 수 있다. 한 컨테이너에서 무한 루프가 돌거나 메모리 누수가 발생해도 다른 컨테이너와 호스트 시스템은 영향받지 않는다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>도커: 재현 환경</span>"
    ]
  },
  {
    "objectID": "docker_concept.html#sec-docker-desktop",
    "href": "docker_concept.html#sec-docker-desktop",
    "title": "27  도커: 재현 환경",
    "section": "\n27.4 Docker Desktop",
    "text": "27.4 Docker Desktop\n도커 데스크톱(Docker Desktop)은 Windows와 macOS 사용자가 도커를 경험하는 첫 관문이다. 도커 컨테이너는 리눅스 커널의 네임스페이스와 cgroups 기술을 기반으로 작동하기 때문에, 리눅스가 아닌 운영체제에서는 직접 실행할 수 없다. Docker Desktop은 내부적으로 경량 리눅스 가상 머신을 실행하고 그 위에서 컨테이너를 구동함으로써 이 문제를 해결한다.\n\n\n\n\n\n그림 27.4: Docker Desktop 아키텍처\n\n\n그림 27.4 은 세 가지 운영체제에서 도커가 작동하는 방식을 보여준다. Windows에서는 WSL 2(Windows Subsystem for Linux 2)와 통합되어 네이티브에 가까운 성능을 제공하고, macOS에서는 Apple Silicon의 Virtualization.framework를 활용한다. 두 환경 모두 사용자가 내부 구조를 의식할 필요 없이 터미널에서 docker run 명령만 실행하면 된다. 리눅스에서는 VM 없이 Docker Engine만 설치해 커널과 직접 통신하므로 가장 효율적이다.\n\n\n\n\n\n그림 27.5: Docker Desktop GUI\n\n\n그림 27.5 는 Docker Desktop 앱의 실제 화면을 보여준다. 왼쪽 사이드바에서 Containers, Images, Volumes 메뉴를 선택하고, 중앙 영역에서 각 컨테이너의 실행 상태, 사용 중인 이미지, 포트 매핑을 한눈에 확인할 수 있다. 녹색 원은 실행 중인 컨테이너, 회색 원은 종료된 컨테이너를 나타낸다. 오른쪽 버튼으로 컨테이너를 시작하거나 중지할 수 있어, 명령어를 외우지 않아도 마우스 클릭만으로 컨테이너를 관리할 수 있다. Docker Compose, Kubernetes(선택) 등 도커 생태계의 핵심 도구도 함께 설치된다.\n# Docker Desktop 설치 후 터미널에서 확인\ndocker --version\n# Docker version 24.0.7, build afdd53b\n\n# 첫 번째 컨테이너 실행 테스트\ndocker run hello-world\n라이선스 정책은 사용 규모에 따라 달라진다. 개인 사용자, 교육 목적, 소규모 기업(직원 250명 미만, 연매출 1,000만 달러 미만)은 무료로 사용할 수 있고, 대기업 환경에서만 유료 구독이 필요하다. 리눅스 서버에서는 Docker Desktop 없이 Docker Engine만 설치하면 되므로 프로덕션 환경에서 라이선스 비용은 발생하지 않는다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>도커: 재현 환경</span>"
    ]
  },
  {
    "objectID": "docker_concept.html#sec-docker-hub",
    "href": "docker_concept.html#sec-docker-hub",
    "title": "27  도커: 재현 환경",
    "section": "\n27.5 Docker Hub",
    "text": "27.5 Docker Hub\nGitHub이 소스 코드를 저장하고 공유하는 플랫폼이라면, 도커 허브(Docker Hub)는 도커 이미지를 저장하고 공유하는 플랫폼이다. 개발자가 GitHub에서 git clone으로 코드를 받듯, Docker Hub에서 docker pull로 이미지를 받는다. 프로젝트의 완전한 재현을 위해서는 코드(GitHub)와 실행 환경(Docker Hub) 둘 다 필요하다.\n\n\n\n\n\n그림 27.6: Docker Hub 구조\n\n\n그림 27.6 는 Docker Hub의 구조를 보여준다. 로컬에서 빌드한 이미지를 docker push로 업로드하면, 서버나 동료가 docker pull로 동일한 이미지를 다운로드한다. Docker Hub에는 rocker/rstudio, python, postgres 같은 공식 이미지와 사용자가 만든 커스텀 이미지가 함께 존재한다.\n버전 태그 시스템은 재현가능성의 핵심이다. rocker/rstudio:4.3.0처럼 특정 버전을 명시하면 5년 후에도 동일한 환경을 보장받는다. 반면 latest 태그는 최신 버전을 가리키므로 시간이 지나면 내용이 바뀔 수 있다. 재현성이 중요한 프로젝트에서는 반드시 특정 버전 태그를 사용해야 한다.\n# Docker Hub에서 이미지 다운로드\ndocker pull rocker/rstudio:4.3.0\n\n# 내 이미지를 Docker Hub에 업로드\ndocker push myname/myimage:1.0",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>도커: 재현 환경</span>"
    ]
  },
  {
    "objectID": "docker_concept.html#sec-docker-rocker",
    "href": "docker_concept.html#sec-docker-rocker",
    "title": "27  도커: 재현 환경",
    "section": "\n27.6 Rocker 프로젝트",
    "text": "27.6 Rocker 프로젝트\nR 환경 설정은 악명 높다. 운영체제마다 다른 설치 방법, 시스템 라이브러리 의존성, 패키지 버전 충돌까지 초보자는 물론 숙련된 개발자도 시간을 허비한다. 특히 sf, terra 같은 지리공간 패키지나 rJava 같은 시스템 의존 패키지는 설치 자체가 하루 일과가 되기도 한다.\nRocker 프로젝트는 이 문제를 해결하기 위해 2014년 Carl Boettiger와 Dirk Eddelbuettel이 시작했다. R 커뮤니티를 위한 공식 도커 이미지를 제공하며, Docker Hub에서 가장 많이 다운로드되는 R 이미지 시리즈다. docker pull rocker/rstudio 한 줄이면 RStudio가 포함된 완전한 R 개발 환경이 준비된다. 시스템 라이브러리 설치, 의존성 해결, 버전 충돌 걱정 없이 바로 분석을 시작할 수 있다.\n\n\n\n\n\n그림 27.7: Rocker 프로젝트 이미지\n\n\n그림 27.7 는 Rocker 이미지 계층 구조를 보여준다. 가장 기본이 되는 rocker/r-ver는 Ubuntu LTS와 R만 포함한 최소 이미지로, R 스크립트 실행에 적합하다. 여기에 RStudio Server를 추가한 rocker/rstudio는 웹 브라우저에서 8787 포트로 접속하는 IDE 환경을 제공한다. 반면 rocker/shiny는 Shiny Server를 포함해 3838 포트에서 앱을 배포한다.\n데이터 분석 작업에는 rocker/tidyverse가 적합하다. tidyverse와 devtools가 사전 설치되어 패키지 설치 시간을 절약한다. 논문이나 보고서 출판이 필요하다면 rocker/verse를 선택한다. Quarto, LaTeX, 폰트가 모두 포함되어 PDF 출력까지 한 번에 해결한다. 머신러닝 작업에는 rocker/ml, 지리공간 분석에는 rocker/geospatial처럼 특화된 이미지도 제공된다.\n모든 Rocker 이미지는 R 버전별 태그를 지원한다. rocker/tidyverse:4.5.0은 R 4.5.0 환경을, rocker/tidyverse:4.3.0은 R 4.3.0 환경을 제공한다. 버전을 명시하지 않으면 latest 태그가 적용되어 최신 버전이 설치된다. 재현성이 중요한 프로젝트에서는 반드시 특정 버전 태그를 사용해야 한다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>도커: 재현 환경</span>"
    ]
  },
  {
    "objectID": "docker_concept.html#sec-dockerfile",
    "href": "docker_concept.html#sec-dockerfile",
    "title": "27  도커: 재현 환경",
    "section": "\n27.7 Dockerfile",
    "text": "27.7 Dockerfile\nRocker 이미지는 훌륭한 출발점이지만, 실제 프로젝트에서는 추가 패키지나 설정이 필요하다. Dockerfile은 이미지를 만드는 “레시피”다. 어떤 베이스 이미지에서 시작할지, 어떤 패키지를 설치할지, 어떤 파일을 복사할지 순서대로 기술한다. 요리 레시피처럼 누구나 같은 Dockerfile로 동일한 이미지를 빌드할 수 있다.\n\n\n\n\n\n그림 27.8: Dockerfile 워크플로우\n\n\n그림 27.8 는 Dockerfile에서 커스텀 이미지가 만들어지는 과정을 보여준다. Dockerfile의 각 명령어가 하나의 레이어로 변환되고, 레이어들이 쌓여 최종 이미지가 완성된다. 레이어는 캐시되므로, 코드만 수정했다면 OS와 패키지 레이어는 재사용되어 빌드 시간이 단축된다.\nDockerfile의 핵심 명령어는 다섯 가지다. FROM은 베이스 이미지를 지정한다. RUN은 셸 명령어를 실행해 패키지를 설치하거나 설정을 변경한다. COPY는 로컬 파일을 이미지 안으로 복사한다. WORKDIR은 작업 디렉토리를 설정하고, CMD는 컨테이너 시작 시 실행할 기본 명령을 정의한다.\n# Dockerfile 예시: 데이터 분석 환경\nFROM rocker/rstudio:4.3.0\n\n# 시스템 라이브러리 설치 (sf, terra 패키지용)\nRUN apt-get update && apt-get install -y \\\n    libgdal-dev \\\n    libgeos-dev \\\n    libproj-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\n# R 패키지 설치\nRUN R -e \"install.packages(c('tidyverse', 'sf', 'terra', 'gt'), repos='https://cloud.r-project.org/')\"\n\n# 프로젝트 파일 복사\nCOPY . /home/rstudio/project\n\n# 작업 디렉토리 설정\nWORKDIR /home/rstudio/project\n\n# RStudio 포트 노출\nEXPOSE 8787\n이미지를 빌드하려면 Dockerfile이 있는 디렉토리에서 docker build 명령을 실행한다. -t 옵션으로 이미지 이름과 태그를 지정한다.\n# 이미지 빌드\ndocker build -t myproject:1.0 .\n\n# 빌드한 이미지 확인\ndocker images\n\n# 컨테이너 실행\ndocker run -d -p 8787:8787 -e PASSWORD=rstudio myproject:1.0\n\n# 브라우저에서 localhost:8787 접속\n포트 매핑(-p)은 컨테이너 내부 포트를 호스트에 노출한다. -p 8787:8787에서 콜론 왼쪽은 호스트 포트, 오른쪽은 컨테이너 포트다. RStudio Server가 컨테이너 내부에서 8787 포트로 실행되고, 이를 호스트의 8787 포트에 연결한다. 호스트 포트를 8888로 바꾸면(-p 8888:8787) 브라우저에서 localhost:8888로 접속한다. 환경 변수(-e)는 컨테이너 실행 시 설정값을 전달한다. Rocker 이미지에서 -e PASSWORD=rstudio는 RStudio 로그인 비밀번호를 지정한다. -e DISABLE_AUTH=true로 인증을 끄거나, -e ROOT=true로 sudo 권한을 부여하는 것도 가능하다.\nDockerfile을 Git으로 버전 관리하면 환경 설정 변경 이력이 남는다. 코드와 함께 Dockerfile을 공유하면 동료는 docker build 한 줄로 동일한 개발 환경을 구축한다. 6개월 후 논문 심사위원이 재현을 요청해도, Dockerfile과 코드만 있으면 정확히 같은 환경에서 분석을 실행할 수 있다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>도커: 재현 환경</span>"
    ]
  },
  {
    "objectID": "docker_concept.html#sec-docker-volumes",
    "href": "docker_concept.html#sec-docker-volumes",
    "title": "27  도커: 재현 환경",
    "section": "\n27.8 데이터와 볼륨",
    "text": "27.8 데이터와 볼륨\n컨테이너는 격리된 환경이라는 장점이 있지만, 이 격리가 데이터 처리에서는 문제가 된다. 컨테이너 내부에서 생성한 파일은 컨테이너를 삭제하면 함께 사라진다. 밤새 돌린 분석 결과가 docker rm 한 줄로 증발하는 것이다. 데이터 과학자에게 이보다 끔찍한 시나리오는 없다.\n볼륨(Volume)과 바인드 마운트(Bind Mount)는 이 문제를 해결한다. 호스트 시스템의 폴더를 컨테이너 내부에 “연결”하면, 컨테이너에서 생성한 파일이 호스트에 직접 저장된다. 컨테이너가 삭제되어도 데이터는 호스트에 그대로 남는다. 반대로 호스트의 데이터 파일을 컨테이너에서 불러와 분석하는 것도 가능하다.\n\n\n\n\n\n그림 27.9: 도커 볼륨과 바인드 마운트\n\n\n그림 27.9 는 호스트와 컨테이너 간 데이터 흐름을 보여준다. -v 옵션으로 호스트 경로와 컨테이너 경로를 연결하면, 양쪽에서 동일한 파일에 접근할 수 있다. 바인드 마운트는 호스트의 특정 폴더를 직접 연결하고, 네임드 볼륨은 도커가 관리하는 저장소를 사용한다. 읽기 전용(:ro) 옵션은 원본 데이터를 실수로 덮어쓰는 것을 방지한다.\n# 바인드 마운트: 호스트 폴더를 컨테이너에 연결\ndocker run -d -p 8787:8787 \\\n  -v /Users/me/project:/home/rstudio/project \\\n  -e PASSWORD=rstudio \\\n  rocker/rstudio:4.3.0\n\n# 컨테이너 내부에서 /home/rstudio/project 접근 가능\n# 분석 결과를 이 경로에 저장하면 호스트에도 저장됨\n데이터 분석 워크플로우에서는 프로젝트 폴더 전체를 마운트하는 것이 일반적이다. data/ 폴더에 원본 데이터를 두고, scripts/ 폴더에 분석 코드를, output/ 폴더에 결과물을 저장한다. 컨테이너 안에서 RStudio로 작업하면 모든 변경사항이 호스트에 실시간으로 반영된다. Git으로 버전 관리도 가능하고, 컨테이너를 삭제했다가 다시 만들어도 데이터는 그대로다.\n# 실전 예시: 현재 디렉토리를 프로젝트 폴더로 마운트\ndocker run -d -p 8787:8787 \\\n  -v $(pwd):/home/rstudio/project \\\n  -e PASSWORD=rstudio \\\n  rocker/tidyverse:4.3.0\n\n# $(pwd): 현재 작업 디렉토리\n# 프로젝트 내 data/, scripts/, output/ 폴더 모두 접근 가능\n대용량 데이터를 다룰 때는 네임드 볼륨이 유용하다. docker volume create 명령으로 볼륨을 생성하면 도커가 최적화된 저장소를 관리한다. 여러 컨테이너가 같은 볼륨을 공유할 수도 있어, R 컨테이너에서 전처리한 데이터를 Python 컨테이너에서 분석하는 파이프라인도 구성할 수 있다.\n# 네임드 볼륨 생성\ndocker volume create analysis_data\n\n# R 컨테이너에서 데이터 전처리\ndocker run -v analysis_data:/data rocker/tidyverse:4.3.0 \\\n  Rscript -e \"saveRDS(mtcars, '/data/processed.rds')\"\n\n# Python 컨테이너에서 동일 데이터 접근\ndocker run -v analysis_data:/data python:3.11 \\\n  python -c \"import pyreadr; df = pyreadr.read_r('/data/processed.rds')\"\n볼륨을 사용할 때 한 가지 주의할 점은 파일 권한이다. 컨테이너 내부 사용자(보통 rstudio 또는 root)와 호스트 사용자의 UID가 다르면 권한 문제가 발생할 수 있다. Rocker 이미지는 이 문제를 대부분 자동 처리하지만, 권한 오류가 발생하면 --user 옵션으로 UID를 맞추거나 호스트에서 폴더 권한을 조정해야 한다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>도커: 재현 환경</span>"
    ]
  },
  {
    "objectID": "docker_concept.html#sec-docker-vs-alternatives",
    "href": "docker_concept.html#sec-docker-vs-alternatives",
    "title": "27  도커: 재현 환경",
    "section": "\n27.9 도커 vs 다른 격리 도구",
    "text": "27.9 도커 vs 다른 격리 도구\n환경 격리 도구는 크게 세 가지 계층으로 나뉜다. 가장 가벼운 것은 패키지 수준 격리다. Python의 uv나 R의 renv가 여기에 해당한다. 프로젝트별로 패키지 버전을 고정하고, requirements.txt나 renv.lock 파일로 의존성을 기록한다. 설정이 간단하고 빠르지만, 시스템 라이브러리나 OS에 의존하는 패키지는 여전히 문제가 된다. sf 패키지가 GDAL 버전에 따라 다르게 동작하거나, macOS와 Linux에서 결과가 달라지는 상황은 패키지 격리만으로 해결할 수 없다.\n한 단계 위가 런타임 수준 격리다. pyenv나 rig 같은 도구로 Python이나 R 버전 자체를 프로젝트별로 전환한다. 패키지뿐 아니라 언어 런타임까지 격리되므로 R 4.2와 R 4.3을 오가며 작업할 수 있다. 하지만 여전히 시스템 라이브러리에는 손을 대지 못한다. Ubuntu에서 만든 코드가 macOS에서 실패하는 문제는 그대로다.\nOS 수준 격리가 도커의 영역이다. 운영체제, 시스템 라이브러리, 런타임, 패키지를 모두 포함한 완전한 환경을 패키징한다. Ubuntu 20.04 + GDAL 3.4 + R 4.3.0 + sf 1.0 조합을 그대로 보존하고 어디서든 재현한다. 초기 학습 곡선이 있지만, 한 번 익히면 환경 문제로 고생할 일이 사라진다.\n어떤 도구를 선택할지는 상황에 따라 다르다. 순수 R/Python 패키지만 사용하는 로컬 분석이라면 renv나 uv로 충분하다. 하지만 GDAL, PostgreSQL, OpenCV 같은 시스템 라이브러리에 의존하거나, R과 Python을 함께 사용하거나, Shiny 앱이나 API를 프로덕션에 배포하거나, 5년 후에도 재현을 보장해야 한다면 도커가 답이다. 일반적인 경로는 renv/uv로 시작해 시스템 의존성이나 배포 요구가 생기면 도커로 전환하는 것이다.\n💡 생각해볼 점\n도커가 필요한 순간은 명확하다. 팀원이 “환경 설정이 안 돼요”라고 할 때, Shiny 앱을 서버에 배포할 때, 1년 전 분석을 재현해야 할 때, OpenCV나 GDAL 같은 시스템 라이브러리가 필요할 때, 논문 제출 시 재현성을 증명해야 할 때다. 이런 상황이 오면 도커를 시작하자.\n이미지와 컨테이너 구분은 도커 이해의 핵심이다. 이미지는 설계도(Class)이고, 컨테이너는 실행 인스턴스(Object)다. docker pull로 이미지를 받고, docker run으로 컨테이너를 실행한다. 하나의 이미지에서 여러 컨테이너를 만들 수 있고, 컨테이너를 삭제해도 이미지는 남는다. Rocker 이미지(rocker/rstudio, rocker/tidyverse, rocker/verse)는 R 사용자의 출발점이다. 버전 태그(:4.3.0)를 명시하면 5년 후에도 동일한 환경을 보장받는다.\n볼륨(-v)은 데이터 영속성의 열쇠다. 컨테이너는 삭제되면 내부 데이터도 사라지지만, 호스트 폴더를 마운트하면 분석 결과가 호스트에 남는다. 밤새 돌린 분석이 docker rm 한 줄로 증발하는 비극을 막으려면 반드시 볼륨을 사용해야 한다.\n도커 학습 곡선은 가파르다. 하지만 한 번 익히면 환경 문제로 고생하는 시간이 사라진다. “내 컴퓨터에서는 되는데”라는 말을 더 이상 하지 않게 된다. 다음 장에서는 이론을 넘어 실전으로 들어간다. Docker Desktop 설치, rocker/rstudio 실행, Dockerfile 작성, 이미지 빌드까지 전 과정을 직접 수행한다. 첫 번째 컨테이너를 만들고, 웹브라우저에서 localhost:8787을 열어 RStudio가 뜨는 것을 확인하면, 도커의 가치를 체감하게 된다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>도커: 재현 환경</span>"
    ]
  },
  {
    "objectID": "make_concept.html",
    "href": "make_concept.html",
    "title": "28  Make: 자동화",
    "section": "",
    "text": "28.1 셸 스크립트의 한계\n연구팀 김 박사는 논문 마감을 앞두고 있었다. 지도교수가 “그래프 폰트를 12pt에서 14pt로 바꿔주세요”라고 요청했다. 단순한 수정이었다. 시각화 스크립트를 고치고, 그래프 5개를 다시 생성하고, 결과 테이블을 업데이트하고, 최종 PDF를 렌더링해야 했다. 김 박사는 터미널에 명령어를 하나씩 입력했다.\n30분 후, 지도교수가 다시 메일을 보냈다. “아, 그리고 색상도 컬러에서 흑백으로 바꿔주세요.” 김 박사는 같은 명령어를 다시 입력했다. 이번에는 plot_figure3.py를 빼먹었다. 최종 PDF에 컬러 그래프가 섞여 있었다. 심사위원 지적으로 논문이 반려되었다.\n이것이 수동 워크플로우의 함정이다. 데이터 분석 파이프라인은 여러 단계로 구성된다. 원시 데이터를 처리하고, 통계를 계산하고, 그래프를 그리고, 보고서를 생성한다. 각 단계는 이전 단계의 출력에 의존한다. 수동으로 관리하면 실수가 생기고, 어떤 파일을 다시 만들어야 하는지 기억해야 하며, 전체 파이프라인을 처음부터 다시 실행하느라 시간을 낭비한다.\nMake는 이 문제를 1977년부터 해결해왔다. 파일 간 의존성을 명시적으로 기록하고, 변경된 파일만 자동으로 다시 빌드한다. 시각화 스크립트를 수정하면 그래프만 다시 생성하고, 원시 데이터가 바뀌면 전체 파이프라인이 자동으로 재실행된다. 50년이 지난 지금도 Make는 재현가능한 연구의 핵심 도구다.\n가장 먼저 떠오르는 해결책은 셸 스크립트다. 모든 명령어를 파일에 저장하고 bash run_pipeline.sh 한 줄로 실행한다. 파이프라인을 문서화하고, 타이핑 실수를 방지하며, 재현성을 높인다.\n하지만 셸 스크립트에는 치명적인 한계가 있다. 항상 처음부터 끝까지 전부 실행된다. 시각화 스크립트만 수정했는데 전처리부터 다시 돌려야 한다. 대용량 데이터 처리에 1시간이 걸린다면? 그래프 색상 하나 바꾸는 데 1시간을 기다려야 한다.\n주석 처리로 해결할 수 있다고 생각할 수 있다.\n이 방법은 오류의 온상이다. 어떤 줄을 주석 처리했는지 기억해야 하고, 원시 데이터가 바뀌면 주석을 다시 풀어야 한다. 복잡한 파이프라인에서 의존성을 머릿속으로 추적하다 보면 결국 실수가 생긴다. 김 박사처럼 중간 단계를 빼먹거나, 불필요한 단계를 다시 실행하거나, 잘못된 순서로 실행하게 된다.\nMake는 이 문제를 우아하게 해결한다. 파일 간 의존성(dependency)을 명시적으로 선언하고, 타임스탬프를 비교해 변경된 파일만 다시 빌드한다. “무엇이 무엇에 의존하는가”를 코드로 기록하면, Make가 “무엇을 다시 만들어야 하는가”를 자동으로 판단한다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Make: 자동화</span>"
    ]
  },
  {
    "objectID": "make_concept.html#sec-make-shell-limits",
    "href": "make_concept.html#sec-make-shell-limits",
    "title": "28  Make: 자동화",
    "section": "",
    "text": "#!/bin/bash\n# run_pipeline.sh - 분석 파이프라인\n\npython preprocess.py data/raw.csv data/clean.csv\npython analyze.py data/clean.csv results/stats.csv\npython visualize.py results/stats.csv figures/plot.png\nquarto render report.qmd\n\n\n#!/bin/bash\n# 이미 실행했으므로 주석 처리\n# python preprocess.py data/raw.csv data/clean.csv\n# python analyze.py data/clean.csv results/stats.csv\n\npython visualize.py results/stats.csv figures/plot.png\nquarto render report.qmd",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Make: 자동화</span>"
    ]
  },
  {
    "objectID": "make_concept.html#sec-make-structure",
    "href": "make_concept.html#sec-make-structure",
    "title": "28  Make: 자동화",
    "section": "28.2 Makefile 기본 구조",
    "text": "28.2 Makefile 기본 구조\nMake는 Makefile이라는 파일에서 명령을 읽는다. Makefile은 규칙(rule)들로 구성된다. 각 규칙은 타겟, 의존성, 액션 세 부분으로 이루어진다.\n# 기본 규칙 구조\n타겟: 의존성1 의존성2\n    액션 (TAB으로 들여쓰기)\n\n\n\n\n\n\n그림 28.1: Makefile 규칙 구조\n\n\n\n그림 28.1 은 Makefile 규칙의 세 구성 요소를 보여준다. 타겟(target)은 만들어질 파일이다. 의존성(dependency)은 타겟을 만드는 데 필요한 파일들이다. 액션(action)은 의존성에서 타겟을 만드는 명령어다. 액션은 반드시 TAB 문자로 들여써야 한다. 스페이스 8개가 아니라 TAB 키 한 번이다. 1977년의 유산이지만, 지금도 지켜야 하는 규칙이다.\n실제 예시를 보자. 텍스트 파일에서 단어 빈도를 계산하는 파이프라인이다.\n# 단어 빈도 계산\nresults.dat : books/novel.txt\n    python countwords.py books/novel.txt results.dat\n이 규칙의 의미는 명확하다. results.dat 파일을 만들려면 books/novel.txt 파일이 필요하고, python countwords.py 명령으로 생성한다. Make를 실행하면 다음과 같이 동작한다.\n$ make results.dat\npython countwords.py books/novel.txt results.dat\n이제 같은 명령을 다시 실행해보자.\n$ make results.dat\nmake: 'results.dat' is up to date.\nMake는 results.dat와 books/novel.txt의 타임스탬프를 비교한다. 타겟이 의존성보다 최신이면 아무것도 하지 않는다. 이것이 증분 빌드(incremental build)다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Make: 자동화</span>"
    ]
  },
  {
    "objectID": "make_concept.html#sec-make-timestamp",
    "href": "make_concept.html#sec-make-timestamp",
    "title": "28  Make: 자동화",
    "section": "28.3 타임스탬프 기반 빌드",
    "text": "28.3 타임스탬프 기반 빌드\nMake의 핵심 원리는 단순하다. 타겟 파일이 존재하지 않거나, 의존성 파일 중 하나라도 타겟보다 최신이면 액션을 실행한다. 파일의 “최신 여부”는 파일시스템 타임스탬프로 판단한다.\n\n\n\n\n\n\n그림 28.2: Make 타임스탬프 비교\n\n\n\n그림 28.2 는 Make가 빌드 여부를 결정하는 과정을 보여준다. 왼쪽은 타겟이 의존성보다 최신인 경우로, Make는 “이미 최신”이라고 판단하고 액션을 건너뛴다. 오른쪽은 의존성이 타겟보다 최신인 경우로, 타겟을 다시 빌드한다.\ntouch 명령으로 파일 타임스탬프를 갱신해 이 동작을 확인할 수 있다.\n# 의존성 파일의 타임스탬프 갱신 (내용은 그대로)\n$ touch books/novel.txt\n\n# 이제 Make가 다시 빌드\n$ make results.dat\npython countwords.py books/novel.txt results.dat\n파일 내용이 바뀌지 않았더라도 타임스탬프가 갱신되면 Make는 다시 빌드한다. Make는 파일 내용을 비교하지 않고 오직 타임스탬프만 본다. 이 방식은 단순하지만 효과적이다. Git처럼 내용 해시를 계산하는 빌드 도구도 있지만, 대부분의 경우 타임스탬프 비교로 충분하다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Make: 자동화</span>"
    ]
  },
  {
    "objectID": "make_concept.html#sec-make-chain",
    "href": "make_concept.html#sec-make-chain",
    "title": "28  Make: 자동화",
    "section": "28.4 의존성 체인",
    "text": "28.4 의존성 체인\n실제 데이터 분석 파이프라인은 여러 단계로 구성된다. 원시 데이터에서 정제 데이터를 만들고, 정제 데이터에서 통계를 계산하고, 통계에서 그래프를 그린다. 각 단계의 출력이 다음 단계의 입력이 된다.\n# 다단계 파이프라인\nclean.csv : raw.csv preprocess.py\n    python preprocess.py raw.csv clean.csv\n\nstats.csv : clean.csv analyze.py\n    python analyze.py clean.csv stats.csv\n\nfigure.png : stats.csv visualize.py\n    python visualize.py stats.csv figure.png\n\n\n\n\n\n\n그림 28.3: Make 의존성 체인\n\n\n\n그림 28.3 은 파일 간 의존성이 체인을 형성하는 모습을 보여준다. figure.png를 만들려면 stats.csv가 필요하고, stats.csv를 만들려면 clean.csv가 필요하다. Make는 이 의존성 그래프를 분석해 필요한 파일만 순서대로 빌드한다.\nMake의 강력함은 의존성의 전이성(transitivity)에 있다. raw.csv를 수정하면 어떻게 될까?\n$ touch raw.csv\n$ make figure.png\npython preprocess.py raw.csv clean.csv\npython analyze.py clean.csv stats.csv\npython visualize.py stats.csv figure.png\nMake는 raw.csv → clean.csv → stats.csv → figure.png 순으로 전체 체인을 자동 재빌드한다. 반면 visualize.py만 수정하면?\n$ touch visualize.py\n$ make figure.png\npython visualize.py stats.csv figure.png\nclean.csv와 stats.csv는 건드리지 않고 figure.png만 다시 생성한다. 데이터 처리에 1시간이 걸려도 시각화 스크립트 수정은 몇 초면 반영된다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Make: 자동화</span>"
    ]
  },
  {
    "objectID": "make_concept.html#sec-make-auto-vars",
    "href": "make_concept.html#sec-make-auto-vars",
    "title": "28  Make: 자동화",
    "section": "28.5 자동 변수",
    "text": "28.5 자동 변수\nMakefile에 중복이 많으면 유지보수가 어렵다. 파일명을 바꿀 때마다 여러 곳을 수정해야 한다. Make는 자동 변수(automatic variable)로 이 문제를 해결한다.\n# 중복이 많은 버전\nresults.dat : books/novel.txt\n    python countwords.py books/novel.txt results.dat\n\n# 자동 변수 사용\nresults.dat : books/novel.txt\n    python countwords.py $&lt; $@\n세 가지 자동 변수를 기억하자.\n\n$@: 현재 규칙의 타겟\n$&lt;: 첫 번째 의존성\n$^: 모든 의존성\n\n# 여러 의존성이 있는 경우\nreport.pdf : chapter1.md chapter2.md chapter3.md style.css\n    pandoc $^ -o $@\n# $^ = chapter1.md chapter2.md chapter3.md style.css\n# $@ = report.pdf\n자동 변수는 패턴 규칙(pattern rule)과 함께 사용할 때 진가를 발휘한다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Make: 자동화</span>"
    ]
  },
  {
    "objectID": "make_concept.html#sec-make-patterns",
    "href": "make_concept.html#sec-make-patterns",
    "title": "28  Make: 자동화",
    "section": "28.6 패턴 규칙",
    "text": "28.6 패턴 규칙\n여러 파일에 같은 처리를 적용해야 할 때, 규칙을 일일이 작성하면 지루하다.\n# 반복적인 규칙\nisles.dat : books/isles.txt countwords.py\n    python countwords.py books/isles.txt isles.dat\n\nabyss.dat : books/abyss.txt countwords.py\n    python countwords.py books/abyss.txt abyss.dat\n\nlast.dat : books/last.txt countwords.py\n    python countwords.py books/last.txt last.dat\n패턴 규칙은 % 와일드카드로 이 중복을 제거한다.\n# 패턴 규칙\n%.dat : books/%.txt countwords.py\n    python countwords.py $&lt; $@\n%는 어떤 문자열과도 매칭된다. isles.dat을 만들 때 %는 isles와 매칭되고, 의존성에서도 같은 값으로 치환된다. 하나의 규칙으로 isles.dat, abyss.dat, last.dat 등 모든 .dat 파일을 생성할 수 있다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Make: 자동화</span>"
    ]
  },
  {
    "objectID": "make_concept.html#sec-make-variables",
    "href": "make_concept.html#sec-make-variables",
    "title": "28  Make: 자동화",
    "section": "28.7 변수와 설정 분리",
    "text": "28.7 변수와 설정 분리\n스크립트 이름이나 실행 옵션이 Makefile 곳곳에 흩어져 있으면 변경이 어렵다. 변수를 사용해 설정을 한 곳에 모은다.\n# 변수 정의\nPYTHON = python3\nCOUNT_SCRIPT = countwords.py\nANALYZE_SCRIPT = testzipf.py\n\n# 변수 사용 - $(변수명)\n%.dat : books/%.txt $(COUNT_SCRIPT)\n    $(PYTHON) $&lt; $@\n\nresults.txt : isles.dat abyss.dat $(ANALYZE_SCRIPT)\n    $(PYTHON) $(ANALYZE_SCRIPT) $^ &gt; $@\n설정을 별도 파일로 분리하면 더 깔끔하다.\n# config.mk\nPYTHON = python3\nCOUNT_SCRIPT = countwords.py\nDATA_DIR = data\nOUTPUT_DIR = results\n# Makefile\ninclude config.mk\n\n%.dat : $(DATA_DIR)/%.txt $(COUNT_SCRIPT)\n    $(PYTHON) $(COUNT_SCRIPT) $&lt; $@\n환경에 따라 config.mk만 바꾸면 된다. 로컬에서는 PYTHON = python3, 서버에서는 PYTHON = /opt/python/3.11/bin/python처럼 설정할 수 있다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Make: 자동화</span>"
    ]
  },
  {
    "objectID": "make_concept.html#sec-make-phony",
    "href": "make_concept.html#sec-make-phony",
    "title": "28  Make: 자동화",
    "section": "28.8 Phony 타겟",
    "text": "28.8 Phony 타겟\n지금까지 타겟은 모두 실제 파일이었다. 하지만 “모든 출력 삭제”나 “전체 빌드” 같은 작업도 Make로 자동화하고 싶다. 이때 사용하는 것이 phony 타겟이다.\n# 실제 파일을 만들지 않는 타겟\n.PHONY : clean all\n\nclean :\n    rm -f *.dat results.txt\n\nall : isles.dat abyss.dat last.dat results.txt\n.PHONY로 선언된 타겟은 파일이 아니라 “명령의 이름”이다. make clean을 실행하면 clean이라는 파일이 있든 없든 항상 액션을 실행한다. .PHONY를 선언하지 않으면, clean이라는 파일이나 디렉토리가 있을 때 Make가 “이미 최신”이라고 판단해 아무것도 하지 않는다.\n관례적으로 사용되는 phony 타겟들이 있다.\n\nall: 기본 타겟, Makefile의 첫 번째 규칙으로 배치\nclean: 생성된 파일 삭제\ninstall: 빌드 결과물 설치\ntest: 테스트 실행\n\n.PHONY : all clean test\n\n# 'make'만 실행하면 all이 기본 타겟\nall : results.txt figures/plot.png\n\nclean :\n    rm -f *.dat results.txt\n    rm -f figures/*.png\n\ntest :\n    python -m pytest tests/",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Make: 자동화</span>"
    ]
  },
  {
    "objectID": "make_concept.html#sec-make-workflow",
    "href": "make_concept.html#sec-make-workflow",
    "title": "28  Make: 자동화",
    "section": "28.9 데이터 과학 워크플로우 예시",
    "text": "28.9 데이터 과학 워크플로우 예시\n지금까지 배운 개념을 종합해 실제 데이터 분석 파이프라인을 구성해보자. Zipf의 법칙을 검증하는 프로젝트다. 여러 책의 단어 빈도를 계산하고, 결과를 시각화하고, 최종 보고서를 생성한다.\n# config.mk\nPYTHON = python\nBOOKS = isles abyss last sierra\n\n# 스크립트\nCOUNT_SCRIPT = countwords.py\nPLOT_SCRIPT = plotcounts.py\nZIPF_SCRIPT = testzipf.py\n# Makefile\ninclude config.mk\n\n# 파일 목록 자동 생성\nDAT_FILES = $(patsubst %, %.dat, $(BOOKS))\nPNG_FILES = $(patsubst %, figures/%.png, $(BOOKS))\n\n.PHONY : all clean\n\n# 기본 타겟\nall : results.txt $(PNG_FILES)\n\n# 패턴 규칙: 단어 빈도 계산\n%.dat : books/%.txt $(COUNT_SCRIPT)\n    $(PYTHON) $(COUNT_SCRIPT) $&lt; $@\n\n# 패턴 규칙: 그래프 생성\nfigures/%.png : %.dat $(PLOT_SCRIPT)\n    $(PYTHON) $(PLOT_SCRIPT) $&lt; $@\n\n# 결과 테이블 생성\nresults.txt : $(ZIPF_SCRIPT) $(DAT_FILES)\n    $(PYTHON) $^ &gt; $@\n\n# 정리\nclean :\n    rm -f *.dat results.txt\n    rm -f figures/*.png\n\n\n\n\n\n\n그림 28.4: 데이터 과학 Make 워크플로우\n\n\n\n그림 28.4 는 완성된 파이프라인의 의존성 그래프다. 원시 텍스트에서 최종 결과물까지의 모든 단계가 명시적으로 기록되어 있다. 새로운 책을 추가하려면 config.mk의 BOOKS 변수에 이름만 추가하면 된다. make all 한 줄로 전체 분석이 재현된다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Make: 자동화</span>"
    ]
  },
  {
    "objectID": "make_concept.html#sec-make-alternatives",
    "href": "make_concept.html#sec-make-alternatives",
    "title": "28  Make: 자동화",
    "section": "28.10 Make vs 현대 도구들",
    "text": "28.10 Make vs 현대 도구들\nMake는 1977년에 만들어졌다. 거의 50년이 지난 지금, 왜 아직도 Make를 사용하는가? 더 현대적인 도구들이 있지 않은가?\nSnakemake는 Python 기반 워크플로우 도구로, Python 문법으로 규칙을 작성한다. 클러스터 제출, 클라우드 실행, 컨테이너 통합을 지원한다. 바이오인포매틱스 분야에서 특히 인기 있다. targets는 R 전용 워크플로우 도구로, R 객체 수준에서 의존성을 추적한다. R 사용자에게 가장 자연스러운 선택이다. DVC는 Git처럼 데이터와 모델 버전을 관리하면서 파이프라인도 정의할 수 있다. 머신러닝 프로젝트에 적합하다.\nMake를 선택해야 하는 상황이 있다. 언어에 구애받지 않는 파이프라인이 필요할 때다. Python과 R과 셸 스크립트가 섞인 프로젝트에서 Make는 중립적인 조율자 역할을 한다. 레거시 시스템과 통합해야 할 때도 Make가 유리하다. C/Fortran으로 작성된 고성능 컴퓨팅 코드는 대부분 Make로 빌드한다. 단순함이 필요할 때 Make의 가치가 드러난다. 의존성 파일 몇 개에 규칙 몇 줄이면 충분한데 Snakemake나 DVC를 도입하는 것은 과잉이다.\n\n\n\n\n\n\n노트데이터 과학 워크플로우 도구 선택\n\n\n\n\n\n\n상황\n권장 도구\n\n\n\n\n간단한 파이프라인, 다국어 혼용\nMake\n\n\nR 중심 분석, 함수 수준 캐싱\ntargets (R)\n\n\nPython 중심, 클러스터/클라우드\nSnakemake\n\n\nML 모델 버전 관리 필요\nDVC\n\n\n복잡한 DAG, 재시도/에러 처리\nAirflow, Prefect\n\n\n\nMake는 “가장 작은 도구로 문제를 해결”하는 유닉스 철학을 따른다. 복잡한 요구사항이 없다면 Make로 시작하고, 필요할 때 전환해도 늦지 않다.\n\n\n💡 생각해볼 점\nMake의 핵심은 의존성의 명시적 선언이다. 어떤 파일이 어떤 파일에서 만들어지는지 Makefile에 기록하면, Make가 변경 사항을 추적하고 필요한 부분만 다시 빌드한다. 셸 스크립트처럼 “무엇을 실행하는가”만 기록하는 것이 아니라, “무엇이 무엇에 의존하는가”를 기록한다. 이 작은 차이가 재현성과 효율성의 큰 차이를 만든다.\n규칙의 구조는 단순하다. 타겟: 의존성 다음 줄에 TAB으로 들여쓴 액션. 자동 변수($@, $&lt;, $^)로 중복을 줄이고, 패턴 규칙(%)으로 여러 파일에 같은 처리를 적용한다. 변수로 설정을 분리하고, phony 타겟으로 clean이나 all 같은 유틸리티 명령을 정의한다.\nMake는 50년 가까이 살아남았다. C 컴파일러부터 데이터 분석 파이프라인까지, 의존성 기반 워크플로우가 필요한 곳에서 여전히 유효하다. 학습 곡선이 가파르지 않고, 대부분의 유닉스 시스템에 기본 설치되어 있으며, 언어와 도구에 중립적이다. 더 현대적인 대안이 있더라도, Make는 “충분히 좋은” 선택이다. 다음 장에서는 Make를 직접 실습하며, 실제 데이터 분석 프로젝트에 적용하는 방법을 다룬다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Make: 자동화</span>"
    ]
  },
  {
    "objectID": "compendium.html",
    "href": "compendium.html",
    "title": "29  연구 컴펜디엄",
    "section": "",
    "text": "29.1 재현성 4가지 개념\n2024년 한 연구팀의 사례다. Nature 자매지에 게재 승인을 받은 논문에 대해 심사위원이 “코드와 데이터를 공개해주세요”라고 요청했다. 연구자는 당황했다. 코드는 GitHub에 있지만, 10개 스크립트가 어떤 순서로 실행되는지 본인도 기억나지 않았다. 데이터 파일 20개 중 어떤 것이 원본이고 어떤 것이 중간 결과물인지 구분이 안 됐다. 패키지 버전은 기록해두지 않았다. 결국 논문 게재가 3개월 지연되었고, 연구자는 자신의 코드를 다시 이해하는 데 2주를 소비했다.\n2016년 Nature 설문조사에서 연구자 70% 이상이 “다른 연구자의 실험을 재현하는 데 실패한 적이 있다”고 답했고, 50% 이상이 “자신의 실험조차 재현하지 못한 적이 있다”고 고백했다 [1]. 재현성 위기(reproducibility crisis)는 과학 전반에 걸친 구조적 문제다.\n재현성 위기의 배경에는 의심스러운 연구 관행(questionable research practices)이 있다 [2]. 2012년 설문조사에 따르면 심리학자의 50% 이상이 이러한 관행을 경험했다고 보고했다. 대표적인 예로 p-해킹(p-hacking)은 통계적으로 유의미한 결과(p &lt; 0.05)가 나올 때까지 데이터를 조작하는 행위이고 [3], HARKing(Hypothesizing After the Results are Known)은 결과를 보고 나서 가설을 사후에 설정하는 것이다 [4]. 선별 취사(cherry-picking)는 자신에게 유리한 데이터만 선택적으로 보고하는 관행이다. 투명한 연구 컴펜디엄은 이러한 관행을 어렵게 만들어 과학의 신뢰성을 높인다.\n연구 컴펜디엄(Research Compendium)1은 재현성 문제에 대한 체계적인 해결책 중 하나로 제시되었다 [5]. 핵심 목표는 “연구의 모든 디지털 구성요소—데이터, 코드, 텍스트—를 하나의 컨테이너에 담아 결과 재현을 간단하게 만드는 것”이다.\n재현성 연구의 역사는 1990년대 초로 거슬러 올라간다. 스탠퍼드 대학 지구물리학 연구실에서 처음 도입되었고 [6], 2004년 R 기반 연구 컴펜디엄의 이론적 토대가 마련되었다 [7]. 2018년에는 이러한 아이디어가 실용적인 R 패키지 기반 워크플로우로 구체화되었다 [5].\n미국 국립과학재단(NSF) 사회과학분야 자문위원회는 2015년 보고서 [8]에서 과학 연구의 핵심 개념을 명확히 정의했다. 재현성(reproducibility), 복제성(replicability), 강건성(robustness), 일반화(generalizability)는 서로 다른 차원의 검증을 의미한다.\n그림 29.1 는 데이터와 분석 방법의 조합에 따른 4가지 검증 수준을 보여준다. 재현성은 동일한 데이터에 동일한 분석을 수행하여 같은 결과를 얻는 것으로, NSF 보고서가 명시한 과학적 발견의 최소한의 필요 조건이다. 복제성은 다른 데이터에 동일한 분석을 적용하여 유사한 결과를 얻는지 검증한다. 강건성은 동일한 데이터를 다른 분석 방법(예: R과 Python)으로 처리해도 같은 결론에 도달하는지 확인한다. 일반화는 복제성과 강건성을 결합한 가장 높은 수준의 검증으로, 다른 데이터와 다른 분석 방법을 사용해도 유사한 결론을 얻는지 평가한다.\n연구 컴펜디엄은 재현성 연구 핵심 개념 중 재현성을 보장하는 도구다. 재현성 없이는 복제성도, 강건성도, 일반화도 확인할 수 없다. 따라서 연구 컴펜디엄은 과학적 발견의 토대를 만드는 작업이다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-concepts",
    "href": "compendium.html#sec-compendium-concepts",
    "title": "29  연구 컴펜디엄",
    "section": "",
    "text": "그림 29.1: 재현성 연구 4가지 핵심 개념",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-what",
    "href": "compendium.html#sec-compendium-what",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.2 연구 컴펜디엄이란",
    "text": "29.2 연구 컴펜디엄이란\n연구 컴펜디엄은 논문과 함께 제공되는 분석 코드, 데이터, 그리고 필요한 계산 환경을 담은 단일 컨테이너다. 단순히 코드를 공개하는 수준을 넘어, 누구나 동일한 결과를 얻을 수 있도록 모든 구성요소를 체계적으로 조직화한다.\n마윅 등 [5]은 연구 컴펜디엄의 세 가지 핵심 원칙을 제시했다. 첫째, 관례적 폴더 구조다. 파일을 표준화된 디렉토리 구조로 조직하면 처음 보는 사람도 프로젝트 구성을 쉽게 파악할 수 있다. 둘째, 데이터·방법·결과 분리다. 원본 데이터, 분석 코드, 생성된 결과물을 명확히 구분하면 어떤 파일이 입력이고 어떤 파일이 산출물인지 혼동하지 않는다. 셋째, 계산 환경 명세다. 사용된 소프트웨어 버전과 의존성을 기록하면 “내 컴퓨터에서는 되는데요”라는 문제를 방지할 수 있다.\n\n\n\n\n\n그림 29.2: 연구 컴펜디엄의 구조\n\n\n그림 29.2 는 연구 컴펜디엄의 기본 구조를 보여준다. 가장 단순한 형태는 data/, analysis/ 폴더와 README.md만으로 구성된다. 하지만 재현가능성을 완전히 보장하려면 Dockerfile로 계산 환경을, renv.lock으로 패키지 버전을, Makefile로 실행 순서를 명시해야 한다. 이 모든 구성요소가 Git으로 버전 관리되고, GitHub에서 공유된다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-why",
    "href": "compendium.html#sec-compendium-why",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.3 왜 연구 컴펜디엄을 만드는가",
    "text": "29.3 왜 연구 컴펜디엄을 만드는가\n연구 컴펜디엄 작성에는 시간과 노력이 필요하다. 그런데도 왜 해야 할까? 마코베츠(Markowetz) [9]는 재현가능 연구를 해야 하는 “5가지 이기적인 이유”를 제시했다. 윤리적 의무를 넘어 연구자 본인에게 실질적으로 유리하기 때문이라는 것이다.\n첫째, 데이터 손실과 재난을 방지한다. 연구자들 사이에 회자되는 악몽이 있다. “SPSS 파일이 열리지 않습니다. 6개월 작업이 날아갔습니다.”라는 이메일이다. 바이너리 파일 형식은 손상 시 복구가 거의 불가능하다. 반면 Git으로 버전 관리되는 플레인 텍스트는 GitHub 서버, 로컬 PC, 연구실 백업 서버에 동시에 존재한다. 리눅스 커널은 20년 넘게 100만 건 이상의 커밋으로 관리되지만 단 한 줄의 코드도 잃어버린 적이 없다. 연구 컴펜디엄은 이런 수준의 안전망을 제공한다.\n둘째, 논문 작성 효율성이 높아진다. “심사자가 로그 스케일로 다시 그려달라고 요청했어요.” 전통적인 워크플로우에서는 엑셀에서 차트를 다시 그리고, 워드에 복사하고, 그림 번호를 다시 매기는 데 며칠이 걸린다. 쿼토로 작성된 논문은 코드 한 줄 수정(scale_y_log10())으로 몇 분 만에 끝난다. 데이터가 갱신되면 표와 그림이 자동으로 갱신되고, 본문의 통계값도 자동으로 업데이트된다. 피엔타 등 [10]은 7,040건의 NSF/NIH 연구비 수혜 과제를 분석한 결과, 데이터를 아카이브한 연구과제가 그렇지 않은 과제보다 출판 논문 수가 두 배(중앙값 10편 vs 5편) 많았음을 보여주었다.\n셋째, 심사자와 소통이 원활해진다. “그림 3의 p-value가 어떻게 계산되었는지 알 수 없습니다”라는 코멘트는 연구자에게 악몽이다. 반면 GitHub 저장소에 데이터와 분석 코드가 공개된 논문은 심사자가 직접 재현해볼 수 있다. PLOS ONE과 eLife 같은 저널들은 이제 코드 공개를 정책으로 요구하고 있다. 코드와 데이터로 말하면 “방법론이 불명확하다”는 심사 의견은 사라진다. 6개월 후의 자신은 거의 다른 사람이다. 잘 구조화된 컴펜디엄은 심사자뿐만 아니라 미래의 자신과도 효과적으로 소통하게 해준다.\n넷째, 연구 영속성이 보장된다. HWP 97로 작성된 문서가 HWP 2020에서 제대로 열리지 않는 경험을 해본 적 있는가? 10년 전 엑셀 파일의 한글 인코딩이 깨져서 당황한 적은? 플레인 텍스트 기반 마크다운 문서는 34년 전 리눅스 0.01 소스코드가 2025년 현재도 완벽하게 읽히는 것처럼 영원히 읽힌다. 소프트웨어가 사라져도, 회사가 망해도, 라이선스 계약이 만료되어도 연구는 남는다.\n다섯째, 국제적 명성과 영향력이 높아진다. 오픈 사이언스 연구 [11]에 따르면 코드와 데이터를 공개한 논문은 비공개 논문 대비 인용 횟수가 최대 3배까지 높다. 피워워 등 [12]의 연구도 데이터를 공개한 논문의 인용률이 유의미하게 높음을 보여주었고, 피워워와 비전 [13]은 이를 “개방 데이터 인용 이점(open data citation advantage)”이라 명명했다. 오픈소스 연구는 단순히 윤리적인 선택이 아니라 연구자의 영향력을 극대화하는 전략적 선택이다. GitHub 프로필은 이제 연구자의 두 번째 이력서가 되었다.\nR 패키지 형식의 컴펜디엄은 추가적인 이점이 있다. devtools::check()를 실행하면 코드의 품질을 자동으로 검사할 수 있고, roxygen2로 함수를 문서화하면 사용법이 명확해진다. 복사-붙여넣기 대신 함수를 작성하면 오류 발생 가능성이 줄어든다.\n\n\n\n\n\n\n노트파이썬 연구 컴펜디엄: 시도와 한계\n\n\n\n파이썬 생태계에서도 재현가능 연구에 대한 논의가 활발하다. 윌슨 등 [14]은 “충분히 좋은 과학 컴퓨팅 실천법”에서 파이썬을 포함한 언어 중립적인 재현가능 연구 가이드라인을 제시했다. 주피터 노트북(Jupyter Notebook)은 클루이버 등 [15]이 “재현가능 계산 워크플로우를 위한 출판 형식”으로 제안한 이후 데이터 과학의 핵심 도구가 되었고, 룰 등 [16]은 주피터 노트북에서 재현가능 연구를 위한 “10가지 간단한 규칙”을 정리했다.\n하지만 현실은 녹록지 않다. 피멘텔 등 [17]이 GitHub의 140만 개 주피터 노트북을 분석한 결과, 실행 가능한 노트북 중 24%만 오류 없이 실행되었고, 원본과 동일한 결과를 재현한 비율은 4%에 불과했다. 이 연구는 주피터 노트북이 재현가능성을 보장하지 않으며, 오히려 비선형 실행과 숨겨진 상태 문제로 재현성을 해칠 수 있음을 보여준다.\nR의 연구 컴펜디엄 개념과 비교하면, 파이썬 생태계는 통합된 프레임워크보다 개별 도구의 조합에 의존한다. Cookiecutter Data Science가 프로젝트 템플릿을, poetry/pipenv가 패키지 관리를, DVC가 데이터 버전 관리를 담당하지만, 이들을 하나로 묶는 학술적 프레임워크는 아직 정립되지 않았다. 마윅 등 [5]처럼 “연구 컴펜디엄”을 학술적으로 정의하고 R 패키지 시스템과 통합한 사례가 파이썬에는 부재하다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-organization",
    "href": "compendium.html#sec-compendium-organization",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.4 파일 조직 원칙",
    "text": "29.4 파일 조직 원칙\n연구 프로젝트의 파일은 세 가지 유형으로 나뉜다. 읽기 전용(read-only) 파일은 원본 데이터와 메타데이터다. 수정해서는 안 되며, 분석의 출발점이다. 사람이 생성한(human-generated) 파일은 분석 스크립트, 논문 원고, 문서다. 연구자가 직접 작성하고 수정한다. 프로젝트가 생성한(project-generated) 파일은 전처리된 데이터, 그래프, 테이블 등 코드 실행의 산출물이다.\n이러한 분류가 중요한 이유는 버전 관리 전략과 직결되기 때문이다. 읽기 전용 데이터는 변경되면 안 되고, 프로젝트 생성 파일은 코드에서 재생성할 수 있으므로 Git에서 제외할 수 있다. 사람이 생성한 파일만 Git으로 추적하면 저장소가 깔끔해진다.\nmyproject/\n├── README.md           # 프로젝트 설명\n├── LICENSE             # 라이선스\n├── DESCRIPTION         # 메타데이터 (R 패키지 형식)\n│\n├── data/               # 읽기 전용\n│   ├── raw/            # 원본 데이터\n│   └── processed/      # 전처리된 데이터 (재생성 가능)\n│\n├── analysis/           # 사람이 생성\n│   ├── 01_import.R\n│   ├── 02_clean.R\n│   ├── 03_analyze.R\n│   └── paper.qmd       # 논문 원고\n│\n├── figures/            # 프로젝트가 생성\n│   ├── figure1.png\n│   └── figure2.png\n│\n├── renv.lock           # R 패키지 버전\n├── Dockerfile          # 계산 환경\n└── Makefile            # 실행 순서\n\n29.4.1 AI 시대 파일 조직 원칙\n앞서 제시한 파일 분류 체계—읽기 전용, 사람 생성, 프로젝트 생성—는 연구자가 모든 코드를 직접 작성하던 시대의 산물이다. 그러나 ChatGPT, Claude, GitHub Copilot 같은 AI 도구가 연구 현장에 빠르게 침투하면서, “누가 이 코드를 작성했는가”라는 질문이 더 이상 자명하지 않게 되었다. AI가 초안을 작성하고 연구자가 검토·수정하는 협업 패턴이 일상화되면서, 파일의 출처와 생성 과정을 추적하는 새로운 원칙이 필요해졌다. 그림 29.3 은 수작업 시대에서 자동화 시대를 거쳐 AI 협업 시대로 이어지는 변화를 보여준다.\n\n\n\n\n\n그림 29.3: 수작업에서 자동화를 거쳐 AI 협업\n\n\n1단계: 수작업 시대(1990s~2010s)에는 연구자가 모든 작업을 직접 수행했다. 코드 작성, 문서화, 폴더 구조 설계, 데이터 전처리, 환경 명세까지 100% 연구자의 몫이었다. 재현성 검증은 “희망사항”에 가까웠고, 실제로 실행되는 경우는 드물었다.\n2단계: 자동화 시대(2010s~2020s)에 들어 Make, CI/CD, renv, Docker 같은 도구가 반복 작업을 자동화했다. 연구자는 “무엇을” 할지 결정하고, 스크립트가 “어떻게” 실행할지 담당했다. 재현성 검증이 자동화되면서 실질적인 재현가능 연구가 가능해졌다.\n3단계: AI 협업 시대(2020s~현재)는 LLM(Large Language Model)과 AI 에이전트가 연구 워크플로우에 참여하는 새로운 패러다임이다. 첸 등 [18]은 이를 “에이전틱 AI(Agentic AI) 시대의 과학 워크플로우 (R)evolution”이라 명명했다. 연구자는 의사결정, 검증, 연구 설계, 인과 추론에 집중하고, AI는 코드 생성, 문서 동기화, 구조 자동 생성, 패턴 인식을 지원한다.\n\n\n\n\n\n\n힌트문학적 프로그래밍의 부활\n\n\n\n도널드 크누스(Donald Knuth)가 1984년 제안한 문학적 프로그래밍(Literate Programming)이 LLM 시대에 부활하고 있다. 주 등 [19]은 “LLM 시대 문학적 프로그래밍의 르네상스”에서 상호운용 가능한 문학적 프로그래밍(Interoperable LP)을 제안했다. 코드와 자연어 설명을 함께 작성하면 LLM이 대규모 프로젝트에서도 더 정확한 코드를 생성한다는 것이다.\n아칸크샤 등 [20]은 한 걸음 더 나아가 자연어 아웃라인(NL Outline)을 제안했다. 코드 함수마다 간결한 산문체 설명을 붙이면, LLM이 코드↔︎자연어 간 양방향 동기화를 수행한다. 연구자가 코드를 수정하면 문서가 자동 갱신되고, 문서를 수정하면 코드가 자동 갱신된다.\n\n\nAI 협업 시대 파일 조직은 기존 원칙에서 확장이 요구되고 있다. 가장 근본적인 변화는 출처 추적(Provenance) 필수화다. 수자 등 [21]은 AI가 통합된 워크플로우에서 “에이전트 행동의 추적 가능성이 책임성, 투명성, 설명가능성, 감사가능성을 위해 필수”라고 강조한다. 어떤 코드가 AI가 생성했는지, 어떤 프롬프트로 생성되었는지, 연구자가 어떻게 검증했는지 기록해야 한다. PROV-AGENT [22]는 출처 추적을 위한 출처 모델을 제안했다.\n출처 추적 필요성은 파일 분류 체계의 확장으로 이어진다. 기존의 세 가지 유형—읽기 전용, 사람 생성, 프로젝트 생성—에 AI 협업 생성(AI-assisted) 파일이 추가된다. AI가 초안을 작성하고 연구자가 검증·수정한 파일이다. AI와 함께 작성된 파일은 생성 이력(프롬프트, 모델 버전, 검증 내역)을 메타데이터로 기록해야 한다.\nmyproject/\n├── README.md           # AI 협업 생성 (프롬프트 이력 포함)\n├── analysis/\n│   ├── 01_import.R     # 사람 생성\n│   ├── 02_clean.R      # AI 협업 생성 (Claude 3.5, 검증 완료)\n│   └── paper.qmd       # AI 협업 생성 (NL 아웃라인 동기화)\n├── .ai/                # AI 협업 메타데이터\n│   ├── prompts.md      # 사용된 프롬프트 기록\n│   ├── model_versions.yml  # 사용된 AI 모델 버전\n│   └── review_log.md   # 연구자 검증 이력\n└── ...\n이러한 메타데이터를 체계적으로 관리하기 위해 AGENT.md 또는 CLAUDE.md 같은 AI 협업 지침 파일이 등장했다. README.md가 “사람을 위한 현관문”이라면, AGENT.md는 “AI를 위한 계약서”다. 프로젝트의 맥락, 코딩 규칙, 선호하는 라이브러리, 금지된 패턴 등을 명시하면 AI가 프로젝트에 맞는 코드를 생성한다. Claude Code는 CLAUDE.md, Cursor는 .cursorrules, GitHub Copilot은 .github/copilot-instructions.md를 읽어 프로젝트별 지침을 따른다.\nAGENT.md의 핵심은 재현가능성의 확장이다. 전통적인 연구 컴펜디엄이 “같은 코드로 같은 결과”를 보장했다면, AI 시대 컴펜디엄은 “같은 지침으로 유사한 코드”까지 보장해야 한다. 연구자가 어떤 프롬프트로 AI에게 코드를 요청했는지, 어떤 모델 버전을 사용했는지, 생성된 코드를 어떻게 검증했는지 기록하면 제3자도 AI 협업 과정을 재현할 수 있다.\n새로운 분류 체계와 함께 검증 워크플로우가 핵심으로 부상한다. AI가 생성한 코드는 반드시 연구자 검증을 거쳐야 한다. 자동화된 테스트만으로는 부족하다. 과학적 발견은 “패턴 인식을 넘어 인과성 이해”를 요구하기 때문이다 [18]. 연구자는 AI를 “똑똑한 조수”로 활용하되, 최종 책임은 연구자에게 있다.\nDESCRIPTION 파일은 R 패키지 표준 메타데이터 형식이다. 연구 컴펜디엄에서 DESCRIPTION을 포함하면, 프로젝트가 “형식적으로 R 패키지”가 되어 패키지 개발 도구(devtools, usethis)의 이점을 활용할 수 있다. 의존성 자동 설치, 문서화, 테스트 프레임워크가 그것이다.\nPackage: myresearch\nTitle: Analysis of Urban Temperature Patterns\nVersion: 0.1.0\nAuthors@R:\n    person(\"홍길동\", email = \"hong@example.com\", role = c(\"aut\", \"cre\"))\nDescription: 도시 열섬 효과 분석을 위한 연구 컴펜디엄.\n    2020-2023년 서울 기온 데이터를 분석한다.\nLicense: MIT + file LICENSE\nDepends:\n    R (&gt;= 4.3.0)\nImports:\n    tidyverse,\n    sf,\n    terra\nSuggests:\n    testthat,\n    knitr",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-components",
    "href": "compendium.html#sec-compendium-components",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.5 핵심 구성요소",
    "text": "29.5 핵심 구성요소\n“모든 파일이 다 있는데 왜 재현이 안 될까?” 연구 컴펜디엄을 처음 접하는 연구자들이 흔히 던지는 질문이다. 코드와 데이터가 있어도 패키지 버전이 다르면 오류가 발생하고, 패키지 버전을 맞춰도 운영체제가 다르면 결과가 달라진다. AI 도구로 생성한 코드라면 어떤 프롬프트로 만들었는지, 어떤 모델을 사용했는지도 기록해야 한다. 재현성은 단순히 파일을 공유하는 것이 아니라, 연구의 모든 맥락을 함께 전달하는 것이다.\n재현성 성숙도 모델(Reproducibility Maturity Model)은 연구 컴펜디엄의 구성요소를 단계별로 정리한 프레임워크다. 기본적인 문서화에서 시작해 법적 명확성, 구조화된 명세, 패키지 고정, 환경 고정을 거쳐 AI 맥락 통합까지 이어진다. 각 단계는 이전 단계 위에 쌓이며, 높은 수준으로 갈수록 더 완전한 재현이 가능해진다. 모든 프로젝트가 최고 수준을 목표로 할 필요는 없다. 프로젝트의 복잡도와 재현성 요구 수준에 따라 적절한 단계를 선택하면 된다.\n\n\n\n\n\n그림 29.4: 연구 컴펜디엄의 핵심 구성요소\n\n\n그림 29.4 는 기본 재현에서 인지 재현(Cognitive Reproducibility)까지 이어지는 여섯 가지 핵심 구성요소를 보여준다. 가장 기본적인 README.md는 프로젝트의 “현관문”으로, 무엇을 연구했는지, 어떻게 실행하는지 설명한다. GitHub 저장소 첫 화면에 표시되므로 처음 방문자가 프로젝트를 이해하는 유일한 경로다.\nLICENSE는 코드와 데이터의 사용 권한을 명시하여 법적 보호를 제공한다. MIT, GPL, CC-BY 중 하나를 선택하며, 라이선스 없이 공개하면 저작권법상 “모든 권리 보유”가 되어 재사용이 법적으로 불가능해진다. DESCRIPTION은 R 패키지 표준 메타데이터 형식으로, 의존성 목록과 저자 정보를 기계가 읽을 수 있는 구조로 제공한다.\n재현성 수준이 높아지면 환경 고정이 필요하다. renv.lock은 R/Python 패키지 버전을 정확한 해시로 고정하여 “tidyverse 2.0에서는 되는데 1.3에서는 안 돼요” 문제를 방지한다. Dockerfile은 한 걸음 더 나아가 운영체제, 시스템 라이브러리, 런타임 환경까지 컨테이너화하여 완전한 환경 스냅샷을 제공한다.\nAI 협업 시대에는 여기서 멈추지 않는다. AGENT.md는 프로젝트 맥락과 도메인 지식, 코딩 규칙과 금지 패턴, 모델 버전과 검증 이력을 기록하여 AI 출처를 추적한다. Docker가 계산 환경을 재현한다면, AGENT.md는 AI 협업 맥락을 재현한다. 이 둘이 결합되어야 AI 시대의 진정한 재현성이 확보된다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-rrtools",
    "href": "compendium.html#sec-compendium-rrtools",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.6 rrtools: 컴펜디엄 자동화",
    "text": "29.6 rrtools: 컴펜디엄 자동화\n연구 컴펜디엄을 처음부터 만드는 것은 번거롭다. rrtools 패키지는 이 과정을 자동화한다. 벤 마윅이 개발한 이 도구는 한 줄 명령으로 재현가능한 연구 프로젝트 구조를 생성한다.\n# rrtools 설치\ninstall.packages(\"devtools\")\ndevtools::install_github(\"benmarwick/rrtools\")\n\n# 새 컴펜디엄 생성\nrrtools::use_compendium(\"myresearch\")\nuse_compendium() 명령은 DESCRIPTION 파일, R 프로젝트 파일(.Rproj), 기본 디렉토리 구조를 자동 생성한다. 이후 필요한 구성요소를 하나씩 추가한다.\n# 분석 폴더 생성 (Quarto 기반)\nrrtools::use_analysis()\n\n# MIT 라이선스 추가\nusethis::use_mit_license()\n\n# renv로 패키지 관리 시작\nrenv::init()\n\n# Docker 지원 추가\nrrtools::use_dockerfile()\n\n# GitHub Actions CI 추가\nrrtools::use_github()\n\n\n\n\n\n그림 29.5: rrtools 워크플로우\n\n\n그림 29.5 는 rrtools가 생성하는 프로젝트 구조를 보여준다. analysis/ 폴더에는 Quarto 기반 논문 템플릿이, R/ 폴더에는 재사용 함수가, 루트에는 DESCRIPTION과 README가 위치한다. GitHub Actions는 코드 변경 시 자동으로 Docker 이미지를 빌드하고 테스트를 실행한다.\nrrtools가 생성하는 analysis/ 폴더 구조:\nanalysis/\n├── paper/\n│   ├── paper.qmd       # 논문 원고\n│   └── references.bib  # 참고문헌\n├── figures/            # 생성된 그래프\n├── data/\n│   ├── raw_data/       # 원본 데이터\n│   └── derived_data/   # 전처리 데이터\n└── supplementary-materials/",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-examples",
    "href": "compendium.html#sec-compendium-examples",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.7 실제 연구 컴펜디엄 사례",
    "text": "29.7 실제 연구 컴펜디엄 사례\n마윅 등 [5]은 논문에서 복잡도에 따른 세 가지 수준의 실제 연구 컴펜디엄 사례를 제시한다. 각 사례는 GitHub에서 개발되고 Zenodo에서 DOI와 함께 아카이브되었다.\n\n29.7.1 소규모 컴펜디엄: Duffy (2015)\n더피(Duffy) 등의 기생충 연구는 가장 단순한 형태의 연구 컴펜디엄을 보여준다.\nBroodParasiteDescription/\n├── DESCRIPTION          # 프로젝트 메타데이터\n├── README.md            # 설명\n├── data/\n│   └── my_data.csv     # 원본 데이터\n└── analysis/\n    └── my_script.R     # 분석 코드\n\n\nGitHub: github.com/duffymeg/BroodParasiteDescription\n\nZenodo: doi.org/10.5281/zenodo.17804\n\nDESCRIPTION 파일이 R 버전(3.2.0 이상)과 의존 패키지를 명시하고 있어, 세 가지 핵심 원칙(관례적 폴더 구조, 데이터·방법 분리, 환경 명세)을 모두 충족한다. 이 정도면 대부분의 단일 논문 프로젝트에 충분하다.\n\n29.7.2 중규모 컴펜디엄: Hollister et al. (2016)\n홀리스터(Hollister) 등의 호수 영양 상태 모델링 연구는 R 패키지 구조를 더 완전하게 활용한다.\nLakeTrophicModelling/\n├── DESCRIPTION\n├── NAMESPACE\n├── README.md\n├── LICENSE\n├── R/                   # 재사용 함수\n│   └── my_functions.R\n├── man/                 # 자동 생성 문서\n│   └── my_functions.Rd\n├── data/\n│   └── my_data.csv\n└── vignettes/           # 논문 원고\n    └── manuscript.Rmd\n\n\nGitHub: github.com/USEPA/LakeTrophicModelling\n\nZenodo: doi.org/10.5281/zenodo.40271\n\n핵심 차이점은 R/ 디렉토리에 재사용 가능한 함수가 있고, man/ 디렉토리에 자동 생성된 문서가 있다는 것이다. 논문 원고는 vignettes/ 디렉토리에 R Markdown 형식으로 작성되어, 패키지 설치 시 자동으로 렌더링된다. 그래프, 테이블, 통계 결과가 모두 코드에서 생성된다.\n\n29.7.3 대규모 컴펜디엄: Boettiger et al. (2015)\n보에티거(Boettiger) 등의 어업 관리 연구는 가장 복잡한 형태의 연구 컴펜디엄이다.\nnonparametric-bayes/\n├── DESCRIPTION\n├── NAMESPACE\n├── README.md\n├── LICENSE\n├── Makefile             # 빌드 자동화\n├── Dockerfile           # 환경 격리\n├── .travis.yml          # 지속적 통합\n├── .zenodo.json         # 메타데이터\n├── R/\n├── man/\n├── data/\n├── tests/               # 단위 테스트\n│   └── testthat/\n└── manuscripts/\n    ├── paper.Rmd\n    └── Makefile\n\n\nGitHub: github.com/cboettig/nonparametric-bayes\n\nZenodo: doi.org/10.5281/zenodo.12669\n\nDocker 컨테이너가 전체 계산 환경을 캡슐화하고, Travis CI(현재는 GitHub Actions으로 대체)가 커밋마다 자동으로 분석을 재실행한다. .zenodo.json 파일은 Zenodo 아카이브를 위한 메타데이터를 제공한다. tests/ 디렉토리의 단위 테스트는 함수가 예상대로 작동하는지 확인한다.\n이 세 가지 사례는 프로젝트 규모와 요구사항에 따라 컴펜디엄 복잡도를 조절할 수 있음을 보여준다. 단순한 분석은 소규모 구조로 충분하고, 복잡한 시뮬레이션이나 장기 프로젝트는 대규모 구조가 필요하다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-environment",
    "href": "compendium.html#sec-compendium-environment",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.8 계산 환경 명세",
    "text": "29.8 계산 환경 명세\n코드와 데이터가 있어도 환경이 다르면 결과가 달라진다. R 4.2에서 4.3으로 업그레이드되면서 기본 난수 생성기가 바뀌었고, ggplot2 3.4에서 테마 기본값이 변경되었다. 재현가능성은 환경 명세에서 완성된다.\n\n29.8.1 R 패키지 버전 관리의 진화\nR 패키지 버전 관리 도구는 시간이 지나면서 발전해왔다.\npackrat(2014)은 RStudio가 개발한 최초의 프로젝트별 패키지 관리 도구였다. 사용한 패키지의 소스 코드를 프로젝트 내 packrat/ 디렉토리에 다운로드하여 저장한다. 그러나 대규모 프로젝트에서 느려지고, 재현성 보장이 불완전한 문제가 있었다.\ncheckpoint(2017)은 Microsoft가 개발했다. Microsoft R Archived Network(MRAN)에서 특정 날짜의 CRAN 스냅샷을 설치한다. “2023년 1월 15일 기준 CRAN”처럼 시점을 지정할 수 있어 간편하지만, MRAN 서비스가 2023년 종료되면서 더 이상 권장되지 않는다.\nrenv(2019)는 packrat의 후속 도구로, 현재 R 환경 관리의 표준이다. 프로젝트별로 독립된 라이브러리를 생성하고, renv.lock 파일에 정확한 버전을 기록한다.\n# renv 초기화\nrenv::init()\n\n# 필요한 패키지 설치\ninstall.packages(c(\"tidyverse\", \"sf\", \"gt\"))\n\n# 현재 상태 스냅샷\nrenv::snapshot()\n\n# 다른 환경에서 복원\nrenv::restore()\nrenv.lock 파일 예시:\n{\n  \"R\": {\n    \"Version\": \"4.3.0\",\n    \"Repositories\": [\n      {\"Name\": \"CRAN\", \"URL\": \"https://cloud.r-project.org\"}\n    ]\n  },\n  \"Packages\": {\n    \"ggplot2\": {\n      \"Package\": \"ggplot2\",\n      \"Version\": \"3.4.2\",\n      \"Source\": \"Repository\"\n    },\n    \"dplyr\": {\n      \"Package\": \"dplyr\",\n      \"Version\": \"1.1.2\",\n      \"Source\": \"Repository\"\n    }\n  }\n}\nrenv만으로는 시스템 라이브러리 의존성을 해결할 수 없다. sf 패키지가 GDAL에 의존하고, rJava가 Java에 의존하는 경우, 패키지 버전이 같아도 시스템 라이브러리 버전에 따라 결과가 달라질 수 있다. 이때 Docker가 필요하다.\nFROM rocker/verse:4.3.0\n\n# 시스템 라이브러리 설치\nRUN apt-get update && apt-get install -y \\\n    libgdal-dev \\\n    libgeos-dev \\\n    libproj-dev\n\n# renv 복원\nCOPY renv.lock renv.lock\nRUN R -e \"renv::restore()\"\n\n# 프로젝트 파일 복사\nCOPY . /home/rstudio/project\nWORKDIR /home/rstudio/project\nDocker와 renv를 함께 사용하면 “완전한 재현”이 가능하다. Docker가 OS와 시스템 라이브러리를, renv가 R 패키지 버전을 보장한다. 5년 후에도, 다른 대륙에서도, 정확히 같은 결과를 얻을 수 있다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-encore",
    "href": "compendium.html#sec-compendium-encore",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.9 ENCORE: 2024년의 실천 프레임워크",
    "text": "29.9 ENCORE: 2024년의 실천 프레임워크\n반캄펜 등 [23]이 Nature Communications에 발표한 ENCORE(ENhancing COmputational REproducibility)는 연구 컴펜디엄 개념을 실천적 프레임워크로 발전시켰다. 기존 가이드라인이 “무엇을 해야 하는지”를 설명했다면, ENCORE는 “어떻게 해야 하는지”를 구체화한다.\nENCORE의 핵심 특징:\n\n\n표준화된 파일 시스템 구조: 모든 프로젝트 구성요소를 정해진 위치에 배치\n\n사전 정의된 문서 템플릿: README, 메서드 설명, 데이터 사전 등을 템플릿으로 제공\n\nGitHub 통합: 버전 관리와 협업을 기본으로 내장\n\nHTML 기반 네비게이터: 프로젝트 구조를 시각적으로 탐색\n\nENCORE_project/\n├── 00_admin/           # 관리 문서\n│   ├── README.md\n│   └── project_log.md\n├── 01_data/\n│   ├── raw/            # 원본 데이터\n│   ├── processed/      # 전처리 데이터\n│   └── metadata/       # 데이터 설명\n├── 02_scripts/\n│   ├── 01_import.R\n│   ├── 02_preprocess.R\n│   └── 03_analyze.R\n├── 03_results/\n│   ├── figures/\n│   └── tables/\n├── 04_manuscript/\n│   └── paper.qmd\n└── 05_environment/\n    ├── Dockerfile\n    └── renv.lock\nENCORE가 지적하는 재현가능성의 가장 큰 장벽은 기술적 문제가 아니다. “연구자가 재현가능성을 위해 시간과 노력을 투자할 인센티브가 부족하다”는 점이다. 논문 출판이 주된 평가 기준인 학계에서, 코드 정리와 문서화에 시간을 쓰는 것은 “보상 없는 노동”으로 여겨진다. ENCORE는 이 문제를 최소한의 추가 노력으로 재현가능성을 달성하도록 프레임워크를 설계함으로써 해결하려 한다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-worldbank",
    "href": "compendium.html#sec-compendium-worldbank",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.10 세계은행의 재현가능 연구 이니셔티브",
    "text": "29.10 세계은행의 재현가능 연구 이니셔티브\n2023년 세계은행(World Bank)은 대규모 조직이 연구 컴펜디엄을 제도화한 첫 사례를 만들었다 [24]. 기존의 개방 데이터·개방 지식 정책을 확장해 재현가능성 패키지(reproducibility package) 제출을 장려하기 시작했다.\n재현가능성 패키지는 세 가지 컬렉션으로 구성된다: - 정책 연구 워킹페이퍼(Policy Research Working Papers) - 학술지 논문(Journal Articles) - 세계은행 보고서(World Bank Reports)\n2024년 6월 기준, 2023년 9월 이후 제출된 워킹페이퍼 중 43.4%가 재현가능성 패키지를 포함했다. 의무가 아닌 자발적 참여임에도 높은 채택률을 보인 것은 연구자들이 재현가능성의 가치를 인식하고 있음을 보여준다.\n세계은행 재현가능성 저장소(Reproducible Research Repository)는 포괄적인 메타데이터와 함께 패키지를 공개해 검색과 발견이 가능하게 한다. 이는 연구 컴펜디엄이 개인 연구자의 선택이 아닌 조직 차원의 정책이 될 수 있음을 증명한 사례다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-sharing",
    "href": "compendium.html#sec-compendium-sharing",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.11 연구 컴펜디엄 공유 방법",
    "text": "29.11 연구 컴펜디엄 공유 방법\n연구 컴펜디엄 공유는 오픈 사이언스의 국제 표준인 FAIR 원칙 [25]에 부합해야 한다. FAIR는 Findable(찾을 수 있는), Accessible(접근 가능한), Interoperable(상호운용 가능한), Reusable(재사용 가능한)의 약자로, 과학 데이터 관리와 공유의 핵심 원칙이다.\n\n\n\n\n\n\n\n\nFAIR\n원칙\n컴펜디엄 적용\n\n\n\n\nFindable\n영구 식별자로 검색 가능\nGitHub + Zenodo DOI\n\n\n\nAccessible\n표준 프로토콜로 접근 가능\nGit clone 무료 접근\n\n\n\nInteroperable\n표준 포맷으로 상호 호환\n플레인 텍스트, CSV, JSON\n\n\n\nReusable\n명확한 라이선스로 재사용 가능\nMIT/CC-BY 라이선스\n\n\n\n\n\n\n\n표 29.1: FAIR 원칙과 연구 컴펜디엄의 관계\n\n\n\n연구 컴펜디엄을 공유할 때는 라이선스, 버전 관리, 영속성, 메타데이터를 고려해야 한다. 마윅 등 [5]은 각 요소에 대한 구체적인 권고사항을 제시한다.\n\n29.11.1 라이선스 선택\n라이선스 없이 공개된 코드와 데이터는 저작권법상 “모든 권리 보유”가 된다. 다른 연구자가 법적으로 재사용할 수 없다. 명시적인 라이선스가 필수다.\n데이터: 스토든 [26]은 데이터에 CC-0(Creative Commons Public Domain)을 권장한다. 데이터는 많은 법적 관할권에서 창작물이 아닌 “사실”로 간주되어 저작권 보호를 받지 못할 수 있다. CC-0는 이러한 법적 불확실성을 제거하고 최대한의 재사용을 허용한다.\n문서와 논문: CC-BY(Creative Commons Attribution)가 적절하다. 원저자 표시를 조건으로 재사용을 허용한다. 학술 인용 관행과 일치한다.\n코드: MIT 또는 GPL 같은 오픈소스 라이선스를 사용한다. MIT는 가장 관대하고, GPL은 파생 저작물도 오픈소스로 유지하도록 요구한다.\nmyproject/\n├── LICENSE            # MIT for code\n├── LICENSE-CC0        # CC-0 for data\n├── data/              # CC-0 applies here\n└── R/                 # MIT applies here\n\n29.11.2 버전 관리와 Git\nGit은 연구 컴펜디엄의 변경 이력을 보존하는 가장 좋은 방법이다. 모든 수정 사항이 기록되어, 특정 시점의 코드 상태를 정확히 복원할 수 있다. GitHub, GitLab 같은 호스팅 서비스는 협업과 배포를 용이하게 한다.\n\n29.11.3 DOI와 영속적 아카이브\nGitHub URL은 영구적이지 않다. 저장소가 삭제되거나 이름이 바뀌면 링크가 깨진다. 클라인 등 [27]의 연구에 따르면, 학술 논문 인용의 5분의 1이 “참조 부패(reference rot)”를 겪고 있다.\nDOI(Digital Object Identifier)를 발급하는 저장소에 아카이브하면 이 문제를 해결할 수 있다:\n\n\nZenodo (zenodo.org): CERN이 운영, GitHub 통합 우수\n\nOSF (osf.io): Open Science Framework, 프로젝트 관리 기능\n\nFigshare (figshare.com): 대용량 파일 지원\n\nDryad (datadryad.org): 생태학·진화생물학 중심\n\nZenodo는 GitHub 저장소와 자동 통합되어, 릴리스를 생성하면 자동으로 DOI가 발급된다. 논문에 이 DOI를 인용하면 심사위원과 독자가 정확한 버전의 코드에 접근할 수 있다.\n\n29.11.4 CRAN을 피하는 이유\nCRAN(Comprehensive R Archive Network)은 R 패키지의 표준 저장소지만, 연구 컴펜디엄에는 적합하지 않다:\n\n\n엄격한 디렉토리 구조: analysis/ 같은 최상위 디렉토리가 허용되지 않음\n\n5 MB 용량 제한: 데이터 파일, 캐시된 결과, 이미지를 포함하기 어려움\n\n업데이트 제약: 잦은 업데이트가 요구되는 개발 중 프로젝트에 부적합\n\nZenodo, OSF 같은 DOI 발급 저장소는 이러한 제한이 없고, 오히려 연구 아카이브에 특화되어 있다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-practice",
    "href": "compendium.html#sec-compendium-practice",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.12 실전: 논문 프로젝트 구축",
    "text": "29.12 실전: 논문 프로젝트 구축\n논문 프로젝트를 연구 컴펜디엄으로 구축하는 전체 과정을 살펴보자.\n1단계: 프로젝트 생성\n# rrtools로 컴펜디엄 생성\nrrtools::use_compendium(\"urban-heat-analysis\")\n\n# Quarto 기반 분석 폴더 추가\nrrtools::use_analysis()\n\n# 라이선스 추가\nusethis::use_mit_license()\n\n# Git 초기화\nusethis::use_git()\n2단계: 패키지 환경 설정\n# renv 초기화\nrenv::init()\n\n# 필요한 패키지 설치\ninstall.packages(c(\"tidyverse\", \"sf\", \"terra\", \"gt\", \"quarto\"))\n\n# 환경 스냅샷\nrenv::snapshot()\n3단계: 데이터 구조화\n# 데이터 폴더 생성\nfs::dir_create(\"analysis/data/raw_data\")\nfs::dir_create(\"analysis/data/derived_data\")\n\n# 원본 데이터 복사 (수정 금지)\nfs::file_copy(\"~/Downloads/seoul_temp_2020-2023.csv\",\n              \"analysis/data/raw_data/\")\n4단계: 분석 스크립트 작성\nanalysis/\n├── 01_import.R         # 데이터 로드\n├── 02_clean.R          # 전처리\n├── 03_analyze.R        # 분석\n├── 04_visualize.R      # 시각화\n└── paper/\n    └── paper.qmd       # 논문 원고\n5단계: Makefile로 워크플로우 정의\nall: paper\n\n# 데이터 전처리\nanalysis/data/derived_data/clean.rds: analysis/data/raw_data/seoul_temp.csv \\\n                                       analysis/01_import.R \\\n                                       analysis/02_clean.R\n    Rscript analysis/01_import.R\n    Rscript analysis/02_clean.R\n\n# 분석 실행\nanalysis/data/derived_data/results.rds: analysis/data/derived_data/clean.rds \\\n                                         analysis/03_analyze.R\n    Rscript analysis/03_analyze.R\n\n# 그래프 생성\nanalysis/figures/figure1.png: analysis/data/derived_data/results.rds \\\n                               analysis/04_visualize.R\n    Rscript analysis/04_visualize.R\n\n# 논문 렌더링\npaper: analysis/figures/figure1.png analysis/paper/paper.qmd\n    quarto render analysis/paper/paper.qmd\n\nclean:\n    rm -rf analysis/data/derived_data/*\n    rm -rf analysis/figures/*\n\n.PHONY: all paper clean\n6단계: Docker 환경 추가\nFROM rocker/verse:4.3.0\n\n# 시스템 의존성 설치\nRUN apt-get update && apt-get install -y \\\n    libgdal-dev libgeos-dev libproj-dev\n\n# renv 복원\nWORKDIR /home/rstudio/project\nCOPY renv.lock renv.lock\nRUN R -e \"renv::restore()\"\n\n# 프로젝트 복사\nCOPY . .\n\n# 분석 실행\nCMD [\"make\", \"all\"]\n7단계: GitHub Actions CI\n# .github/workflows/reproduce.yml\nname: Reproduce Analysis\n\non: [push, pull_request]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    container:\n      image: rocker/verse:4.3.0\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Restore renv\n        run: R -e \"renv::restore()\"\n\n      - name: Run analysis\n        run: make all\n\n      - name: Upload paper\n        uses: actions/upload-artifact@v4\n        with:\n          name: paper\n          path: analysis/paper/paper.pdf",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-comparison",
    "href": "compendium.html#sec-compendium-comparison",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.13 연구 컴펜디엄 vs 단순 코드 공개",
    "text": "29.13 연구 컴펜디엄 vs 단순 코드 공개\n“GitHub에 코드만 올리면 되지 않나요?”라는 질문을 자주 받는다. 코드 공개와 연구 컴펜디엄은 근본적으로 다르다.\n\n\n\n\n\n\n\n\n구분\n단순 코드 공개\n연구 컴펜디엄\n\n\n\n폴더 구조\n임의\n표준화\n\n\n패키지 버전\n미기록\nrenv.lock\n\n\n실행 순서\n추측 필요\nMakefile\n\n\n계산 환경\n미명세\nDockerfile\n\n\n문서화\n최소\nREADME + DESCRIPTION\n\n\n재현 난이도\n높음 (며칠~몇 주)\n낮음 (분~시간)\n\n\n\n\n\n\n\n표 29.2: 코드 공개 vs 연구 컴펜디엄 비교\n\n\n\n단순 코드 공개는 “이론적으로 재현 가능”하다. 하지만 실제로 재현하려면 패키지 버전을 맞추고, 실행 순서를 파악하고, 누락된 데이터를 찾아야 한다. 연구 컴펜디엄은 “실질적으로 재현 가능”하다. docker build && docker run 두 줄이면 논문의 모든 결과를 재생성한다.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#sec-compendium-turingway",
    "href": "compendium.html#sec-compendium-turingway",
    "title": "29  연구 컴펜디엄",
    "section": "\n29.14 The Turing Way: 커뮤니티 가이드",
    "text": "29.14 The Turing Way: 커뮤니티 가이드\nThe Turing Way [28]는 영국 앨런 튜링 연구소가 운영하는 재현가능 연구 가이드북이다. 오픈소스 커뮤니티 방식으로 작성되어 누구나 기여할 수 있으며, 연구 컴펜디엄을 포함한 재현가능 연구의 모든 측면을 다룬다.\nThe Turing Way가 제시하는 연구 컴펜디엄의 두 가지 수준:\n기본 컴펜디엄 (Basic Compendium):\ncompendium/\n├── data/\n├── analysis/\n├── DESCRIPTION\n└── README.md\n실행 가능 컴펜디엄 (Executable Compendium):\ncompendium/\n├── CITATION\n├── code/\n├── data_clean/\n├── data_raw/\n├── Dockerfile\n├── figures/\n├── LICENSE\n├── Makefile\n├── paper.Rmd\n└── README.md\n기본 컴펜디엄은 코드와 데이터를 조직화한 최소 구조다. 실행 가능 컴펜디엄은 make all 한 줄로 전체 분석을 재현할 수 있는 완전한 구조다. 프로젝트 규모와 재현성 요구 수준에 따라 선택한다.\n💡 생각해볼 점\n연구 컴펜디엄은 “미래의 나를 위한 투자”다. 6개월 후 심사위원이 재현을 요청하면, 잘 구조화된 컴펜디엄은 몇 시간 만에 응답할 수 있게 한다. 반면 정리되지 않은 코드는 며칠, 심지어 몇 주의 작업이 필요하다. “지금 바빠서 나중에 정리하겠다”는 말은 “영원히 정리하지 않겠다”와 같다.\n마윅 등 [5]의 세 가지 원칙—관례적 폴더 구조, 데이터·방법·결과 분리, 환경 명세—을 기억하자. rrtools로 프로젝트를 시작하면 이 원칙이 자동으로 적용된다. renv로 패키지 버전을 잠그고, Docker로 전체 환경을 패키징하면 “완전한 재현”이 가능해진다.\n세계은행의 사례가 보여주듯, 연구 컴펜디엄은 개인의 선택에서 조직의 정책으로 발전하고 있다. Nature, Science 같은 주요 저널도 재현가능성 패키지 제출을 권장하기 시작했다. 지금 연구 컴펜디엄을 익히는 것은 미래의 표준을 선점하는 것이다.\n다음 장에서는 Make와 연구 컴펜디엄을 결합해 전체 분석 파이프라인을 자동화하는 방법을 다룬다. make all 한 줄로 데이터 전처리부터 논문 렌더링까지 완료하는 워크플로우를 구축한다.\n\n\n\n\n[1] \nM. Baker, “1,500 scientists lift the lid on reproducibility”, Nature, vol 533, 호 7604, pp 452–454, 5 2016, doi: 10.1038/533452a.\n\n\n[2] \nL. K. John, G. Loewenstein, 와/과 D. Prelec, “Measuring the prevalence of questionable research practices with incentives for truth telling”, Psychological Science, vol 23, 호 5, pp 524–532, 5 2012, doi: 10.1177/0956797611430953.\n\n\n[3] \nJ. P. Simmons, L. D. Nelson, 와/과 U. Simonsohn, “False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant”, Psychological Science, vol 22, 호 11, pp 1359–1366, 11 2011, doi: 10.1177/0956797611417632.\n\n\n[4] \nN. L. Kerr, “HARKing: Hypothesizing after the results are known”, Personality and Social Psychology Review, vol 2, 호 3, pp 196–217, 1998, doi: 10.1207/s15327957pspr0203_4.\n\n\n[5] \nB. Marwick, C. Boettiger, 와/과 L. Mullen, “Packaging data analytical work reproducibly using R (and friends)”, The American Statistician, vol 72, 호 1, pp 80–88, 2018.\n\n\n[6] \nJ. F. Claerbout 와/과 M. Karrenbach, “Electronic Documents Give Reproducible Research a New Meaning”, in SEG Technical Program Expanded Abstracts, New Orleans, Louisiana: Society of Exploration Geophysicists, 1992, pp 601–604. doi: 10.1190/1.1822162.\n\n\n[7] \nR. Gentleman 와/과 D. Temple Lang, “Statistical Analyses and Reproducible Research”, Bioconductor Project, Working Paper 2, 5 2004. Available at: https://biostats.bepress.com/bioconductor/paper2/\n\n\n\n[8] \nK. Bollen, J. T. Cacioppo, R. M. Kaplan, J. A. Krosnick, J. L. Olds, 와/과 H. Dean, “Social, Behavioral, and Economic Sciences Perspectives on Robust and Reliable Science”, National Science Foundation, Directorate for Social, Behavioral,; Economic Sciences, Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation, 2015. Available at: https://www.nsf.gov/sbe/AC_Materials/SBE_Robust_and_Reliable_Research_Report.pdf\n\n\n\n[9] \nF. Markowetz, “Five selfish reasons to work reproducibly”, Genome Biology, vol 16, 호 1, p 274, 12 2015, doi: 10.1186/s13059-015-0850-7.\n\n\n[10] \nA. M. Pienta, G. C. Alter, 와/과 J. A. Lyle, “The Enduring Value of Social Science Research: The Use and Reuse of Primary Research Data”, in The Organisation, Economics and Policy of Scientific Research Workshop, Torino, Italy, 2010. Available at: https://deepblue.lib.umich.edu/handle/2027.42/78307\n\n\n\n[11] \nE. C. McKiernan 기타, “How open science helps researchers succeed”, eLife, vol 5, p e16800, 2016, doi: 10.7554/eLife.16800.\n\n\n[12] \nH. A. Piwowar, R. S. Day, 와/과 D. B. Fridsma, “Sharing Detailed Research Data Is Associated with Increased Citation Rate”, PLoS ONE, vol 2, 호 3, p e308, 3 2007, doi: 10.1371/journal.pone.0000308.\n\n\n[13] \nH. A. Piwowar 와/과 T. J. Vision, “Data reuse and the open data citation advantage”, PeerJ, vol 1, p e175, 2013, doi: 10.7717/peerj.175.\n\n\n[14] \nG. Wilson, J. Bryan, K. Cranston, J. Kitzes, L. Nederbragt, 와/과 T. K. Teal, “Good enough practices in scientific computing”, PLOS Computational Biology, vol 13, 호 6, p e1005510, 2017, doi: 10.1371/journal.pcbi.1005510.\n\n\n[15] \nT. Kluyver 기타, “Jupyter Notebooks – a publishing format for reproducible computational workflows”, in Positioning and Power in Academic Publishing: Players, Agents and Agendas, IOS Press, 2016, pp 87–90. doi: 10.3233/978-1-61499-649-1-87.\n\n\n[16] \nA. Rule 기타, “Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks”, PLOS Computational Biology, vol 15, 호 7, p e1007007, 2019, doi: 10.1371/journal.pcbi.1007007.\n\n\n[17] \nJ. F. Pimentel, L. Murta, V. Braganholo, 와/과 J. Freire, “A Large-scale Study about Quality and Reproducibility of Jupyter Notebooks”, in Proceedings of the 16th International Conference on Mining Software Repositories (MSR), IEEE, 2019, pp 507–517. doi: 10.1109/MSR.2019.00077.\n\n\n[18] \nH. Chen, R. Souza, 기타, “The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science”, Proceedings of the SC ’25 Workshops, 2025, doi: 10.1145/3731599.3767580.\n\n\n[19] \nY. Zhu 기타, “Renaissance of Literate Programming in the Era of LLMs: Enhancing LLM-Based Code Generation in Large-Scale Projects”, arXiv preprint, 2024, Available at: https://arxiv.org/abs/2502.17441\n\n\n\n[20] \nAakanksha, P. Kumari, M. Thakur, 와/과 S. Chakraborty, “Natural Language Outlines for Code: Literate Programming in the LLM Era”, Proceedings of the ACM International Conference on the Foundations of Software Engineering (FSE), 2024, doi: 10.1145/3696630.3728541.\n\n\n[21] \nR. Souza 기타, “Workflow Provenance in the Computing Continuum for Responsible, Trustworthy, and Energy-Efficient AI”, in IEEE International Conference on e-Science, 2024.\n\n\n[22] \nR. Souza 기타, “PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows”, 5th Workshop on Reproducible Workflows, Data Management, and Security, 2024.\n\n\n[23] \nA. H. C. van Kampen 기타, “ENCORE: a practical implementation to improve reproducibility and transparency of computational research”, Nature Communications, vol 15, 호 1, p 8117, 9 2024, doi: 10.1038/s41467-024-52446-8.\n\n\n[24] \nM. Jones, “Introducing Reproducible Research Standards at the World Bank”, Harvard Data Science Review, vol 6, 호 4, 10 2024, doi: 10.1162/99608f92.21328ce3.\n\n\n[25] \nM. D. Wilkinson 기타, “The FAIR Guiding Principles for scientific data management and stewardship”, Scientific Data, vol 3, 호 1, p 160018, 2016, doi: 10.1038/sdata.2016.18.\n\n\n[26] \nV. Stodden, “The Legal Framework for Reproducible Scientific Research: Licensing and Copyright”, Computing in Science & Engineering, vol 11, 호 1, pp 35–40, 2009, doi: 10.1109/MCSE.2009.19.\n\n\n[27] \nM. Klein 기타, “Scholarly Context Not Found: One in Five Articles Suffers from Reference Rot”, PLoS ONE, vol 9, 호 12, p e115253, 2014, doi: 10.1371/journal.pone.0115253.\n\n\n[28] \nThe Turing Way Community, The Turing Way: A handbook for reproducible, ethical and collaborative research. Zenodo, 2024. doi: 10.5281/zenodo.3233853.",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "compendium.html#footnotes",
    "href": "compendium.html#footnotes",
    "title": "29  연구 컴펜디엄",
    "section": "",
    "text": "컴펜디엄(compendium)은 라틴어 compendere(함께 저울질하다)에서 유래했다. 원래 “특정 주제에 관한 정보를 간결하게 모아놓은 모음집”을 의미하며, 중세 학자들이 방대한 지식을 요약한 편람을 지칭할 때 사용했다. 연구 컴펜디엄은 이 전통을 이어받아 “연구의 모든 구성요소를 하나로 모은 패키지”라는 의미로 사용된다.↩︎",
    "crumbs": [
      "**5부** 자동화, 도커, 재현성",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>연구 컴펜디엄</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "용어사전",
    "section": "",
    "text": "컴퓨터 하드웨어\n프로그래밍과 컴퓨터 과학에서 사용되는 핵심 용어를 정리한다. 각 용어는 한글 표기와 함께 영문 원어를 병기하고, 간략한 정의를 제공한다.",
    "crumbs": [
      "**부록**",
      "용어사전"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "[1] F.\nMarkowetz, “Five selfish reasons to work reproducibly,”\nGenome Biology, vol. 16, no. 1, p. 274, Dec. 2015, doi: 10.1186/s13059-015-0850-7.\n\n\n[2] M.\nBaker, “1,500 scientists lift the lid on reproducibility,”\nNature, vol. 533, no. 7604, pp. 452–454, May 2016, doi: 10.1038/533452a.\n\n\n[3] A.\nVaswani et al., “Attention is all you need,”\nAdvances in neural information processing systems, vol. 30,\n2017.\n\n\n[4] T.\nWu et al., “A brief overview of ChatGPT: The history,\nstatus quo and potential future development,” IEEE/CAA\nJournal of Automatica Sinica, vol. 10, no. 5, pp. 1122–1136,\n2023.\n\n\n[5] T.\nChiang, “ChatGPT is a blurry JPEG of the web,” The New\nYorker, vol. 9, p. 2023, 2023.\n\n\n[6] H.\nWickham, M. Çetinkaya-Rundel, and G. Grolemund, R for data\nscience. \" O’Reilly Media, Inc.\", 2023.\n\n\n[7] R.\nGozalo-Brizuela and E. C. Garrido-Merchan, “ChatGPT is not all you\nneed. A state of the art review of large generative AI models,”\narXiv preprint arXiv:2301.04655, 2023.\n\n\n[8] B.\nMarwick, C. Boettiger, and L. Mullen, “Packaging data analytical\nwork reproducibly using r (and friends),” The American\nStatistician, vol. 72, no. 1, pp. 80–88, 2018.\n\n\n[9] E.\nC. McKiernan et al., “How open science helps researchers\nsucceed,” eLife, vol. 5, p. e16800, 2016, doi: 10.7554/eLife.16800.\n\n\n[10] K.\nBollen, J. T. Cacioppo, R. M. Kaplan, J. A. Krosnick, J. L. Olds, and H.\nDean, “Social, behavioral, and economic sciences perspectives on\nrobust and reliable science,” National Science Foundation,\nDirectorate for Social, Behavioral,; Economic Sciences, Report of the\nSubcommittee on Replicability in Science Advisory Committee to the\nNational Science Foundation, 2015. Available: https://www.nsf.gov/sbe/AC_Materials/SBE_Robust_and_Reliable_Research_Report.pdf\n\n\n[11] J.\nP. Simmons, L. D. Nelson, and U. Simonsohn, “False-positive\npsychology: Undisclosed flexibility in data collection and analysis\nallows presenting anything as significant,” Psychological\nScience, vol. 22, no. 11, pp. 1359–1366, Nov. 2011, doi: 10.1177/0956797611417632.\n\n\n[12] N.\nL. Kerr, “HARKing: Hypothesizing after the results are\nknown,” Personality and Social Psychology Review, vol.\n2, no. 3, pp. 196–217, 1998, doi: 10.1207/s15327957pspr0203_4.\n\n\n[13] L.\nK. John, G. Loewenstein, and D. Prelec, “Measuring the prevalence\nof questionable research practices with incentives for truth\ntelling,” Psychological Science, vol. 23, no. 5, pp.\n524–532, May 2012, doi: 10.1177/0956797611430953.\n\n\n[14] M.\nD. Wilkinson et al., “The FAIR guiding principles for\nscientific data management and stewardship,” Scientific\nData, vol. 3, no. 1, p. 160018, 2016, doi: 10.1038/sdata.2016.18.\n\n\n[15] R.\nGentleman and D. Temple Lang, “Statistical analyses and\nreproducible research,” Bioconductor Project, Working Paper 2,\nMay 2004. Available: https://biostats.bepress.com/bioconductor/paper2/\n\n\n[16] J.\nF. Claerbout and M. Karrenbach, “Electronic documents give\nreproducible research a new meaning,” in SEG technical\nprogram expanded abstracts, New Orleans, Louisiana: Society of\nExploration Geophysicists, 1992, pp. 601–604. doi: 10.1190/1.1822162.\n\n\n[17] H.\nA. Piwowar, R. S. Day, and D. B. Fridsma, “Sharing detailed\nresearch data is associated with increased citation rate,”\nPLoS ONE, vol. 2, no. 3, p. e308, Mar. 2007, doi: 10.1371/journal.pone.0000308.\n\n\n[18] H.\nA. Piwowar and T. J. Vision, “Data reuse and the open data\ncitation advantage,” PeerJ, vol. 1, p. e175, 2013, doi:\n10.7717/peerj.175.\n\n\n[19] A.\nM. Pienta, G. C. Alter, and J. A. Lyle, “The enduring value of\nsocial science research: The use and reuse of primary research\ndata,” in The organisation, economics and policy of\nscientific research workshop, Torino, Italy, 2010. Available: https://deepblue.lib.umich.edu/handle/2027.42/78307\n\n\n[20] V.\nStodden, “The legal framework for reproducible scientific\nresearch: Licensing and copyright,” Computing in Science\n& Engineering, vol. 11, no. 1, pp. 35–40, 2009, doi: 10.1109/MCSE.2009.19.\n\n\n[21] M.\nKlein et al., “Scholarly context not found: One in five\narticles suffers from reference rot,” PLoS ONE, vol. 9,\nno. 12, p. e115253, 2014, doi: 10.1371/journal.pone.0115253.\n\n\n[22] A.\nH. C. van Kampen et al., “ENCORE: A\npractical implementation to improve reproducibility and transparency of\ncomputational research,” Nature Communications, vol. 15,\nno. 1, p. 8117, Sep. 2024, doi: 10.1038/s41467-024-52446-8.\n\n\n[23] The Turing Way Community, The turing way: A\nhandbook for reproducible, ethical and collaborative research.\nZenodo, 2024. doi: 10.5281/zenodo.3233853.\n\n\n[24] M.\nJones, “Introducing reproducible research standards at the world\nbank,” Harvard Data Science Review, vol. 6, no. 4, Oct.\n2024, doi: 10.1162/99608f92.21328ce3.\n\n\n[25] G.\nWilson, J. Bryan, K. Cranston, J. Kitzes, L. Nederbragt, and T. K. Teal,\n“Good enough practices in scientific computing,” PLOS\nComputational Biology, vol. 13, no. 6, p. e1005510, 2017, doi: 10.1371/journal.pcbi.1005510.\n\n\n[26] A.\nRule et al., “Ten simple rules for writing and sharing\ncomputational analyses in jupyter notebooks,” PLOS\nComputational Biology, vol. 15, no. 7, p. e1007007, 2019, doi: 10.1371/journal.pcbi.1007007.\n\n\n[27] J.\nF. Pimentel, L. Murta, V. Braganholo, and J. Freire, “A\nlarge-scale study about quality and reproducibility of jupyter\nnotebooks,” in Proceedings of the 16th international\nconference on mining software repositories (MSR), IEEE, 2019, pp.\n507–517. doi: 10.1109/MSR.2019.00077.\n\n\n[28] T.\nKluyver et al., “Jupyter notebooks – a publishing format\nfor reproducible computational workflows,” in Positioning and\npower in academic publishing: Players, agents and agendas, IOS\nPress, 2016, pp. 87–90. doi: 10.3233/978-1-61499-649-1-87.\n\n\n[29] Aakanksha, P. Kumari, M. Thakur, and S.\nChakraborty, “Natural language outlines for code: Literate\nprogramming in the LLM era,” Proceedings of the ACM\nInternational Conference on the Foundations of Software Engineering\n(FSE), 2024, doi: 10.1145/3696630.3728541.\n\n\n[30] Y.\nZhu et al., “Renaissance of literate programming in the\nera of LLMs: Enhancing LLM-based code generation in large-scale\nprojects,” arXiv preprint, 2024, Available: https://arxiv.org/abs/2502.17441\n\n\n[31] R.\nSouza et al., “Workflow provenance in the computing\ncontinuum for responsible, trustworthy, and energy-efficient AI,”\nin IEEE international conference on e-science, 2024.\n\n\n[32] R.\nSouza et al., “PROV-AGENT: Unified provenance for\ntracking AI agent interactions in agentic workflows,” 5th\nWorkshop on Reproducible Workflows, Data Management, and Security,\n2024.\n\n\n[33] H.\nChen, R. Souza, et al., “The (r)evolution of scientific\nworkflows in the agentic AI era: Towards autonomous science,”\nProceedings of the SC ’25 Workshops, 2025, doi: 10.1145/3731599.3767580.",
    "crumbs": [
      "참고문헌"
    ]
  },
  {
    "objectID": "glossary.html#sec-glossary-hardware",
    "href": "glossary.html#sec-glossary-hardware",
    "title": "용어사전",
    "section": "",
    "text": "기계어 코드(machine code)\n\n중앙처리장치(CPU)가 직접 실행할 수 있는 가장 낮은 수준의 프로그래밍 언어. 0과 1의 이진수로 구성된다.\n\n보조 기억장치(secondary memory)\n\n전원이 꺼져도 데이터를 보존하는 저장 장치. 하드 디스크, SSD, USB 플래시 드라이브 등이 해당한다. 주기억장치보다 속도는 느리지만 용량이 크고 영구적이다.\n\n입출력 장치(input/output device)\n\n컴퓨터와 외부 세계 간에 데이터를 주고받는 장치. 키보드, 마우스, 모니터, 프린터 등이 해당한다.\n\n주기억장치(main memory)\n\nCPU가 직접 접근하여 데이터를 읽고 쓸 수 있는 고속 저장 장치. RAM(Random Access Memory)이 대표적이며, 전원이 꺼지면 데이터가 사라지는 휘발성 메모리다.\n\n중앙처리장치(central processing unit, CPU)\n\n컴퓨터의 두뇌에 해당하는 핵심 부품. 프로그램의 명령어를 해석하고 실행하며, 연산과 제어를 담당한다. 3.0 GHz CPU는 초당 약 30억 개의 명령어를 처리할 수 있다.",
    "crumbs": [
      "**부록**",
      "용어사전"
    ]
  },
  {
    "objectID": "glossary.html#sec-glossary-language",
    "href": "glossary.html#sec-glossary-language",
    "title": "용어사전",
    "section": "프로그래밍 언어",
    "text": "프로그래밍 언어\n\n로우레벨 언어(low-level language)\n\n기계어와 가까운 프로그래밍 언어. 하드웨어를 직접 제어할 수 있지만 사람이 읽고 쓰기 어렵다. 어셈블리 언어가 대표적이다.\n\n예약어(reserved word)\n\n프로그래밍 언어에서 특별한 의미를 가지며, 변수명이나 함수명으로 사용할 수 없는 단어. 파이썬의 if, for, def, return 등이 해당한다.\n\n하이레벨 언어(high-level language)\n\n사람이 이해하기 쉽게 설계된 프로그래밍 언어. 파이썬, 자바, C++ 등이 해당한다. 실행 전에 기계어로 번역되어야 한다.",
    "crumbs": [
      "**부록**",
      "용어사전"
    ]
  },
  {
    "objectID": "glossary.html#sec-glossary-execution",
    "href": "glossary.html#sec-glossary-execution",
    "title": "용어사전",
    "section": "프로그램 실행",
    "text": "프로그램 실행\n\n인터프리터(interpreter)\n\n소스 코드를 한 줄씩 읽어서 즉시 실행하는 프로그램. 파이썬, 자바스크립트가 인터프리터 방식을 사용한다. 코드를 바로 실행할 수 있어 개발과 테스트가 편리하다.\n\n컴파일(compile)\n\n하이레벨 언어로 작성된 소스 코드 전체를 기계어로 번역하는 과정. 컴파일된 프로그램은 실행 속도가 빠르지만, 수정 후 다시 컴파일해야 한다.\n\n파싱(parsing)\n\n소스 코드의 문법 구조를 분석하는 과정. 인터프리터나 컴파일러가 코드를 실행하거나 번역하기 전에 수행한다.",
    "crumbs": [
      "**부록**",
      "용어사전"
    ]
  },
  {
    "objectID": "glossary.html#sec-glossary-components",
    "href": "glossary.html#sec-glossary-components",
    "title": "용어사전",
    "section": "프로그램 구성 요소",
    "text": "프로그램 구성 요소\n\n변수(variable)\n\n데이터를 저장하는 이름이 붙은 메모리 공간. 프로그램 실행 중에 값을 저장하고 참조하는 데 사용한다.\n\n스크립트(script)\n\n인터프리터가 실행할 수 있는 명령어들이 저장된 텍스트 파일. 파이썬 스크립트는 .py 확장자를 사용한다.\n\n소스 코드(source code)\n\n프로그래머가 작성한 프로그램의 원본 텍스트. 하이레벨 언어로 작성되며, 사람이 읽고 수정할 수 있다.\n\n프로그램(program)\n\n컴퓨터가 특정 작업을 수행하도록 작성된 명령어들의 집합. 소프트웨어라고도 부른다.\n\n프롬프트(prompt)\n\n사용자의 입력을 기다리는 상태를 나타내는 기호. 파이썬 인터랙티브 모드에서는 &gt;&gt;&gt;가 프롬프트로 사용된다.",
    "crumbs": [
      "**부록**",
      "용어사전"
    ]
  },
  {
    "objectID": "glossary.html#sec-glossary-activities",
    "href": "glossary.html#sec-glossary-activities",
    "title": "용어사전",
    "section": "프로그래밍 활동",
    "text": "프로그래밍 활동\n\n디버깅(debugging)\n\n프로그램의 오류(버그)를 찾아서 수정하는 과정. 프로그래밍에서 가장 많은 시간이 소요되는 활동 중 하나다.\n\n문제 해결(problem solving)\n\n문제를 분석하고, 해결 방법을 설계하며, 그 해결책을 구현하는 과정. 프로그래밍의 본질적인 활동이다.\n\n이식성(portability)\n\n프로그램이 다른 컴퓨터 환경에서도 실행될 수 있는 특성. 하이레벨 언어로 작성된 프로그램은 이식성이 높다.\n\n인터랙티브 모드(interactive mode)\n\n프롬프트에서 명령어를 입력하면 즉시 실행 결과를 확인할 수 있는 모드. 파이썬 REPL(Read-Eval-Print Loop)이 대표적이다.",
    "crumbs": [
      "**부록**",
      "용어사전"
    ]
  },
  {
    "objectID": "glossary.html#sec-glossary-errors",
    "href": "glossary.html#sec-glossary-errors",
    "title": "용어사전",
    "section": "오류 유형",
    "text": "오류 유형\n\n구문 오류(syntax error)\n\n프로그래밍 언어의 문법 규칙을 위반했을 때 발생하는 오류. 괄호 누락, 오타, 잘못된 들여쓰기 등이 원인이다. 프로그램 실행 전에 발견된다.\n\n논리 오류(logic error)\n\n문법적으로는 올바르지만 프로그램의 실행 순서나 조건이 잘못되어 의도한 결과가 나오지 않는 오류. 찾기 어려운 경우가 많다.\n\n버그(bug)\n\n프로그램의 오류를 일컫는 일반적인 용어. 1947년 하버드 대학의 Mark II 컴퓨터에서 실제 나방(bug)이 발견된 것에서 유래했다는 설이 있다.\n\n의미론적 오류(semantic error)\n\n문법적으로 올바르고 실행도 되지만, 프로그래머가 의도한 것과 다른 동작을 하는 오류. 프로그램이 “잘못된 일을 올바르게” 수행하는 경우다.",
    "crumbs": [
      "**부록**",
      "용어사전"
    ]
  },
  {
    "objectID": "glossary.html#sec-glossary-output",
    "href": "glossary.html#sec-glossary-output",
    "title": "용어사전",
    "section": "출력과 통신",
    "text": "출력과 통신\n\n출력문(print statement)\n\n프로그램의 결과를 화면에 표시하는 명령문. 파이썬에서는 print() 함수를 사용한다.",
    "crumbs": [
      "**부록**",
      "용어사전"
    ]
  },
  {
    "objectID": "glossary.html#sec-glossary-semantics",
    "href": "glossary.html#sec-glossary-semantics",
    "title": "용어사전",
    "section": "의미론",
    "text": "의미론\n\n구문론(syntax)\n\n프로그래밍 언어의 문법 규칙. 코드가 어떤 형식으로 작성되어야 하는지를 정의한다.\n\n의미론(semantics)\n\n프로그램이나 명령문의 의미. 코드가 실행될 때 어떤 동작을 수행하는지를 나타낸다.",
    "crumbs": [
      "**부록**",
      "용어사전"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-new-contract",
    "href": "01-intro.html#sec-intro-new-contract",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.2 개발자와 AI의 새로운 계약",
    "text": "1.2 개발자와 AI의 새로운 계약\nAI 시대에 인간 개발자의 역할이 바뀌고 있다. ‘코드를 쌓는 벽돌공’에서 ‘도시를 설계하는 건축가’로.\nAI는 지금까지 본 적 없는 뛰어난 벽돌공이다. 지치지 않고, 불평하지 않으며, 놀라운 속도로 벽돌(코드)을 쌓아 올린다. 하지만 어떤 건물을 지어야 할지, 도시의 맥락과 조화를 이루는지, 사람들에게 어떤 영향을 미칠지는 모른다.\n그것이 바로 ’건축가’인 우리의 새로운 역할이다.\n\n\n\n\n\n그림 1.1: 프로그래밍 패러다임의 변화: 벽돌공에서 건축가로\n\n\n\n\n시스템 설계자 (Architect): 어떤 문제를 풀 것인지 정의하고, 어떤 기술 요소로 청사진을 그릴지 결정한다.\n\n비판적 검토자(Critic): AI가 쌓은 벽이 튼튼한지, 설계도대로 지어졌는지 끊임없이 의심하고 검증한다.\n\n윤리적 감시자(Ethicist): 우리가 짓는 건물이 사람들과 사회에 긍정적인 영향을 미치는지 성찰하고, 잠재적 위험을 관리한다.\n\n최종 책임자(Accountable Owner): AI의 도움을 받았더라도 완성된 건물에 대한 최종 책임은 우리에게 있다.\n\n더 이상 단순히 ’코딩’을 배우는 것이 아니다. ‘시스템을 설계하고 AI와 협업하는 법’을 배운다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-talk-computer",
    "href": "01-intro.html#sec-intro-talk-computer",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.3 컴퓨터와 대화하기",
    "text": "1.3 컴퓨터와 대화하기\n프로그래밍은 창의적인 활동이다. 투자한 것 이상으로 돌아오는 것도 많다. 어려운 자료분석 문제를 해결하려는 목적도 있고, 다른 사람의 문제를 해결해주는 재미도 있다. 프로그램을 어떻게 만드는지 알게 되면, 새로 습득한 기술로 하고자 하는 것을 해결할 수 있다.\n일상은 노트북에서 스마트폰까지 다양한 컴퓨터에 둘러싸여 있다. 컴퓨터를 많은 일을 대신해 주는 “개인비서”로 생각해보자. 컴퓨터 하드웨어는 끊임없이 “다음에 무엇을 할까요?”라고 묻도록 설계되어 있다.\n\n\n\n\n\n그림 1.2: 컴퓨터 소개\n\n\n프로그래머가 운영체제와 하드웨어에 응용 프로그램을 추가하면서, 컴퓨터는 개인 휴대 정보 단말(PDA)로 진화했다. 최근에는 AI가 탑재된 스마트폰이 PDA 역할을 대신한다.\n컴퓨터가 이해하는 언어로 “다음에 이것을 해”라고 지시할 수 있다면, 컴퓨터는 빠른 속도와 큰 저장 용량 덕분에 매우 유용해진다. 컴퓨터 언어를 알면 반복적인 작업을 컴퓨터에 맡길 수 있다. 흥미롭게도, 컴퓨터가 가장 잘하는 작업은 사람이 지루하고 재미없다고 느끼는 것들이다.\n예를 들어, 이 장의 첫 세 문단에서 가장 많이 나오는 단어를 찾아보라. 사람은 몇 초 만에 단어를 읽고 이해할 수 있지만, 단어 빈도를 세는 것은 고된 작업이다. 사람은 지루하고 반복적인 일에 적합하지 않기 때문이다. 컴퓨터는 정반대다. 텍스트를 읽고 이해하는 것은 어렵지만, 단어를 세고 가장 많이 쓰인 단어를 찾는 것은 식은 죽 먹기다.\n\n$ R words.R\nEnter file: words.txt\nto 16\n\n컴퓨터는 첫 세 문단에서 단어 “to”가 16번으로 가장 많이 사용되었다고 즉시 답한다.\n사람이 못하는 것을 컴퓨터가 잘한다는 사실을 이해하면, 왜 “컴퓨터 언어”를 배워야 하는지 알 수 있다. 파이썬(Python) 같은 언어를 배우면, 지루하고 반복적인 일은 컴퓨터에 맡기고 사람에게 적합한 일에 집중할 수 있다. 직관, 창의성, 독창성을 컴퓨터라는 파트너와 함께 발휘한다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-motivation",
    "href": "01-intro.html#sec-intro-motivation",
    "title": "1  프로그래밍 학습 이유",
    "section": "\n1.4 창의성과 동기",
    "text": "1.4 창의성과 동기\n이 책은 직업 프로그래머를 위한 것은 아니다. 하지만 프로그래밍은 개인적으로나 경제적으로 매력적인 일이다. 유용하고, 아름답고, 똑똑한 프로그램을 만들어 다른 사람이 쓰게 하는 것은 창의적인 활동이다. 스마트폰에 설치된 수많은 앱을 보라. 프로그래머들이 사용자의 관심을 얻기 위해 경쟁하며 만든 결과물이다. 사용자가 원하는 바를 충족시키고 훌륭한 경험을 제공하려고 노력한 흔적이 담겨 있다.\n프로그램을 프로그래머 집단의 창의적 결과물로 본다면, 스마트폰의 앱들이 바로 그 증거다.\n이 책에서는 사업이나 사용자를 기쁘게 하는 것보다, 일상에서 맞닥뜨리는 자료와 정보를 다뤄 삶을 생산적으로 만드는 데 초점을 맞춘다. 프로그램을 만들기 시작하면, 프로그래머이면서 동시에 자신이 만든 프로그램의 사용자가 된다. 기술을 습득하고 프로그래밍이 창의적으로 느껴지기 시작하면, 다른 사람을 위해 프로그램을 개발할 준비가 된 것이다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 학습 이유</span>"
    ]
  },
  {
    "objectID": "index.html#ai-네이티브-시스템의-구성-요소",
    "href": "index.html#ai-네이티브-시스템의-구성-요소",
    "title": "챗GPT 코딩2",
    "section": "AI 네이티브 시스템의 구성 요소",
    "text": "AI 네이티브 시스템의 구성 요소\n현대적인 AI 시스템은 하나의 언어나 도구로 만들어지지 않습니다. 여러 기술이 각자의 역할을 수행하며 유기적으로 연결된 ’스택(Stack)’의 형태를 띱니다. 이 책에서 우리는 AI 네이티브 시스템을 구성하는 네 개의 핵심 계층을 탐험할 것이며, 각 계층은 다음과 같은 근본적인 질문에 답합니다.\n\n\n\n\n\n\n그림 1: AI 네이티브 시스템 스택\n\n\n\n\n기반 계층 (Foundation Layer): Git, Docker, Shell “이 코드가 1년 뒤 내 동료의 컴퓨터에서도 똑같이 작동한다고 어떻게 확신할 수 있는가?” 모든 것의 기반이 되는 재현성과 환경의 기술입니다. 코드를 관리하고, 협업하며, 어떤 컴퓨터에서든 동일한 환경을 보장하는 단단한 암반과 같습니다.\n처리 계층 (Processing Layer): R & Python “이 지저분한 원본 데이터를 어떻게 의미 있는 형태로 가장 효율적으로 바꿀 수 있는가?” 데이터를 읽고, 변형하며, 비즈니스 로직을 구현하는 논리와 연산의 엔진입니다.\n지능 계층 (Intelligence Layer): LLM APIs, Vector DBs “수만 개의 고객 리뷰를 모두 읽지 않고도, 핵심적인 불만 사항과 긍정적인 반응을 어떻게 추출할 수 있는가?” 시스템에 ’생각하는 능력’을 부여하는 인지의 코어입니다.\n조율 계층 (Orchestration Layer): Make, GitHub Actions “매일 아침 9시, 내가 키보드에 손대지 않고도 이 모든 과정이 자동으로 실행되게 하려면 어떻게 해야 하는가?” 각 계층의 작업을 정해진 순서에 따라 자동으로 실행하도록 지휘하는 오케스트라 지휘자입니다.\n\n이 책의 최종 목표는, 이 모든 것을 통합하여 매일 새로운 데이터를 바탕으로 AI의 지능을 빌려 새로운 지식을 창출하고, 그 결과를 정해진 형태로 만들어내는 하나의 완전한 ’자동화된 지능형 시스템’을 완성하는 것입니다.\n이 책을 덮을 때쯤, 독자 여러분은 단순히 코드를 작성하는 사람을 넘어, 신뢰할 수 있고 자동화된 데이터 시스템을 설계하고 구축하는 전문가로 성장해 있을 것입니다. AI의 시대, 데이터로 진짜 문제를 해결하는 전문가가 되기 위한 여정을 이제 시작하겠습니다.",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-why",
    "href": "01-intro.html#sec-intro-why",
    "title": "1  프로그래밍 시작하기",
    "section": "",
    "text": "1.1.1 개발자의 새로운 역할\nAI 시대에 인간 개발자의 역할이 바뀌고 있다. ‘코드를 쌓는 벽돌공’에서 ‘도시를 설계하는 건축가’로.\nAI는 지금까지 본 적 없는 뛰어난 벽돌공이다. 지치지 않고, 불평하지 않으며, 놀라운 속도로 벽돌(코드)을 쌓아 올린다. 하지만 어떤 건물을 지어야 할지, 도시의 맥락과 조화를 이루는지, 사람들에게 어떤 영향을 미칠지는 모른다. 그림 1.1 는 벽돌공에서 건축가로의 전환을 보여준다.\n\n\n\n\n\n그림 1.1: 프로그래밍 패러다임 변화 - 벽돌공에서 건축가로\n\n\n그림 1.1 왼쪽에서 과거의 개발자는 망치를 들고 직접 벽돌을 쌓는다. 오른쪽에서 AI 시대의 개발자는 설계도를 들고 AI에게 지시한다. 건축가는 어떤 문제를 풀 것인지 정의하고, 청사진을 그린다. AI가 쌓은 벽이 튼튼한지, 설계도대로 지어졌는지 끊임없이 의심하고 검증한다. 건물이 사람들과 사회에 긍정적인 영향을 미치는지 성찰하고, 잠재적 위험을 관리한다. AI의 도움을 받았더라도 완성된 건물에 대한 최종 책임은 건축가에게 있다.\n더 이상 단순히 ’코딩’을 배우는 것이 아니다. 시스템을 설계하고 AI와 협업하는 법을 배운다.\n\n1.1.2 프로그래밍 매력\n프로그래밍은 개인적으로나 경제적으로 매력적인 활동이다. 유용하고, 아름답고, 똑똑한 프로그램을 만들어 다른 사람이 쓰게 하는 것은 창의적인 작업이다. 스마트폰에 설치된 수많은 앱을 보라. 프로그래머들이 사용자의 관심을 얻기 위해 경쟁하며 만든 결과물이다.\n이 책에서는 사업이나 사용자를 기쁘게 하는 것보다, 일상에서 맞닥뜨리는 자료와 정보를 다뤄 삶을 생산적으로 만드는 데 초점을 맞춘다. 프로그램을 만들기 시작하면, 프로그래머이면서 동시에 자신이 만든 프로그램의 사용자가 된다. 기술을 습득하고 프로그래밍이 창의적으로 느껴지기 시작하면, 다른 사람을 위해 프로그램을 개발할 준비가 된 것이다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 시작하기</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-what",
    "href": "01-intro.html#sec-intro-what",
    "title": "1  프로그래밍 시작하기",
    "section": "\n1.2 컴퓨터란 무엇인가",
    "text": "1.2 컴퓨터란 무엇인가\n \n컴퓨터는 다섯 가지 요소로 이루어진 시스템이다. 하드웨어가 물리적 기반을 제공하고, 소프트웨어가 하드웨어에 명령을 내린다. 네트워크로 다른 컴퓨터와 연결되고, 데이터를 저장하고 처리한다. 최근에는 AI가 다섯 번째 요소로 추가되어, 사람의 판단이 필요했던 영역까지 자동화하고 있다.\n프로그래머는 다섯 요소를 조합해 문제를 해결한다. 하드웨어의 한계를 이해하고, 소프트웨어로 논리를 구현하며, 네트워크로 서비스를 연결하고, 데이터에서 의미를 추출한다. AI 시대에는 AI를 언제 어떻게 활용할지 판단하는 능력까지 요구된다.\n\n1.2.1 하드웨어 구조\n일상은 노트북에서 스마트폰까지 다양한 컴퓨터에 둘러싸여 있다. 컴퓨터를 많은 일을 대신해 주는 “개인비서”로 생각해보자. 컴퓨터 하드웨어는 끊임없이 “다음에 무엇을 할까요?”라고 묻도록 설계되어 있다. 그림 1.2 는 컴퓨터가 수행하는 기본 작업을 보여준다.\n\n\n\n\n\n그림 1.2: 컴퓨터 소개\n\n\n그림 1.2 에서 보듯이 컴퓨터는 입력을 받아 처리하고 출력한다. 텍스트와 클릭부터 음성과 이미지까지 다양한 형태의 입력을 받아들이고, “다음에 무엇을 할까요?”라는 질문에 답하며, 결과를 화면이나 파일로 내보낸다. 컴퓨터가 질문에 답하려면 여러 부품이 협력해야 한다. 노트북이나 스마트폰을 분해하면 여섯 가지 핵심 부품을 발견할 수 있다. 각 부품은 고유한 역할을 맡아 전체 시스템이 작동하도록 돕는다. 그림 1.3 는 부품 간 연결 구조를 보여준다.\n\n\n\n\n\n그림 1.3: 컴퓨터 아키텍처\n\n\n그림 1.3 에서 CPU가 중심에 위치하고, 입출력장치, 네트워크, GPU, 메모리가 연결된다. 중앙처리장치(CPU)는 “다음에 뭘 할까?”라는 질문에 답하는 두뇌다. 순차적으로 명령을 처리하며, 3.0 GHz CPU라면 초당 30억 개의 범용 연산을 수행한다. GPU는 CPU와 다른 방식으로 일한다. 수천 개의 작은 코어가 동시에 계산을 처리하는 병렬 처리에 특화되어 있어, AI 모델 학습과 추론에 필수적인 부품이 되었다.\n주기억장치(RAM)는 CPU가 즉시 필요한 정보를 저장한다. 빠르지만 전원이 꺼지면 내용이 사라진다. 보조기억장치(SSD/HDD)는 느리지만 전원이 꺼져도 정보가 유지된다. 입출력장치는 키보드와 마우스로 텍스트와 클릭을 입력받고, 화면과 파일로 결과를 출력한다. AI 시대에는 음성과 이미지도 자연스러운 입출력 수단이 되었다. 네트워크는 인터넷과 클라우드를 통해 외부 데이터를 주고받으며, 최근에는 AI API 호출의 통로 역할까지 담당한다.\n프로그래머는 CPU에 “다음에 무엇을 실행하라”고 지시한다. 필요에 따라 GPU에 병렬 연산을 맡기고, 메모리에 데이터를 저장하며, 네트워크를 통해 외부 서비스와 통신한다.\n과거에는 프로그래머가 컴퓨터와 직접 대화했다. 문법을 외우고, 한 줄 한 줄 코드를 작성해야 했다. AI 시대에는 중간에 새로운 계층이 등장했다. 프로그래머는 자연어로 의도를 전달하고, AI 에이전트가 맥락을 이해해 파이썬이나 R 코드로 번역한다. 컴퓨터는 여전히 코드를 실행하지만, 코드를 작성하는 주체가 달라졌다. 그림 1.4 은 사람 → AI → 기계로 이어지는 새로운 흐름을 보여준다.\n\n\n\n\n\n그림 1.4: AI 시대 컴퓨팅 스택\n\n\n그림 1.4 에서 사용자는 자연어와 테스트로 의도를 전달하고, AI 에이전트가 파이썬이나 R 코드로 변환해 컴퓨터에 전달한다. 프로그래머의 역할은 코드 작성에서 의도 정의와 검증으로 이동했다. AI가 생성한 코드가 원래 의도와 맞는지 확인하고, 오류가 있으면 수정을 요청한다. “의도”만 명확하면 실행 가능한 프로그램이 나온다. 문법 암기보다 문제 정의와 결과 검증 능력이 중요해진 이유다.\n\n1.2.2 프로그램이란\n프로그램(Program)은 특정 작업을 수행하는 파이썬 문장의 집합이다. hello.py처럼 한 줄짜리도 프로그램이고, 수천 줄에 달하는 복잡한 시스템도 프로그램이다. 프로그램의 핵심은 반복적인 작업을 자동화한다는 데 있다.\n소셜 미디어 게시물에서 가장 자주 사용된 단어를 찾는 연구를 한다고 가정하자. 아래 텍스트에서 가장 많이 나오는 단어와 빈도를 손으로 세어보라.\nthe clown ran after the car and the car ran into the tent\nand the tent fell down on the clown and the car\n두 줄짜리 문장도 손으로 세면 시간이 걸리고 실수하기 쉽다. 수백만 줄이라면 어떨까? 파이썬 프로그램은 이러한 단순 반복 작업을 순식간에 처리한다.\ntext = 'the clown ran after the car and the car ran into the tent and the tent fell down on the clown and the car'\n\nwords = text.split()\ncounts = dict()\n\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1\n\nbigcount = None\nbigword = None\n\nfor word, count in counts.items():\n    if bigcount is None or count &gt; bigcount:\n        bigword = word\n        bigcount = count\n\nprint(bigword, bigcount)\n# 출력: the 7\n위 코드는 텍스트를 단어로 쪼개고, 각 단어가 몇 번 등장했는지 세어, 가장 많이 나온 단어를 출력한다. 지금 당장 모든 문법을 이해할 필요는 없다. 프로그램이 어떤 일을 하는지 대략적인 흐름만 파악하면 된다. 파이썬 문법은 앞으로 여러 장에 걸쳐 차근차근 배운다.\n\n1.2.3 프로그램 구성요소\n프로그램 작성에 사용되는 기본 패턴이 있다. 파이썬만이 아니라 기계어부터 고수준 언어까지 모든 언어에 공통된다. 그림 1.5 는 여섯 가지 핵심 구성요소를 보여준다.\n\n\n\n\n\n그림 1.5: 프로그램 구성요소\n\n\n그림 1.5 에서 프로그램은 입력에서 시작해 출력으로 끝난다. 입력은 파일, 키보드, 센서, API 등 외부에서 데이터를 가져오는 것이다. 순차 실행은 작성된 순서대로 위에서 아래로 한 줄씩 명령을 처리한다. 조건 실행은 특정 조건을 확인하고 분기를 선택해 실행하거나 건너뛴다. 반복 실행은 조건이 충족될 때까지 같은 명령을 되풀이한다. 출력은 처리 결과를 화면, 파일, 네트워크 등으로 내보낸다.\n그림 1.5 하단 재사용(함수)은 특별한 패턴이다. 자주 쓰는 명령문 묶음에 이름을 붙여 저장하고, 필요할 때 불러 쓴다. 순차, 조건, 반복 실행 어디서든 호출할 수 있어 코드 중복을 줄이고 구조를 명확하게 만든다.\n간단해 보이지만 그렇지 않다. 프로그래밍의 예술은 기본 요소를 조합해 사용자에게 유용한 것을 만드는 데 있다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 시작하기</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-how",
    "href": "01-intro.html#sec-intro-how",
    "title": "1  프로그래밍 시작하기",
    "section": "\n1.3 파이썬으로 대화하기",
    "text": "1.3 파이썬으로 대화하기\n \n프로그래밍 언어도 “언어”다. 한국어나 영어처럼 단어가 있고, 단어를 배열하는 규칙이 있으며, 단어 배열에는 특정한 의미가 전달된다. 다만 대화 상대가 사람이 아니라 컴퓨터라는 점이 다르다. 컴퓨터는 모호함을 이해하지 못하기 때문에, 프로그래밍 언어는 인간 언어보다 훨씬 단순하지만 엄격하다.\n\n1.3.1 언어의 구성요소\n인간 언어든 프로그래밍 언어든 모든 언어는 세 가지 요소로 이루어진다. 사용할 수 있는 단어 목록이 있고, 단어를 배열하는 규칙이 있으며, 배열된 문장이 전달하는 의미가 있다. 그림 1.6 는 어휘, 문법, 의미라는 공통 구성요소를 보여준다.\n\n\n\n\n\n그림 1.6: 언어의 구성요소\n\n\n그림 1.6 에서 어휘(Vocabulary)는 언어가 사용하는 단어 목록이다. 한국어에는 수십만 개의 단어가 있고, 파이썬 3.10+ 기준 35개 예약어가 있다. 문법(Syntax)은 단어를 배열하는 규칙이다. “나는 밥을 먹는다”는 올바른 한국어 문장이고, if x &gt; 0:은 올바른 파이썬 문장이다. 의미(Semantics)는 문장이 전달하는 뜻이다. 문법적으로 올바른 문장이라도 의미가 없을 수 있다. “무색의 녹색 아이디어가 맹렬히 잠든다”는 문법적으로 완벽하지만 의미를 파악하기 어렵다.\n\n1.3.2 인간 언어와 프로그래밍 언어\n어휘, 문법, 의미라는 구성요소는 같지만, 인간 언어와 프로그래밍 언어는 성격이 크게 다르다. 인간 언어는 수십만 개의 단어를 유연한 규칙으로 조합하고, 맥락에 따라 의미가 달라지는 것을 허용한다. 프로그래밍 언어는 수십 개의 단어를 엄격한 규칙으로 조합하고, 의미는 항상 하나로 고정된다. 그림 1.7 은 네 가지 측면에서 차이를 비교한다.\n\n\n\n\n\n그림 1.7: 인간 언어와 프로그래밍 언어 비교\n\n\n그림 1.7 에서 가장 눈에 띄는 차이는 어휘의 크기다. 한국어 화자는 평균 5만 개 이상의 단어를 알고, 영어 원어민은 2만~3만 개를 구사한다. 반면 파이썬 예약어는 35개에 불과하다. 문법도 다르다. “나 밥 먹었어”처럼 조사를 생략해도 의사소통이 되지만, 파이썬에서 콜론 하나를 빠뜨리면 프로그램이 멈춘다. 의미 해석도 차이가 크다. “배가 아프다”에서 “배”가 신체 부위인지 과일인지 선박인지는 맥락에 따라 달라지지만, 파이썬에서 bae는 항상 같은 변수를 가리킨다.\n\n\n\n\n\n\n노트프로그래밍 언어가 엄격한 이유\n\n\n\n컴퓨터는 초당 수십억 번의 연산을 수행한다. 모호함이 허용되면 매 연산마다 “어떤 의미로 해석할까?”를 판단해야 하고, 속도가 급격히 떨어진다. 프로그래밍 언어가 엄격한 이유는 컴퓨터의 한계 때문이 아니라, 빠른 실행을 위한 설계 선택이다. 인간이 모호함을 해결하고, 컴퓨터는 명확한 지시만 실행한다.\n\n\n\n1.3.3 예약어와 문법\n파이썬 어휘의 핵심은 예약어(reserved words)다. 예약어는 파이썬이 특별한 의미로 인식하는 단어로, if를 만나면 “조건을 검사하라”는 뜻으로만 해석한다. 프로그래머는 예약어 외에 자신만의 단어도 만들 수 있는데, 이를 변수(Variable)라 부른다. 변수 이름은 my_data, total_count처럼 자유롭게 지을 수 있지만, 예약어를 변수 이름으로 쓸 수는 없다. if = 10이라고 쓰면 파이썬은 혼란에 빠진다.\n강아지 훈련에 비유하면 이해가 쉽다. 강아지에게 “앉아”, “기다려”, “가져와” 같은 특별한 명령어가 있듯이, 파이썬에도 정해진 명령어가 있다. 예약어가 아닌 말을 하면 강아지는 물끄러미 쳐다볼 뿐이고, 파이썬은 오류를 뱉는다. 다만 강아지와 달리 파이썬은 이미 완벽하게 훈련되어 있어서, try라고 말하면 매번 정확히 같은 동작을 수행한다.\n표 1.1 은 파이썬 3.10+ 기준 35개 예약어를 분류별로 정리한 것이다. 파이썬 REPL에서 help(\"keywords\")를 입력하면 전체 목록을 확인할 수 있다.\n\n\n\n\n\n\n\n\n분류\n예약어\n용도\n\n\n\n제어 흐름\nif, elif, else, for, while, break, continue\n조건 분기와 반복 제어\n\n\n함수\ndef, return, lambda\n함수 정의와 반환\n\n\n클래스\nclass\n객체 지향 클래스 정의\n\n\n모듈\nimport, from, as\n외부 모듈 가져오기\n\n\n예외 처리\ntry, except, finally, raise\n오류 감지와 처리\n\n\n상수\nTrue, False, None\n논리값과 빈 값\n\n\n연산자\nand, or, not, in, is\n논리 연산과 포함 검사\n\n\n기타\nwith, pass, yield, assert, del, global, nonlocal, async, await\n컨텍스트 관리, 비동기 등\n\n\n\n\n\n\n\n표 1.1: 파이썬 3.10+ 예약어 35개\n\n\n\n예약어 35개를 모두 외울 필요는 없다. 프로그램을 작성하면서 자연스럽게 익히게 된다. 지금은 파이썬에 말을 거는 가장 간단한 방법부터 시작하자. 내장 함수 print에 따옴표로 감싼 메시지를 전달하면, 파이썬이 해당 메시지를 화면에 출력한다.\nprint(\"헬로 월드!\")\n위 코드는 파이썬 문법에 맞는 완전한 문장이다. print는 “출력하라”는 명령이고, 괄호 안의 \"헬로 월드!\"는 출력할 내용이다. 따옴표는 “이것은 텍스트 데이터다”라고 파이썬에게 알려주는 역할을 한다. 따옴표 없이 print(헬로 월드!)라고 쓰면 파이썬은 헬로라는 변수를 찾으려 하고, 찾지 못하면 오류를 낸다.\n\n1.3.4 인터프리터와 대화\n \n간단한 문장을 만들었으니, 이제 파이썬과 대화를 시작해보자. 파이썬을 공식 웹사이트에서 설치한 뒤, 터미널이나 명령 프롬프트에서 python을 입력하면 인터프리터가 인터랙티브 모드로 시작된다. 화면에 &gt;&gt;&gt; 프롬프트가 나타나면 파이썬이 대화할 준비가 된 것이다.\nPython 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n인터랙티브 모드는 REPL(Read-Eval-Print Loop)이라고도 부른다. 그림 1.8 에서 볼 수 있듯이, 사용자가 코드를 입력하면(Read), 파이썬이 이를 실행하고(Eval), 결과를 출력한 뒤(Print), 다시 입력을 기다리는(Loop) 과정이 끊임없이 반복된다. 이 순환 구조 덕분에 코드를 한 줄씩 실행하며 즉각적인 피드백을 받을 수 있어, 새로운 문법을 익히거나 아이디어를 빠르게 시험해보기에 안성맞춤이다.\n\n\n\n\n\n그림 1.8: REPL: 인터랙티브 모드의 순환 구조\n\n\n파이썬 언어를 전혀 모른다고 가정하고, 외계 행성에 착륙한 우주비행사처럼 말을 걸어보자.\n&gt;&gt;&gt; I come in peace, please take me to your leader\n  File \"&lt;stdin&gt;\", line 1\n    I come in peace, please take me to your leader\n      ^^^^^\nSyntaxError: invalid syntax\n영어로 말을 걸었지만 파이썬은 알아듣지 못한다. 파이썬 문법에 맞게 다시 시도해보자.\n&gt;&gt;&gt; print(\"헬로 파이썬!\")\n헬로 파이썬!\n&gt;&gt;&gt; print('당신은 하늘에서 온 전설적인 신이 분명합니다')\n당신은 하늘에서 온 전설적인 신이 분명합니다\n이번에는 대화가 잘 진행된다. 하지만 사소한 실수를 저지르면 파이썬은 곧바로 오류를 내며 대화를 중단한다. 파이썬은 복잡하고 강력하지만 구문(Syntax)에 엄격하기 때문에, 정해진 규칙대로 말해야만 알아듣는다. 대화를 끝내려면 exit() 또는 quit()를 입력한다.\n&gt;&gt;&gt; good-bye\nNameError: name 'good' is not defined\n&gt;&gt;&gt; exit()\ngood-bye가 오류를 낸 이유는 파이썬이 good이라는 변수를 찾으려 했기 때문이다. 하이픈(-)은 파이썬에서 빼기 연산자이므로, good - bye는 “good이라는 변수에서 bye라는 변수를 빼라”는 뜻으로 해석된다.\n\n\n\n\n\n\n힌트인터프리터와 컴파일러\n\n\n\n \n파이썬은 사람이 읽고 쓸 수 있으면서 컴퓨터도 처리할 수 있는 고수준(High-level) 언어다. 자바, C++, PHP, 루비, 자바스크립트 등도 고수준 언어에 속한다. 하지만 CPU가 이해하는 것은 0과 1로만 이루어진 기계어(machine language)뿐이다. 기계어는 01010001110100100101010000001111처럼 생겼는데, 단순해 보여도 실제로는 파이썬보다 훨씬 복잡하다. 기계어로 직접 코드를 작성하는 프로그래머는 극소수이며, 대신 고수준 언어를 기계어로 변환하는 번역기(translator)를 사용한다.\n\n\n\n\n\n그림 1.9: 인터프리터와 컴파일러 비교\n\n\n그림 1.9 에서 보듯이 번역기는 인터프리터(Interpreter)와 컴파일러(Compiler) 두 종류로 나뉜다. 인터프리터는 소스 코드를 한 줄씩 읽고 즉석에서 실행하며, 파이썬이 여기에 속한다. 컴파일러는 전체 소스 코드를 먼저 기계어로 번역해 실행 파일을 만든 뒤 실행한다. C, C++, Go, Rust 등이 컴파일러 방식을 사용한다.\n인터프리터가 즉각적인 피드백을 제공한다면, 컴파일러는 왜 여전히 쓰일까? 핵심은 실행 속도다. 컴파일러는 실행 전에 전체 코드를 분석하고 최적화하므로, 같은 작업을 수십~수백 배 빠르게 처리한다. 운영체제, 게임 엔진, 데이터베이스처럼 성능이 생명인 소프트웨어는 컴파일러 언어로 작성된다. 파이썬 인터프리터 자체도 C 언어로 만들어졌다(CPython). 결국 두 방식은 “개발 속도 vs 실행 속도”라는 상충 관계에 있으며, 용도에 따라 선택한다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 시작하기</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-challenge",
    "href": "01-intro.html#sec-intro-challenge",
    "title": "1  프로그래밍 시작하기",
    "section": "\n1.4 오류와 디버깅",
    "text": "1.4 오류와 디버깅\n \n앞서 보았듯이 파이썬 코드는 명확하게 작성해야 한다. 작은 실수 하나가 프로그램 전체를 멈추게 만들기도 한다. 초보자는 파이썬이 냉정하다고 느끼지만, 파이썬은 감정 없는 도구일 뿐이다. 오류 메시지는 비난이 아니라 도움 요청이다. “입력한 것을 이해하지 못하겠다”고 말하는 것뿐이므로, 파이썬과 싸워봐야 소용없다.\n&gt;&gt;&gt; primt(\"안녕 세상!\")\nNameError: name 'primt' is not defined\n&gt;&gt;&gt; 나는 파이썬이 싫어!\nSyntaxError: invalid syntax\n프로그램이 복잡해지면 그림 1.10 에서 보는 것처럼 세 종류의 오류를 만나게 된다. 구문 오류(Syntax Error)는 가장 흔하고 고치기 쉽다. 파이썬 문법에 맞지 않는다는 뜻이며, 오류 위치를 알려주지만 실제 원인은 앞줄에 있을 수도 있다. 논리 오류(Logic Error)는 구문은 맞지만 실행 순서가 잘못된 경우다. “물병에서 한 모금 마시고, 가방에 넣고, 도서관으로 걸어가서, 물병을 닫는다”처럼 순서가 뒤바뀌면 원하는 결과를 얻지 못한다. 의미론적 오류(Semantic Error)는 구문도 맞고 순서도 맞지만 의도한 대로 동작하지 않는 경우다. 식당 가는 길을 알려주면서 “왼쪽”이라고 해야 할 곳에서 “오른쪽”이라고 말한 것과 같다. 어떤 오류든 파이썬은 요청받은 대로 충실히 실행할 뿐이다.\n\n\n\n\n\n그림 1.10: 세 가지 오류 유형\n\n\n처음에 개념이 잘 와닿지 않아도 기죽을 필요 없다. 말하기를 배울 때도 처음 몇 년은 옹알이를 하고, 어휘에서 문장으로, 문장에서 문단으로 넘어가는 데 몇 년이 걸린다. 프로그래밍도 마찬가지다. 이 책은 파이썬을 빠르게 배울 수 있도록 정보를 제공하지만, 새 언어를 익히는 것처럼 자연스럽게 느껴지기까지 시간이 필요하다. 꼭 순서대로 읽을 필요는 없으니 앞뒤를 넘나들며 읽어도 좋다.\n첫 프로그래밍 언어를 배울 때는 “유레카!” 순간이 몇 번 찾아온다. 망치와 끌로 돌을 깎아 조각품을 만드는 과정과 비슷하다. 뭔가 특별히 어렵다면 밤새 붙잡고 있지 말고, 쉬거나 낮잠을 자거나 간식을 먹거나 다른 사람(또는 강아지)에게 문제를 설명해보라. 머리를 식힌 뒤 다시 시도하면 의외로 쉽게 풀리는 경우가 많다.",
    "crumbs": [
      "**1부** AI 시대 프로그래밍",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>프로그래밍 시작하기</span>"
    ]
  }
]